{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of UIP_Final_Github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "19kmHEkCq3iH",
        "colab_type": "code",
        "outputId": "2fdcaaf2-3c57-4ca8-be2a-e2646b4c2e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import Embedding, LSTM, Dense, Flatten, LeakyReLU\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRHkmSOZrloo",
        "colab_type": "code",
        "outputId": "613bd5b0-b252-457a-9d14-bf2756be0676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnW35XGq3iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREPROCESSING THE DATASET\n",
        "\n",
        "data = pd.read_excel('/content/gdrive/My Drive/training_set_rel3.xls')\n",
        "\n",
        "maxForSet = [12, 6, 3, 3, 4, 4, 30, 60]\n",
        "# NORMALIZING THE ESSAY GRADES TO 0 - 1\n",
        "for essay_set in range(1,9):\n",
        "    starter = (data['essay_set'] == essay_set).idxmax()\n",
        "    if(essay_set!=8):\n",
        "        end = (data['essay_set'] == essay_set+1).idxmax()\n",
        "    else:\n",
        "        end = 12978\n",
        "    for i in range(starter, end):\n",
        "      data.at[i,'domain1_score'] = data['domain1_score'][i]/maxForSet[essay_set-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD9PsbZbq3it",
        "colab_type": "code",
        "outputId": "428c0c9d-736e-4f8a-b695-0bb845659357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "source": [
        "# SANITIZING THE DATA\n",
        "test = data\n",
        "test.replace(' ', np.nan, inplace=True)\n",
        "test = test.dropna(subset=['domain1_score'])\n",
        "test['domain1_score'].isnull().values.any()\n",
        "data = test\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>rater1_domain1</th>\n",
              "      <th>rater2_domain1</th>\n",
              "      <th>rater3_domain1</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>rater1_domain2</th>\n",
              "      <th>rater2_domain2</th>\n",
              "      <th>domain2_score</th>\n",
              "      <th>rater1_trait1</th>\n",
              "      <th>rater1_trait2</th>\n",
              "      <th>rater1_trait3</th>\n",
              "      <th>rater1_trait4</th>\n",
              "      <th>rater1_trait5</th>\n",
              "      <th>rater1_trait6</th>\n",
              "      <th>rater2_trait1</th>\n",
              "      <th>rater2_trait2</th>\n",
              "      <th>rater2_trait3</th>\n",
              "      <th>rater2_trait4</th>\n",
              "      <th>rater2_trait5</th>\n",
              "      <th>rater2_trait6</th>\n",
              "      <th>rater3_trait1</th>\n",
              "      <th>rater3_trait2</th>\n",
              "      <th>rater3_trait3</th>\n",
              "      <th>rater3_trait4</th>\n",
              "      <th>rater3_trait5</th>\n",
              "      <th>rater3_trait6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I think that computers have a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Did you know that more and more people these d...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>@PERCENT1 of people agree that computers make ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear reader, @ORGANIZATION1 has had a dramatic...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>In the @LOCATION1 we have the technology of a ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  essay_set  ... rater3_trait5  rater3_trait6\n",
              "0         1          1  ...           NaN            NaN\n",
              "1         2          1  ...           NaN            NaN\n",
              "2         3          1  ...           NaN            NaN\n",
              "3         4          1  ...           NaN            NaN\n",
              "4         5          1  ...           NaN            NaN\n",
              "5         6          1  ...           NaN            NaN\n",
              "6         7          1  ...           NaN            NaN\n",
              "7         8          1  ...           NaN            NaN\n",
              "8         9          1  ...           NaN            NaN\n",
              "9        10          1  ...           NaN            NaN\n",
              "\n",
              "[10 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Mn2Ks-_KKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data[data['essay_set']==1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFKT4V08Ozx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_sentences(essay, remove_stopwords):\n",
        "  tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "  sentences = tokenizer.tokenize(essay)\n",
        "  print(sentences)\n",
        "  sentences_as_wordlists = []\n",
        "  for sentence in sentences:\n",
        "      if len(sentence) > 0:\n",
        "          sentences_as_wordlists.append(convert_to_wordlist(sentence, remove_stopwords))\n",
        "  return sentences_as_wordlists\n",
        "\n",
        "\n",
        "def convert_to_wordlist(essay, remove_stopwords):\n",
        "  essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n",
        "  word_list = essay.lower().split()\n",
        "  if remove_stopwords:\n",
        "    list_of_stopwords = stopwords.words(\"english\")\n",
        "    word_list_stopfree = []\n",
        "    for word in word_list:\n",
        "      if word not in list_of_stopwords:\n",
        "        word_list_stopfree.append(word)\n",
        "  return word_list_stopfree\n",
        "\n",
        "def avg_feature_vecs(essays, model, num_features):\n",
        "  essays_feature_vecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "  index2word_set = model.wv.index2word\n",
        "  counter = 0\n",
        "  for essay in essays:\n",
        "    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    for word in essay:\n",
        "      if word in index2word_set:\n",
        "        num_words += 1\n",
        "        feature_vec = np.add(feature_vec,model[word])\n",
        "    feature_vec = np.divide(feature_vec,num_words)\n",
        "    essays_feature_vecs[counter] = feature_vec\n",
        "    counter = counter + 1\n",
        "  return essays_feature_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7FE1J-AEwO9D",
        "colab": {}
      },
      "source": [
        "train_essays = data['essay']\n",
        "score = data['domain1_score']\n",
        "\n",
        "sentences = []\n",
        "for essay in train_essays:\n",
        "            sentences += convert_to_sentences(essay, remove_stopwords = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ayx-k1o7ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sumy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy973Zoqo-DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "\n",
        "listOfSummaries = []\n",
        "for essay in train_essays:\n",
        "  parser = PlaintextParser.from_string(essay, Tokenizer(\"english\"))\n",
        "  summarizer = LexRankSummarizer()\n",
        "  summary = summarizer(parser.document, 4)\n",
        "  s = ''\n",
        "  for sentence in summary:\n",
        "    s += ' ' + str(sentence)\n",
        "  listOfSummaries.append(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilcVp09GmEWU",
        "colab_type": "code",
        "outputId": "fa180f3e-5311-45c7-bd82-07f9c6d017b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "num_features = 300 \n",
        "min_word_count = 40\n",
        "num_workers = 4\n",
        "context = 10\n",
        "\n",
        "model_words = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context)\n",
        "clean_train_essays = []\n",
        "for essay in train_essays:\n",
        "  clean_train_essays.append(convert_to_wordlist(essay, remove_stopwords=True))\n",
        "trainDataVecs = avg_feature_vecs(clean_train_essays, model_words, num_features)\n",
        "\n",
        "trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "trainDataVecs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1783, 1, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr_kNH_RpOCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(trainDataVecs, score, \n",
        "                                                    test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG1XHMdwoOEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape = [1,300], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il2RtBCeQoim",
        "colab_type": "code",
        "outputId": "63dc3246-fe8d-45b5-f433-ad816b11377a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ONLY FOR GRAPH. DO NOT RUN OTHERWISE\n",
        "# TRAINING\n",
        "KAPPA_COHEN_SCORES = []\n",
        "for i in range(0,50):\n",
        "  lstm_model = get_model()\n",
        "  history = lstm_model.fit(X_train, y_train, batch_size=256, epochs=i, validation_data=[X_test,y_test], verbose = 1)\n",
        "  y_pred = lstm_model.predict(trainDataVecs)\n",
        "  # score\n",
        "  y_pred = y_pred * 100\n",
        "  y_pred = np.around(y_pred)\n",
        "  kappa_score = score\n",
        "  kappa_score = kappa_score * 100\n",
        "  kappa_score = np.around(kappa_score)\n",
        "  result = cohen_kappa_score(kappa_score,y_pred,weights='quadratic')\n",
        "  KAPPA_COHEN_SCORES.append(result)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/1\n",
            "10381/10381 [==============================] - 2s 189us/step - loss: 0.0709 - mean_absolute_error: 0.2043 - val_loss: 0.0423 - val_mean_absolute_error: 0.1605\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/2\n",
            "10381/10381 [==============================] - 2s 216us/step - loss: 0.0734 - mean_absolute_error: 0.2078 - val_loss: 0.0421 - val_mean_absolute_error: 0.1593\n",
            "Epoch 2/2\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0470 - mean_absolute_error: 0.1690 - val_loss: 0.0391 - val_mean_absolute_error: 0.1517\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/3\n",
            "10381/10381 [==============================] - 3s 264us/step - loss: 0.0750 - mean_absolute_error: 0.2103 - val_loss: 0.0422 - val_mean_absolute_error: 0.1589\n",
            "Epoch 2/3\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0468 - mean_absolute_error: 0.1684 - val_loss: 0.0407 - val_mean_absolute_error: 0.1534\n",
            "Epoch 3/3\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0436 - mean_absolute_error: 0.1605 - val_loss: 0.0437 - val_mean_absolute_error: 0.1584\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/4\n",
            "10381/10381 [==============================] - 3s 299us/step - loss: 0.0721 - mean_absolute_error: 0.2064 - val_loss: 0.0473 - val_mean_absolute_error: 0.1693\n",
            "Epoch 2/4\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0483 - mean_absolute_error: 0.1715 - val_loss: 0.0399 - val_mean_absolute_error: 0.1518\n",
            "Epoch 3/4\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0428 - mean_absolute_error: 0.1606 - val_loss: 0.0434 - val_mean_absolute_error: 0.1586\n",
            "Epoch 4/4\n",
            "10381/10381 [==============================] - 0s 36us/step - loss: 0.0415 - mean_absolute_error: 0.1572 - val_loss: 0.0369 - val_mean_absolute_error: 0.1488\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_11 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/5\n",
            "10381/10381 [==============================] - 4s 349us/step - loss: 0.0720 - mean_absolute_error: 0.2063 - val_loss: 0.0442 - val_mean_absolute_error: 0.1603\n",
            "Epoch 2/5\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0480 - mean_absolute_error: 0.1705 - val_loss: 0.0391 - val_mean_absolute_error: 0.1527\n",
            "Epoch 3/5\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0435 - mean_absolute_error: 0.1624 - val_loss: 0.0381 - val_mean_absolute_error: 0.1502\n",
            "Epoch 4/5\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0415 - mean_absolute_error: 0.1576 - val_loss: 0.0370 - val_mean_absolute_error: 0.1480\n",
            "Epoch 5/5\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0399 - mean_absolute_error: 0.1542 - val_loss: 0.0368 - val_mean_absolute_error: 0.1496\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/6\n",
            "10381/10381 [==============================] - 4s 404us/step - loss: 0.0727 - mean_absolute_error: 0.2073 - val_loss: 0.0419 - val_mean_absolute_error: 0.1572\n",
            "Epoch 2/6\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0476 - mean_absolute_error: 0.1695 - val_loss: 0.0390 - val_mean_absolute_error: 0.1521\n",
            "Epoch 3/6\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0432 - mean_absolute_error: 0.1613 - val_loss: 0.0396 - val_mean_absolute_error: 0.1565\n",
            "Epoch 4/6\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0415 - mean_absolute_error: 0.1573 - val_loss: 0.0366 - val_mean_absolute_error: 0.1455\n",
            "Epoch 5/6\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0387 - mean_absolute_error: 0.1511 - val_loss: 0.0349 - val_mean_absolute_error: 0.1441\n",
            "Epoch 6/6\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0381 - mean_absolute_error: 0.1499 - val_loss: 0.0375 - val_mean_absolute_error: 0.1469\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_15 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/7\n",
            "10381/10381 [==============================] - 5s 436us/step - loss: 0.0727 - mean_absolute_error: 0.2072 - val_loss: 0.0442 - val_mean_absolute_error: 0.1683\n",
            "Epoch 2/7\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0465 - mean_absolute_error: 0.1682 - val_loss: 0.0389 - val_mean_absolute_error: 0.1504\n",
            "Epoch 3/7\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0434 - mean_absolute_error: 0.1609 - val_loss: 0.0390 - val_mean_absolute_error: 0.1498\n",
            "Epoch 4/7\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0405 - mean_absolute_error: 0.1545 - val_loss: 0.0371 - val_mean_absolute_error: 0.1463\n",
            "Epoch 5/7\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0388 - mean_absolute_error: 0.1511 - val_loss: 0.0393 - val_mean_absolute_error: 0.1512\n",
            "Epoch 6/7\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0376 - mean_absolute_error: 0.1488 - val_loss: 0.0356 - val_mean_absolute_error: 0.1441\n",
            "Epoch 7/7\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0358 - mean_absolute_error: 0.1455 - val_loss: 0.0329 - val_mean_absolute_error: 0.1383\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_17 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_18 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/8\n",
            "10381/10381 [==============================] - 5s 479us/step - loss: 0.0727 - mean_absolute_error: 0.2059 - val_loss: 0.0428 - val_mean_absolute_error: 0.1580\n",
            "Epoch 2/8\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0471 - mean_absolute_error: 0.1687 - val_loss: 0.0384 - val_mean_absolute_error: 0.1501\n",
            "Epoch 3/8\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0433 - mean_absolute_error: 0.1604 - val_loss: 0.0371 - val_mean_absolute_error: 0.1489\n",
            "Epoch 4/8\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0403 - mean_absolute_error: 0.1545 - val_loss: 0.0358 - val_mean_absolute_error: 0.1443\n",
            "Epoch 5/8\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0388 - mean_absolute_error: 0.1511 - val_loss: 0.0391 - val_mean_absolute_error: 0.1507\n",
            "Epoch 6/8\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0377 - mean_absolute_error: 0.1492 - val_loss: 0.0367 - val_mean_absolute_error: 0.1455\n",
            "Epoch 7/8\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0362 - mean_absolute_error: 0.1460 - val_loss: 0.0358 - val_mean_absolute_error: 0.1439\n",
            "Epoch 8/8\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0354 - mean_absolute_error: 0.1444 - val_loss: 0.0346 - val_mean_absolute_error: 0.1420\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_19 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/9\n",
            "10381/10381 [==============================] - 6s 532us/step - loss: 0.0722 - mean_absolute_error: 0.2055 - val_loss: 0.0425 - val_mean_absolute_error: 0.1574\n",
            "Epoch 2/9\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0475 - mean_absolute_error: 0.1689 - val_loss: 0.0417 - val_mean_absolute_error: 0.1616\n",
            "Epoch 3/9\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0428 - mean_absolute_error: 0.1601 - val_loss: 0.0420 - val_mean_absolute_error: 0.1558\n",
            "Epoch 4/9\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0407 - mean_absolute_error: 0.1558 - val_loss: 0.0380 - val_mean_absolute_error: 0.1528\n",
            "Epoch 5/9\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0390 - mean_absolute_error: 0.1517 - val_loss: 0.0364 - val_mean_absolute_error: 0.1497\n",
            "Epoch 6/9\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0377 - mean_absolute_error: 0.1492 - val_loss: 0.0355 - val_mean_absolute_error: 0.1474\n",
            "Epoch 7/9\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0365 - mean_absolute_error: 0.1468 - val_loss: 0.0331 - val_mean_absolute_error: 0.1386\n",
            "Epoch 8/9\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0361 - mean_absolute_error: 0.1457 - val_loss: 0.0329 - val_mean_absolute_error: 0.1384\n",
            "Epoch 9/9\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0355 - mean_absolute_error: 0.1442 - val_loss: 0.0356 - val_mean_absolute_error: 0.1447\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_21 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/10\n",
            "10381/10381 [==============================] - 6s 571us/step - loss: 0.0734 - mean_absolute_error: 0.2068 - val_loss: 0.0462 - val_mean_absolute_error: 0.1650\n",
            "Epoch 2/10\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0474 - mean_absolute_error: 0.1694 - val_loss: 0.0397 - val_mean_absolute_error: 0.1537\n",
            "Epoch 3/10\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0442 - mean_absolute_error: 0.1628 - val_loss: 0.0383 - val_mean_absolute_error: 0.1539\n",
            "Epoch 4/10\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0412 - mean_absolute_error: 0.1563 - val_loss: 0.0381 - val_mean_absolute_error: 0.1483\n",
            "Epoch 5/10\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0392 - mean_absolute_error: 0.1522 - val_loss: 0.0364 - val_mean_absolute_error: 0.1478\n",
            "Epoch 6/10\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0380 - mean_absolute_error: 0.1500 - val_loss: 0.0379 - val_mean_absolute_error: 0.1491\n",
            "Epoch 7/10\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0366 - mean_absolute_error: 0.1473 - val_loss: 0.0374 - val_mean_absolute_error: 0.1494\n",
            "Epoch 8/10\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0363 - mean_absolute_error: 0.1459 - val_loss: 0.0331 - val_mean_absolute_error: 0.1394\n",
            "Epoch 9/10\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0352 - mean_absolute_error: 0.1439 - val_loss: 0.0342 - val_mean_absolute_error: 0.1412\n",
            "Epoch 10/10\n",
            "10381/10381 [==============================] - 0s 36us/step - loss: 0.0347 - mean_absolute_error: 0.1428 - val_loss: 0.0346 - val_mean_absolute_error: 0.1420\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_23 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_24 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/11\n",
            "10381/10381 [==============================] - 6s 614us/step - loss: 0.0733 - mean_absolute_error: 0.2086 - val_loss: 0.0498 - val_mean_absolute_error: 0.1806\n",
            "Epoch 2/11\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0474 - mean_absolute_error: 0.1691 - val_loss: 0.0435 - val_mean_absolute_error: 0.1590\n",
            "Epoch 3/11\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0436 - mean_absolute_error: 0.1611 - val_loss: 0.0386 - val_mean_absolute_error: 0.1493\n",
            "Epoch 4/11\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0415 - mean_absolute_error: 0.1571 - val_loss: 0.0366 - val_mean_absolute_error: 0.1494\n",
            "Epoch 5/11\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0390 - mean_absolute_error: 0.1516 - val_loss: 0.0381 - val_mean_absolute_error: 0.1493\n",
            "Epoch 6/11\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0378 - mean_absolute_error: 0.1494 - val_loss: 0.0366 - val_mean_absolute_error: 0.1479\n",
            "Epoch 7/11\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0369 - mean_absolute_error: 0.1472 - val_loss: 0.0351 - val_mean_absolute_error: 0.1461\n",
            "Epoch 8/11\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0360 - mean_absolute_error: 0.1459 - val_loss: 0.0336 - val_mean_absolute_error: 0.1399\n",
            "Epoch 9/11\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0356 - mean_absolute_error: 0.1446 - val_loss: 0.0338 - val_mean_absolute_error: 0.1418\n",
            "Epoch 10/11\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0346 - mean_absolute_error: 0.1431 - val_loss: 0.0324 - val_mean_absolute_error: 0.1375\n",
            "Epoch 11/11\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0343 - mean_absolute_error: 0.1419 - val_loss: 0.0319 - val_mean_absolute_error: 0.1363\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_25 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_26 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/12\n",
            "10381/10381 [==============================] - 7s 651us/step - loss: 0.0734 - mean_absolute_error: 0.2080 - val_loss: 0.0431 - val_mean_absolute_error: 0.1630\n",
            "Epoch 2/12\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0469 - mean_absolute_error: 0.1687 - val_loss: 0.0444 - val_mean_absolute_error: 0.1697\n",
            "Epoch 3/12\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0432 - mean_absolute_error: 0.1607 - val_loss: 0.0370 - val_mean_absolute_error: 0.1478\n",
            "Epoch 4/12\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0409 - mean_absolute_error: 0.1560 - val_loss: 0.0369 - val_mean_absolute_error: 0.1483\n",
            "Epoch 5/12\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0392 - mean_absolute_error: 0.1521 - val_loss: 0.0362 - val_mean_absolute_error: 0.1479\n",
            "Epoch 6/12\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0378 - mean_absolute_error: 0.1493 - val_loss: 0.0354 - val_mean_absolute_error: 0.1434\n",
            "Epoch 7/12\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0363 - mean_absolute_error: 0.1461 - val_loss: 0.0346 - val_mean_absolute_error: 0.1441\n",
            "Epoch 8/12\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0357 - mean_absolute_error: 0.1452 - val_loss: 0.0331 - val_mean_absolute_error: 0.1399\n",
            "Epoch 9/12\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0354 - mean_absolute_error: 0.1445 - val_loss: 0.0324 - val_mean_absolute_error: 0.1371\n",
            "Epoch 10/12\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0342 - mean_absolute_error: 0.1420 - val_loss: 0.0324 - val_mean_absolute_error: 0.1391\n",
            "Epoch 11/12\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0342 - mean_absolute_error: 0.1417 - val_loss: 0.0354 - val_mean_absolute_error: 0.1435\n",
            "Epoch 12/12\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0337 - mean_absolute_error: 0.1411 - val_loss: 0.0318 - val_mean_absolute_error: 0.1365\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_27 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_28 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/13\n",
            "10381/10381 [==============================] - 7s 680us/step - loss: 0.0758 - mean_absolute_error: 0.2115 - val_loss: 0.0435 - val_mean_absolute_error: 0.1651\n",
            "Epoch 2/13\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0479 - mean_absolute_error: 0.1701 - val_loss: 0.0460 - val_mean_absolute_error: 0.1650\n",
            "Epoch 3/13\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0438 - mean_absolute_error: 0.1631 - val_loss: 0.0389 - val_mean_absolute_error: 0.1547\n",
            "Epoch 4/13\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0408 - mean_absolute_error: 0.1563 - val_loss: 0.0398 - val_mean_absolute_error: 0.1527\n",
            "Epoch 5/13\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0401 - mean_absolute_error: 0.1543 - val_loss: 0.0390 - val_mean_absolute_error: 0.1495\n",
            "Epoch 6/13\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0387 - mean_absolute_error: 0.1511 - val_loss: 0.0348 - val_mean_absolute_error: 0.1439\n",
            "Epoch 7/13\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0368 - mean_absolute_error: 0.1472 - val_loss: 0.0346 - val_mean_absolute_error: 0.1420\n",
            "Epoch 8/13\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0361 - mean_absolute_error: 0.1459 - val_loss: 0.0327 - val_mean_absolute_error: 0.1381\n",
            "Epoch 9/13\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0351 - mean_absolute_error: 0.1443 - val_loss: 0.0346 - val_mean_absolute_error: 0.1430\n",
            "Epoch 10/13\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0348 - mean_absolute_error: 0.1436 - val_loss: 0.0327 - val_mean_absolute_error: 0.1390\n",
            "Epoch 11/13\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0339 - mean_absolute_error: 0.1411 - val_loss: 0.0318 - val_mean_absolute_error: 0.1362\n",
            "Epoch 12/13\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0339 - mean_absolute_error: 0.1417 - val_loss: 0.0334 - val_mean_absolute_error: 0.1396\n",
            "Epoch 13/13\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0330 - mean_absolute_error: 0.1400 - val_loss: 0.0350 - val_mean_absolute_error: 0.1446\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_29 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_30 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/14\n",
            "10381/10381 [==============================] - 8s 740us/step - loss: 0.0733 - mean_absolute_error: 0.2080 - val_loss: 0.0435 - val_mean_absolute_error: 0.1640\n",
            "Epoch 2/14\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0477 - mean_absolute_error: 0.1696 - val_loss: 0.0464 - val_mean_absolute_error: 0.1740\n",
            "Epoch 3/14\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0433 - mean_absolute_error: 0.1613 - val_loss: 0.0377 - val_mean_absolute_error: 0.1484\n",
            "Epoch 4/14\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0406 - mean_absolute_error: 0.1555 - val_loss: 0.0363 - val_mean_absolute_error: 0.1465\n",
            "Epoch 5/14\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0396 - mean_absolute_error: 0.1530 - val_loss: 0.0366 - val_mean_absolute_error: 0.1479\n",
            "Epoch 6/14\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0377 - mean_absolute_error: 0.1490 - val_loss: 0.0340 - val_mean_absolute_error: 0.1403\n",
            "Epoch 7/14\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0370 - mean_absolute_error: 0.1477 - val_loss: 0.0338 - val_mean_absolute_error: 0.1405\n",
            "Epoch 8/14\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0362 - mean_absolute_error: 0.1466 - val_loss: 0.0337 - val_mean_absolute_error: 0.1412\n",
            "Epoch 9/14\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0353 - mean_absolute_error: 0.1444 - val_loss: 0.0351 - val_mean_absolute_error: 0.1459\n",
            "Epoch 10/14\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0344 - mean_absolute_error: 0.1426 - val_loss: 0.0333 - val_mean_absolute_error: 0.1397\n",
            "Epoch 11/14\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0339 - mean_absolute_error: 0.1414 - val_loss: 0.0332 - val_mean_absolute_error: 0.1393\n",
            "Epoch 12/14\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0334 - mean_absolute_error: 0.1401 - val_loss: 0.0328 - val_mean_absolute_error: 0.1402\n",
            "Epoch 13/14\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0327 - mean_absolute_error: 0.1392 - val_loss: 0.0340 - val_mean_absolute_error: 0.1421\n",
            "Epoch 14/14\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0328 - mean_absolute_error: 0.1391 - val_loss: 0.0314 - val_mean_absolute_error: 0.1353\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_31 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_32 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/15\n",
            "10381/10381 [==============================] - 8s 777us/step - loss: 0.0746 - mean_absolute_error: 0.2095 - val_loss: 0.0482 - val_mean_absolute_error: 0.1778\n",
            "Epoch 2/15\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0465 - mean_absolute_error: 0.1681 - val_loss: 0.0404 - val_mean_absolute_error: 0.1549\n",
            "Epoch 3/15\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0433 - mean_absolute_error: 0.1605 - val_loss: 0.0403 - val_mean_absolute_error: 0.1591\n",
            "Epoch 4/15\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0416 - mean_absolute_error: 0.1571 - val_loss: 0.0362 - val_mean_absolute_error: 0.1470\n",
            "Epoch 5/15\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0392 - mean_absolute_error: 0.1519 - val_loss: 0.0370 - val_mean_absolute_error: 0.1464\n",
            "Epoch 6/15\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0381 - mean_absolute_error: 0.1500 - val_loss: 0.0344 - val_mean_absolute_error: 0.1409\n",
            "Epoch 7/15\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0368 - mean_absolute_error: 0.1475 - val_loss: 0.0342 - val_mean_absolute_error: 0.1425\n",
            "Epoch 8/15\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0361 - mean_absolute_error: 0.1459 - val_loss: 0.0330 - val_mean_absolute_error: 0.1383\n",
            "Epoch 9/15\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0354 - mean_absolute_error: 0.1446 - val_loss: 0.0399 - val_mean_absolute_error: 0.1525\n",
            "Epoch 10/15\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0351 - mean_absolute_error: 0.1443 - val_loss: 0.0324 - val_mean_absolute_error: 0.1379\n",
            "Epoch 11/15\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0341 - mean_absolute_error: 0.1419 - val_loss: 0.0356 - val_mean_absolute_error: 0.1447\n",
            "Epoch 12/15\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0338 - mean_absolute_error: 0.1412 - val_loss: 0.0323 - val_mean_absolute_error: 0.1370\n",
            "Epoch 13/15\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0335 - mean_absolute_error: 0.1408 - val_loss: 0.0312 - val_mean_absolute_error: 0.1352\n",
            "Epoch 14/15\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0331 - mean_absolute_error: 0.1395 - val_loss: 0.0317 - val_mean_absolute_error: 0.1371\n",
            "Epoch 15/15\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0321 - mean_absolute_error: 0.1386 - val_loss: 0.0327 - val_mean_absolute_error: 0.1382\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_33 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_34 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/16\n",
            "10381/10381 [==============================] - 9s 823us/step - loss: 0.0720 - mean_absolute_error: 0.2065 - val_loss: 0.0423 - val_mean_absolute_error: 0.1596\n",
            "Epoch 2/16\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0475 - mean_absolute_error: 0.1691 - val_loss: 0.0404 - val_mean_absolute_error: 0.1545\n",
            "Epoch 3/16\n",
            "10381/10381 [==============================] - 0s 36us/step - loss: 0.0441 - mean_absolute_error: 0.1629 - val_loss: 0.0365 - val_mean_absolute_error: 0.1463\n",
            "Epoch 4/16\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0407 - mean_absolute_error: 0.1556 - val_loss: 0.0360 - val_mean_absolute_error: 0.1482\n",
            "Epoch 5/16\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0386 - mean_absolute_error: 0.1510 - val_loss: 0.0355 - val_mean_absolute_error: 0.1446\n",
            "Epoch 6/16\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0381 - mean_absolute_error: 0.1503 - val_loss: 0.0345 - val_mean_absolute_error: 0.1414\n",
            "Epoch 7/16\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0373 - mean_absolute_error: 0.1487 - val_loss: 0.0338 - val_mean_absolute_error: 0.1398\n",
            "Epoch 8/16\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0357 - mean_absolute_error: 0.1453 - val_loss: 0.0392 - val_mean_absolute_error: 0.1548\n",
            "Epoch 9/16\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0350 - mean_absolute_error: 0.1437 - val_loss: 0.0329 - val_mean_absolute_error: 0.1380\n",
            "Epoch 10/16\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0346 - mean_absolute_error: 0.1428 - val_loss: 0.0333 - val_mean_absolute_error: 0.1390\n",
            "Epoch 11/16\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0341 - mean_absolute_error: 0.1416 - val_loss: 0.0339 - val_mean_absolute_error: 0.1422\n",
            "Epoch 12/16\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0336 - mean_absolute_error: 0.1405 - val_loss: 0.0325 - val_mean_absolute_error: 0.1375\n",
            "Epoch 13/16\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0330 - mean_absolute_error: 0.1398 - val_loss: 0.0324 - val_mean_absolute_error: 0.1386\n",
            "Epoch 14/16\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0326 - mean_absolute_error: 0.1389 - val_loss: 0.0309 - val_mean_absolute_error: 0.1345\n",
            "Epoch 15/16\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0318 - mean_absolute_error: 0.1375 - val_loss: 0.0308 - val_mean_absolute_error: 0.1336\n",
            "Epoch 16/16\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0318 - mean_absolute_error: 0.1372 - val_loss: 0.0309 - val_mean_absolute_error: 0.1342\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_35 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_36 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/17\n",
            "10381/10381 [==============================] - 9s 856us/step - loss: 0.0729 - mean_absolute_error: 0.2073 - val_loss: 0.0425 - val_mean_absolute_error: 0.1588\n",
            "Epoch 2/17\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0479 - mean_absolute_error: 0.1701 - val_loss: 0.0459 - val_mean_absolute_error: 0.1738\n",
            "Epoch 3/17\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0434 - mean_absolute_error: 0.1612 - val_loss: 0.0406 - val_mean_absolute_error: 0.1546\n",
            "Epoch 4/17\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0407 - mean_absolute_error: 0.1556 - val_loss: 0.0365 - val_mean_absolute_error: 0.1468\n",
            "Epoch 5/17\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0387 - mean_absolute_error: 0.1512 - val_loss: 0.0368 - val_mean_absolute_error: 0.1458\n",
            "Epoch 6/17\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0378 - mean_absolute_error: 0.1492 - val_loss: 0.0345 - val_mean_absolute_error: 0.1421\n",
            "Epoch 7/17\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0365 - mean_absolute_error: 0.1466 - val_loss: 0.0353 - val_mean_absolute_error: 0.1467\n",
            "Epoch 8/17\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0361 - mean_absolute_error: 0.1463 - val_loss: 0.0358 - val_mean_absolute_error: 0.1441\n",
            "Epoch 9/17\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0348 - mean_absolute_error: 0.1435 - val_loss: 0.0335 - val_mean_absolute_error: 0.1392\n",
            "Epoch 10/17\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0344 - mean_absolute_error: 0.1426 - val_loss: 0.0342 - val_mean_absolute_error: 0.1429\n",
            "Epoch 11/17\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0339 - mean_absolute_error: 0.1412 - val_loss: 0.0325 - val_mean_absolute_error: 0.1381\n",
            "Epoch 12/17\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0338 - mean_absolute_error: 0.1411 - val_loss: 0.0314 - val_mean_absolute_error: 0.1349\n",
            "Epoch 13/17\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0326 - mean_absolute_error: 0.1387 - val_loss: 0.0325 - val_mean_absolute_error: 0.1396\n",
            "Epoch 14/17\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0333 - mean_absolute_error: 0.1400 - val_loss: 0.0341 - val_mean_absolute_error: 0.1407\n",
            "Epoch 15/17\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0321 - mean_absolute_error: 0.1376 - val_loss: 0.0311 - val_mean_absolute_error: 0.1332\n",
            "Epoch 16/17\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0319 - mean_absolute_error: 0.1373 - val_loss: 0.0304 - val_mean_absolute_error: 0.1335\n",
            "Epoch 17/17\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0314 - mean_absolute_error: 0.1362 - val_loss: 0.0301 - val_mean_absolute_error: 0.1326\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_37 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_38 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/18\n",
            "10381/10381 [==============================] - 9s 902us/step - loss: 0.0744 - mean_absolute_error: 0.2089 - val_loss: 0.0454 - val_mean_absolute_error: 0.1611\n",
            "Epoch 2/18\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0477 - mean_absolute_error: 0.1699 - val_loss: 0.0428 - val_mean_absolute_error: 0.1669\n",
            "Epoch 3/18\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0439 - mean_absolute_error: 0.1618 - val_loss: 0.0387 - val_mean_absolute_error: 0.1492\n",
            "Epoch 4/18\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0407 - mean_absolute_error: 0.1558 - val_loss: 0.0367 - val_mean_absolute_error: 0.1492\n",
            "Epoch 5/18\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0395 - mean_absolute_error: 0.1526 - val_loss: 0.0386 - val_mean_absolute_error: 0.1499\n",
            "Epoch 6/18\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0378 - mean_absolute_error: 0.1488 - val_loss: 0.0350 - val_mean_absolute_error: 0.1432\n",
            "Epoch 7/18\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0373 - mean_absolute_error: 0.1483 - val_loss: 0.0353 - val_mean_absolute_error: 0.1449\n",
            "Epoch 8/18\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0359 - mean_absolute_error: 0.1452 - val_loss: 0.0339 - val_mean_absolute_error: 0.1413\n",
            "Epoch 9/18\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0353 - mean_absolute_error: 0.1442 - val_loss: 0.0327 - val_mean_absolute_error: 0.1383\n",
            "Epoch 10/18\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0348 - mean_absolute_error: 0.1432 - val_loss: 0.0353 - val_mean_absolute_error: 0.1442\n",
            "Epoch 11/18\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0338 - mean_absolute_error: 0.1416 - val_loss: 0.0335 - val_mean_absolute_error: 0.1401\n",
            "Epoch 12/18\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0338 - mean_absolute_error: 0.1409 - val_loss: 0.0312 - val_mean_absolute_error: 0.1350\n",
            "Epoch 13/18\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0331 - mean_absolute_error: 0.1400 - val_loss: 0.0322 - val_mean_absolute_error: 0.1364\n",
            "Epoch 14/18\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0326 - mean_absolute_error: 0.1383 - val_loss: 0.0308 - val_mean_absolute_error: 0.1339\n",
            "Epoch 15/18\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0324 - mean_absolute_error: 0.1384 - val_loss: 0.0310 - val_mean_absolute_error: 0.1346\n",
            "Epoch 16/18\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0325 - mean_absolute_error: 0.1386 - val_loss: 0.0326 - val_mean_absolute_error: 0.1376\n",
            "Epoch 17/18\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0316 - mean_absolute_error: 0.1363 - val_loss: 0.0333 - val_mean_absolute_error: 0.1419\n",
            "Epoch 18/18\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0317 - mean_absolute_error: 0.1365 - val_loss: 0.0310 - val_mean_absolute_error: 0.1345\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_39 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_40 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/19\n",
            "10381/10381 [==============================] - 10s 955us/step - loss: 0.0744 - mean_absolute_error: 0.2084 - val_loss: 0.0472 - val_mean_absolute_error: 0.1647\n",
            "Epoch 2/19\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0473 - mean_absolute_error: 0.1691 - val_loss: 0.0416 - val_mean_absolute_error: 0.1550\n",
            "Epoch 3/19\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0435 - mean_absolute_error: 0.1614 - val_loss: 0.0403 - val_mean_absolute_error: 0.1526\n",
            "Epoch 4/19\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0413 - mean_absolute_error: 0.1567 - val_loss: 0.0369 - val_mean_absolute_error: 0.1477\n",
            "Epoch 5/19\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0394 - mean_absolute_error: 0.1522 - val_loss: 0.0370 - val_mean_absolute_error: 0.1484\n",
            "Epoch 6/19\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0379 - mean_absolute_error: 0.1502 - val_loss: 0.0352 - val_mean_absolute_error: 0.1439\n",
            "Epoch 7/19\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0372 - mean_absolute_error: 0.1488 - val_loss: 0.0342 - val_mean_absolute_error: 0.1436\n",
            "Epoch 8/19\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0358 - mean_absolute_error: 0.1453 - val_loss: 0.0339 - val_mean_absolute_error: 0.1401\n",
            "Epoch 9/19\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0358 - mean_absolute_error: 0.1457 - val_loss: 0.0346 - val_mean_absolute_error: 0.1422\n",
            "Epoch 10/19\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0342 - mean_absolute_error: 0.1415 - val_loss: 0.0319 - val_mean_absolute_error: 0.1366\n",
            "Epoch 11/19\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0341 - mean_absolute_error: 0.1415 - val_loss: 0.0325 - val_mean_absolute_error: 0.1381\n",
            "Epoch 12/19\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0335 - mean_absolute_error: 0.1409 - val_loss: 0.0325 - val_mean_absolute_error: 0.1385\n",
            "Epoch 13/19\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0331 - mean_absolute_error: 0.1401 - val_loss: 0.0320 - val_mean_absolute_error: 0.1373\n",
            "Epoch 14/19\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0331 - mean_absolute_error: 0.1394 - val_loss: 0.0307 - val_mean_absolute_error: 0.1344\n",
            "Epoch 15/19\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0323 - mean_absolute_error: 0.1381 - val_loss: 0.0312 - val_mean_absolute_error: 0.1355\n",
            "Epoch 16/19\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0320 - mean_absolute_error: 0.1376 - val_loss: 0.0312 - val_mean_absolute_error: 0.1356\n",
            "Epoch 17/19\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0313 - mean_absolute_error: 0.1355 - val_loss: 0.0331 - val_mean_absolute_error: 0.1399\n",
            "Epoch 18/19\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0312 - mean_absolute_error: 0.1351 - val_loss: 0.0360 - val_mean_absolute_error: 0.1426\n",
            "Epoch 19/19\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0303 - mean_absolute_error: 0.1339 - val_loss: 0.0301 - val_mean_absolute_error: 0.1328\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_41 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_42 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/20\n",
            "10381/10381 [==============================] - 11s 1ms/step - loss: 0.0736 - mean_absolute_error: 0.2081 - val_loss: 0.0487 - val_mean_absolute_error: 0.1761\n",
            "Epoch 2/20\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0471 - mean_absolute_error: 0.1689 - val_loss: 0.0420 - val_mean_absolute_error: 0.1625\n",
            "Epoch 3/20\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0432 - mean_absolute_error: 0.1609 - val_loss: 0.0393 - val_mean_absolute_error: 0.1513\n",
            "Epoch 4/20\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0408 - mean_absolute_error: 0.1561 - val_loss: 0.0371 - val_mean_absolute_error: 0.1483\n",
            "Epoch 5/20\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0395 - mean_absolute_error: 0.1526 - val_loss: 0.0354 - val_mean_absolute_error: 0.1438\n",
            "Epoch 6/20\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0374 - mean_absolute_error: 0.1482 - val_loss: 0.0346 - val_mean_absolute_error: 0.1422\n",
            "Epoch 7/20\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0375 - mean_absolute_error: 0.1485 - val_loss: 0.0352 - val_mean_absolute_error: 0.1431\n",
            "Epoch 8/20\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0359 - mean_absolute_error: 0.1458 - val_loss: 0.0342 - val_mean_absolute_error: 0.1407\n",
            "Epoch 9/20\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0352 - mean_absolute_error: 0.1440 - val_loss: 0.0345 - val_mean_absolute_error: 0.1421\n",
            "Epoch 10/20\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0347 - mean_absolute_error: 0.1431 - val_loss: 0.0335 - val_mean_absolute_error: 0.1396\n",
            "Epoch 11/20\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0338 - mean_absolute_error: 0.1412 - val_loss: 0.0322 - val_mean_absolute_error: 0.1360\n",
            "Epoch 12/20\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0336 - mean_absolute_error: 0.1412 - val_loss: 0.0325 - val_mean_absolute_error: 0.1374\n",
            "Epoch 13/20\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0331 - mean_absolute_error: 0.1398 - val_loss: 0.0309 - val_mean_absolute_error: 0.1341\n",
            "Epoch 14/20\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0328 - mean_absolute_error: 0.1390 - val_loss: 0.0326 - val_mean_absolute_error: 0.1400\n",
            "Epoch 15/20\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0322 - mean_absolute_error: 0.1380 - val_loss: 0.0316 - val_mean_absolute_error: 0.1379\n",
            "Epoch 16/20\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0316 - mean_absolute_error: 0.1367 - val_loss: 0.0311 - val_mean_absolute_error: 0.1355\n",
            "Epoch 17/20\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0313 - mean_absolute_error: 0.1361 - val_loss: 0.0333 - val_mean_absolute_error: 0.1393\n",
            "Epoch 18/20\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0311 - mean_absolute_error: 0.1360 - val_loss: 0.0303 - val_mean_absolute_error: 0.1341\n",
            "Epoch 19/20\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0307 - mean_absolute_error: 0.1345 - val_loss: 0.0295 - val_mean_absolute_error: 0.1305\n",
            "Epoch 20/20\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0303 - mean_absolute_error: 0.1332 - val_loss: 0.0293 - val_mean_absolute_error: 0.1309\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_43 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_44 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/21\n",
            "10381/10381 [==============================] - 11s 1ms/step - loss: 0.0742 - mean_absolute_error: 0.2098 - val_loss: 0.0437 - val_mean_absolute_error: 0.1621\n",
            "Epoch 2/21\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0462 - mean_absolute_error: 0.1673 - val_loss: 0.0507 - val_mean_absolute_error: 0.1721\n",
            "Epoch 3/21\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0436 - mean_absolute_error: 0.1611 - val_loss: 0.0410 - val_mean_absolute_error: 0.1547\n",
            "Epoch 4/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0409 - mean_absolute_error: 0.1561 - val_loss: 0.0386 - val_mean_absolute_error: 0.1545\n",
            "Epoch 5/21\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0393 - mean_absolute_error: 0.1526 - val_loss: 0.0363 - val_mean_absolute_error: 0.1461\n",
            "Epoch 6/21\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0375 - mean_absolute_error: 0.1482 - val_loss: 0.0359 - val_mean_absolute_error: 0.1437\n",
            "Epoch 7/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0371 - mean_absolute_error: 0.1479 - val_loss: 0.0342 - val_mean_absolute_error: 0.1409\n",
            "Epoch 8/21\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0364 - mean_absolute_error: 0.1460 - val_loss: 0.0367 - val_mean_absolute_error: 0.1462\n",
            "Epoch 9/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0354 - mean_absolute_error: 0.1446 - val_loss: 0.0328 - val_mean_absolute_error: 0.1401\n",
            "Epoch 10/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0347 - mean_absolute_error: 0.1426 - val_loss: 0.0331 - val_mean_absolute_error: 0.1389\n",
            "Epoch 11/21\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0339 - mean_absolute_error: 0.1413 - val_loss: 0.0353 - val_mean_absolute_error: 0.1473\n",
            "Epoch 12/21\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0339 - mean_absolute_error: 0.1418 - val_loss: 0.0320 - val_mean_absolute_error: 0.1360\n",
            "Epoch 13/21\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0330 - mean_absolute_error: 0.1398 - val_loss: 0.0369 - val_mean_absolute_error: 0.1516\n",
            "Epoch 14/21\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0330 - mean_absolute_error: 0.1397 - val_loss: 0.0322 - val_mean_absolute_error: 0.1392\n",
            "Epoch 15/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0324 - mean_absolute_error: 0.1384 - val_loss: 0.0312 - val_mean_absolute_error: 0.1347\n",
            "Epoch 16/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0319 - mean_absolute_error: 0.1376 - val_loss: 0.0309 - val_mean_absolute_error: 0.1344\n",
            "Epoch 17/21\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0313 - mean_absolute_error: 0.1357 - val_loss: 0.0318 - val_mean_absolute_error: 0.1365\n",
            "Epoch 18/21\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0312 - mean_absolute_error: 0.1355 - val_loss: 0.0312 - val_mean_absolute_error: 0.1363\n",
            "Epoch 19/21\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0308 - mean_absolute_error: 0.1351 - val_loss: 0.0295 - val_mean_absolute_error: 0.1313\n",
            "Epoch 20/21\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0305 - mean_absolute_error: 0.1341 - val_loss: 0.0317 - val_mean_absolute_error: 0.1356\n",
            "Epoch 21/21\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0306 - mean_absolute_error: 0.1339 - val_loss: 0.0303 - val_mean_absolute_error: 0.1348\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_45 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_46 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/22\n",
            "10381/10381 [==============================] - 11s 1ms/step - loss: 0.0718 - mean_absolute_error: 0.2055 - val_loss: 0.0513 - val_mean_absolute_error: 0.1887\n",
            "Epoch 2/22\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0476 - mean_absolute_error: 0.1698 - val_loss: 0.0397 - val_mean_absolute_error: 0.1572\n",
            "Epoch 3/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0438 - mean_absolute_error: 0.1626 - val_loss: 0.0372 - val_mean_absolute_error: 0.1473\n",
            "Epoch 4/22\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0414 - mean_absolute_error: 0.1568 - val_loss: 0.0383 - val_mean_absolute_error: 0.1492\n",
            "Epoch 5/22\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0391 - mean_absolute_error: 0.1522 - val_loss: 0.0365 - val_mean_absolute_error: 0.1458\n",
            "Epoch 6/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0380 - mean_absolute_error: 0.1497 - val_loss: 0.0347 - val_mean_absolute_error: 0.1423\n",
            "Epoch 7/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0370 - mean_absolute_error: 0.1482 - val_loss: 0.0331 - val_mean_absolute_error: 0.1383\n",
            "Epoch 8/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0367 - mean_absolute_error: 0.1471 - val_loss: 0.0390 - val_mean_absolute_error: 0.1512\n",
            "Epoch 9/22\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0349 - mean_absolute_error: 0.1433 - val_loss: 0.0377 - val_mean_absolute_error: 0.1504\n",
            "Epoch 10/22\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0349 - mean_absolute_error: 0.1440 - val_loss: 0.0339 - val_mean_absolute_error: 0.1408\n",
            "Epoch 11/22\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0338 - mean_absolute_error: 0.1414 - val_loss: 0.0348 - val_mean_absolute_error: 0.1438\n",
            "Epoch 12/22\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0336 - mean_absolute_error: 0.1411 - val_loss: 0.0318 - val_mean_absolute_error: 0.1363\n",
            "Epoch 13/22\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0334 - mean_absolute_error: 0.1405 - val_loss: 0.0342 - val_mean_absolute_error: 0.1418\n",
            "Epoch 14/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0329 - mean_absolute_error: 0.1396 - val_loss: 0.0316 - val_mean_absolute_error: 0.1360\n",
            "Epoch 15/22\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0321 - mean_absolute_error: 0.1373 - val_loss: 0.0316 - val_mean_absolute_error: 0.1355\n",
            "Epoch 16/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0316 - mean_absolute_error: 0.1368 - val_loss: 0.0318 - val_mean_absolute_error: 0.1363\n",
            "Epoch 17/22\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0323 - mean_absolute_error: 0.1381 - val_loss: 0.0326 - val_mean_absolute_error: 0.1415\n",
            "Epoch 18/22\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0310 - mean_absolute_error: 0.1355 - val_loss: 0.0300 - val_mean_absolute_error: 0.1322\n",
            "Epoch 19/22\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0309 - mean_absolute_error: 0.1345 - val_loss: 0.0307 - val_mean_absolute_error: 0.1336\n",
            "Epoch 20/22\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0310 - mean_absolute_error: 0.1353 - val_loss: 0.0307 - val_mean_absolute_error: 0.1349\n",
            "Epoch 21/22\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0302 - mean_absolute_error: 0.1339 - val_loss: 0.0319 - val_mean_absolute_error: 0.1391\n",
            "Epoch 22/22\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0298 - mean_absolute_error: 0.1329 - val_loss: 0.0299 - val_mean_absolute_error: 0.1320\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_47 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_48 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/23\n",
            "10381/10381 [==============================] - 12s 1ms/step - loss: 0.0715 - mean_absolute_error: 0.2064 - val_loss: 0.0441 - val_mean_absolute_error: 0.1634\n",
            "Epoch 2/23\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0474 - mean_absolute_error: 0.1699 - val_loss: 0.0405 - val_mean_absolute_error: 0.1583\n",
            "Epoch 3/23\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0434 - mean_absolute_error: 0.1614 - val_loss: 0.0368 - val_mean_absolute_error: 0.1466\n",
            "Epoch 4/23\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0418 - mean_absolute_error: 0.1576 - val_loss: 0.0376 - val_mean_absolute_error: 0.1519\n",
            "Epoch 5/23\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0394 - mean_absolute_error: 0.1526 - val_loss: 0.0369 - val_mean_absolute_error: 0.1513\n",
            "Epoch 6/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0378 - mean_absolute_error: 0.1497 - val_loss: 0.0344 - val_mean_absolute_error: 0.1414\n",
            "Epoch 7/23\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0367 - mean_absolute_error: 0.1473 - val_loss: 0.0338 - val_mean_absolute_error: 0.1410\n",
            "Epoch 8/23\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0362 - mean_absolute_error: 0.1459 - val_loss: 0.0333 - val_mean_absolute_error: 0.1389\n",
            "Epoch 9/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0354 - mean_absolute_error: 0.1447 - val_loss: 0.0342 - val_mean_absolute_error: 0.1412\n",
            "Epoch 10/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0342 - mean_absolute_error: 0.1419 - val_loss: 0.0333 - val_mean_absolute_error: 0.1394\n",
            "Epoch 11/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0341 - mean_absolute_error: 0.1425 - val_loss: 0.0321 - val_mean_absolute_error: 0.1373\n",
            "Epoch 12/23\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0341 - mean_absolute_error: 0.1420 - val_loss: 0.0334 - val_mean_absolute_error: 0.1402\n",
            "Epoch 13/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0332 - mean_absolute_error: 0.1399 - val_loss: 0.0326 - val_mean_absolute_error: 0.1401\n",
            "Epoch 14/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0324 - mean_absolute_error: 0.1386 - val_loss: 0.0313 - val_mean_absolute_error: 0.1352\n",
            "Epoch 15/23\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0323 - mean_absolute_error: 0.1375 - val_loss: 0.0320 - val_mean_absolute_error: 0.1386\n",
            "Epoch 16/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0320 - mean_absolute_error: 0.1375 - val_loss: 0.0307 - val_mean_absolute_error: 0.1342\n",
            "Epoch 17/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0315 - mean_absolute_error: 0.1369 - val_loss: 0.0314 - val_mean_absolute_error: 0.1364\n",
            "Epoch 18/23\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0318 - mean_absolute_error: 0.1371 - val_loss: 0.0305 - val_mean_absolute_error: 0.1337\n",
            "Epoch 19/23\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0305 - mean_absolute_error: 0.1340 - val_loss: 0.0312 - val_mean_absolute_error: 0.1340\n",
            "Epoch 20/23\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0308 - mean_absolute_error: 0.1344 - val_loss: 0.0309 - val_mean_absolute_error: 0.1355\n",
            "Epoch 21/23\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0301 - mean_absolute_error: 0.1334 - val_loss: 0.0296 - val_mean_absolute_error: 0.1317\n",
            "Epoch 22/23\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0297 - mean_absolute_error: 0.1325 - val_loss: 0.0299 - val_mean_absolute_error: 0.1328\n",
            "Epoch 23/23\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0296 - mean_absolute_error: 0.1325 - val_loss: 0.0315 - val_mean_absolute_error: 0.1370\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_49 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_50 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/24\n",
            "10381/10381 [==============================] - 12s 1ms/step - loss: 0.0750 - mean_absolute_error: 0.2103 - val_loss: 0.0452 - val_mean_absolute_error: 0.1696\n",
            "Epoch 2/24\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0474 - mean_absolute_error: 0.1695 - val_loss: 0.0391 - val_mean_absolute_error: 0.1497\n",
            "Epoch 3/24\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0439 - mean_absolute_error: 0.1621 - val_loss: 0.0396 - val_mean_absolute_error: 0.1553\n",
            "Epoch 4/24\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0408 - mean_absolute_error: 0.1554 - val_loss: 0.0367 - val_mean_absolute_error: 0.1471\n",
            "Epoch 5/24\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0394 - mean_absolute_error: 0.1524 - val_loss: 0.0363 - val_mean_absolute_error: 0.1446\n",
            "Epoch 6/24\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0387 - mean_absolute_error: 0.1511 - val_loss: 0.0339 - val_mean_absolute_error: 0.1409\n",
            "Epoch 7/24\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0365 - mean_absolute_error: 0.1468 - val_loss: 0.0338 - val_mean_absolute_error: 0.1406\n",
            "Epoch 8/24\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0355 - mean_absolute_error: 0.1452 - val_loss: 0.0345 - val_mean_absolute_error: 0.1417\n",
            "Epoch 9/24\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0355 - mean_absolute_error: 0.1448 - val_loss: 0.0339 - val_mean_absolute_error: 0.1404\n",
            "Epoch 10/24\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0344 - mean_absolute_error: 0.1421 - val_loss: 0.0337 - val_mean_absolute_error: 0.1407\n",
            "Epoch 11/24\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0337 - mean_absolute_error: 0.1413 - val_loss: 0.0313 - val_mean_absolute_error: 0.1354\n",
            "Epoch 12/24\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0335 - mean_absolute_error: 0.1405 - val_loss: 0.0335 - val_mean_absolute_error: 0.1395\n",
            "Epoch 13/24\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0329 - mean_absolute_error: 0.1391 - val_loss: 0.0324 - val_mean_absolute_error: 0.1377\n",
            "Epoch 14/24\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0325 - mean_absolute_error: 0.1384 - val_loss: 0.0310 - val_mean_absolute_error: 0.1341\n",
            "Epoch 15/24\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0322 - mean_absolute_error: 0.1376 - val_loss: 0.0313 - val_mean_absolute_error: 0.1345\n",
            "Epoch 16/24\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0315 - mean_absolute_error: 0.1364 - val_loss: 0.0306 - val_mean_absolute_error: 0.1329\n",
            "Epoch 17/24\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0312 - mean_absolute_error: 0.1360 - val_loss: 0.0301 - val_mean_absolute_error: 0.1324\n",
            "Epoch 18/24\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0311 - mean_absolute_error: 0.1352 - val_loss: 0.0325 - val_mean_absolute_error: 0.1383\n",
            "Epoch 19/24\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0306 - mean_absolute_error: 0.1340 - val_loss: 0.0321 - val_mean_absolute_error: 0.1365\n",
            "Epoch 20/24\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0301 - mean_absolute_error: 0.1329 - val_loss: 0.0294 - val_mean_absolute_error: 0.1305\n",
            "Epoch 21/24\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0300 - mean_absolute_error: 0.1335 - val_loss: 0.0302 - val_mean_absolute_error: 0.1317\n",
            "Epoch 22/24\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0301 - mean_absolute_error: 0.1332 - val_loss: 0.0309 - val_mean_absolute_error: 0.1340\n",
            "Epoch 23/24\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0299 - mean_absolute_error: 0.1331 - val_loss: 0.0301 - val_mean_absolute_error: 0.1325\n",
            "Epoch 24/24\n",
            "10381/10381 [==============================] - 1s 50us/step - loss: 0.0294 - mean_absolute_error: 0.1314 - val_loss: 0.0300 - val_mean_absolute_error: 0.1319\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_51 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_52 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/25\n",
            "10381/10381 [==============================] - 12s 1ms/step - loss: 0.0738 - mean_absolute_error: 0.2086 - val_loss: 0.0427 - val_mean_absolute_error: 0.1584\n",
            "Epoch 2/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0474 - mean_absolute_error: 0.1696 - val_loss: 0.0424 - val_mean_absolute_error: 0.1577\n",
            "Epoch 3/25\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0433 - mean_absolute_error: 0.1605 - val_loss: 0.0372 - val_mean_absolute_error: 0.1476\n",
            "Epoch 4/25\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0404 - mean_absolute_error: 0.1551 - val_loss: 0.0371 - val_mean_absolute_error: 0.1476\n",
            "Epoch 5/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0383 - mean_absolute_error: 0.1507 - val_loss: 0.0360 - val_mean_absolute_error: 0.1444\n",
            "Epoch 6/25\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0375 - mean_absolute_error: 0.1490 - val_loss: 0.0349 - val_mean_absolute_error: 0.1422\n",
            "Epoch 7/25\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0368 - mean_absolute_error: 0.1471 - val_loss: 0.0341 - val_mean_absolute_error: 0.1430\n",
            "Epoch 8/25\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0356 - mean_absolute_error: 0.1443 - val_loss: 0.0347 - val_mean_absolute_error: 0.1456\n",
            "Epoch 9/25\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0348 - mean_absolute_error: 0.1428 - val_loss: 0.0343 - val_mean_absolute_error: 0.1420\n",
            "Epoch 10/25\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0341 - mean_absolute_error: 0.1424 - val_loss: 0.0329 - val_mean_absolute_error: 0.1406\n",
            "Epoch 11/25\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0335 - mean_absolute_error: 0.1404 - val_loss: 0.0325 - val_mean_absolute_error: 0.1378\n",
            "Epoch 12/25\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0337 - mean_absolute_error: 0.1404 - val_loss: 0.0316 - val_mean_absolute_error: 0.1358\n",
            "Epoch 13/25\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0328 - mean_absolute_error: 0.1390 - val_loss: 0.0314 - val_mean_absolute_error: 0.1357\n",
            "Epoch 14/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0320 - mean_absolute_error: 0.1377 - val_loss: 0.0319 - val_mean_absolute_error: 0.1367\n",
            "Epoch 15/25\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0317 - mean_absolute_error: 0.1365 - val_loss: 0.0329 - val_mean_absolute_error: 0.1381\n",
            "Epoch 16/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0313 - mean_absolute_error: 0.1359 - val_loss: 0.0328 - val_mean_absolute_error: 0.1396\n",
            "Epoch 17/25\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0315 - mean_absolute_error: 0.1360 - val_loss: 0.0296 - val_mean_absolute_error: 0.1312\n",
            "Epoch 18/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0312 - mean_absolute_error: 0.1353 - val_loss: 0.0298 - val_mean_absolute_error: 0.1320\n",
            "Epoch 19/25\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0309 - mean_absolute_error: 0.1352 - val_loss: 0.0307 - val_mean_absolute_error: 0.1337\n",
            "Epoch 20/25\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0306 - mean_absolute_error: 0.1348 - val_loss: 0.0297 - val_mean_absolute_error: 0.1315\n",
            "Epoch 21/25\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0298 - mean_absolute_error: 0.1328 - val_loss: 0.0295 - val_mean_absolute_error: 0.1312\n",
            "Epoch 22/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0298 - mean_absolute_error: 0.1323 - val_loss: 0.0301 - val_mean_absolute_error: 0.1325\n",
            "Epoch 23/25\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0295 - mean_absolute_error: 0.1321 - val_loss: 0.0288 - val_mean_absolute_error: 0.1294\n",
            "Epoch 24/25\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0288 - mean_absolute_error: 0.1301 - val_loss: 0.0288 - val_mean_absolute_error: 0.1297\n",
            "Epoch 25/25\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0288 - mean_absolute_error: 0.1305 - val_loss: 0.0292 - val_mean_absolute_error: 0.1305\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_53 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_54 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/26\n",
            "10381/10381 [==============================] - 13s 1ms/step - loss: 0.0750 - mean_absolute_error: 0.2094 - val_loss: 0.0434 - val_mean_absolute_error: 0.1637\n",
            "Epoch 2/26\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0468 - mean_absolute_error: 0.1680 - val_loss: 0.0427 - val_mean_absolute_error: 0.1606\n",
            "Epoch 3/26\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0437 - mean_absolute_error: 0.1620 - val_loss: 0.0377 - val_mean_absolute_error: 0.1474\n",
            "Epoch 4/26\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0404 - mean_absolute_error: 0.1546 - val_loss: 0.0366 - val_mean_absolute_error: 0.1457\n",
            "Epoch 5/26\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0396 - mean_absolute_error: 0.1532 - val_loss: 0.0352 - val_mean_absolute_error: 0.1432\n",
            "Epoch 6/26\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0373 - mean_absolute_error: 0.1482 - val_loss: 0.0350 - val_mean_absolute_error: 0.1431\n",
            "Epoch 7/26\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0369 - mean_absolute_error: 0.1475 - val_loss: 0.0339 - val_mean_absolute_error: 0.1404\n",
            "Epoch 8/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0360 - mean_absolute_error: 0.1456 - val_loss: 0.0337 - val_mean_absolute_error: 0.1408\n",
            "Epoch 9/26\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0358 - mean_absolute_error: 0.1450 - val_loss: 0.0335 - val_mean_absolute_error: 0.1416\n",
            "Epoch 10/26\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0345 - mean_absolute_error: 0.1427 - val_loss: 0.0349 - val_mean_absolute_error: 0.1422\n",
            "Epoch 11/26\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0341 - mean_absolute_error: 0.1417 - val_loss: 0.0329 - val_mean_absolute_error: 0.1400\n",
            "Epoch 12/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0331 - mean_absolute_error: 0.1404 - val_loss: 0.0321 - val_mean_absolute_error: 0.1366\n",
            "Epoch 13/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0327 - mean_absolute_error: 0.1388 - val_loss: 0.0310 - val_mean_absolute_error: 0.1344\n",
            "Epoch 14/26\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0330 - mean_absolute_error: 0.1397 - val_loss: 0.0330 - val_mean_absolute_error: 0.1407\n",
            "Epoch 15/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0319 - mean_absolute_error: 0.1372 - val_loss: 0.0325 - val_mean_absolute_error: 0.1367\n",
            "Epoch 16/26\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0318 - mean_absolute_error: 0.1372 - val_loss: 0.0328 - val_mean_absolute_error: 0.1402\n",
            "Epoch 17/26\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0317 - mean_absolute_error: 0.1369 - val_loss: 0.0311 - val_mean_absolute_error: 0.1351\n",
            "Epoch 18/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0310 - mean_absolute_error: 0.1351 - val_loss: 0.0332 - val_mean_absolute_error: 0.1398\n",
            "Epoch 19/26\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0309 - mean_absolute_error: 0.1352 - val_loss: 0.0296 - val_mean_absolute_error: 0.1316\n",
            "Epoch 20/26\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0308 - mean_absolute_error: 0.1347 - val_loss: 0.0310 - val_mean_absolute_error: 0.1341\n",
            "Epoch 21/26\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0304 - mean_absolute_error: 0.1335 - val_loss: 0.0296 - val_mean_absolute_error: 0.1308\n",
            "Epoch 22/26\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0295 - mean_absolute_error: 0.1318 - val_loss: 0.0318 - val_mean_absolute_error: 0.1365\n",
            "Epoch 23/26\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0301 - mean_absolute_error: 0.1329 - val_loss: 0.0288 - val_mean_absolute_error: 0.1298\n",
            "Epoch 24/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0299 - mean_absolute_error: 0.1327 - val_loss: 0.0291 - val_mean_absolute_error: 0.1305\n",
            "Epoch 25/26\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0289 - mean_absolute_error: 0.1303 - val_loss: 0.0311 - val_mean_absolute_error: 0.1342\n",
            "Epoch 26/26\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0287 - mean_absolute_error: 0.1300 - val_loss: 0.0286 - val_mean_absolute_error: 0.1283\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_55 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_56 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/27\n",
            "10381/10381 [==============================] - 13s 1ms/step - loss: 0.0731 - mean_absolute_error: 0.2086 - val_loss: 0.0435 - val_mean_absolute_error: 0.1609\n",
            "Epoch 2/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0473 - mean_absolute_error: 0.1689 - val_loss: 0.0422 - val_mean_absolute_error: 0.1638\n",
            "Epoch 3/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0429 - mean_absolute_error: 0.1596 - val_loss: 0.0399 - val_mean_absolute_error: 0.1570\n",
            "Epoch 4/27\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0404 - mean_absolute_error: 0.1557 - val_loss: 0.0365 - val_mean_absolute_error: 0.1461\n",
            "Epoch 5/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0390 - mean_absolute_error: 0.1516 - val_loss: 0.0365 - val_mean_absolute_error: 0.1491\n",
            "Epoch 6/27\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0371 - mean_absolute_error: 0.1477 - val_loss: 0.0358 - val_mean_absolute_error: 0.1443\n",
            "Epoch 7/27\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0372 - mean_absolute_error: 0.1481 - val_loss: 0.0371 - val_mean_absolute_error: 0.1511\n",
            "Epoch 8/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0357 - mean_absolute_error: 0.1450 - val_loss: 0.0350 - val_mean_absolute_error: 0.1428\n",
            "Epoch 9/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0354 - mean_absolute_error: 0.1449 - val_loss: 0.0323 - val_mean_absolute_error: 0.1375\n",
            "Epoch 10/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0347 - mean_absolute_error: 0.1426 - val_loss: 0.0326 - val_mean_absolute_error: 0.1381\n",
            "Epoch 11/27\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0340 - mean_absolute_error: 0.1414 - val_loss: 0.0336 - val_mean_absolute_error: 0.1396\n",
            "Epoch 12/27\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0333 - mean_absolute_error: 0.1404 - val_loss: 0.0331 - val_mean_absolute_error: 0.1399\n",
            "Epoch 13/27\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0327 - mean_absolute_error: 0.1383 - val_loss: 0.0341 - val_mean_absolute_error: 0.1407\n",
            "Epoch 14/27\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0326 - mean_absolute_error: 0.1388 - val_loss: 0.0318 - val_mean_absolute_error: 0.1368\n",
            "Epoch 15/27\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0319 - mean_absolute_error: 0.1372 - val_loss: 0.0331 - val_mean_absolute_error: 0.1415\n",
            "Epoch 16/27\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0319 - mean_absolute_error: 0.1372 - val_loss: 0.0304 - val_mean_absolute_error: 0.1337\n",
            "Epoch 17/27\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0313 - mean_absolute_error: 0.1360 - val_loss: 0.0313 - val_mean_absolute_error: 0.1345\n",
            "Epoch 18/27\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0307 - mean_absolute_error: 0.1349 - val_loss: 0.0299 - val_mean_absolute_error: 0.1316\n",
            "Epoch 19/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0310 - mean_absolute_error: 0.1349 - val_loss: 0.0296 - val_mean_absolute_error: 0.1314\n",
            "Epoch 20/27\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0304 - mean_absolute_error: 0.1335 - val_loss: 0.0312 - val_mean_absolute_error: 0.1346\n",
            "Epoch 21/27\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0302 - mean_absolute_error: 0.1329 - val_loss: 0.0294 - val_mean_absolute_error: 0.1321\n",
            "Epoch 22/27\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0299 - mean_absolute_error: 0.1328 - val_loss: 0.0319 - val_mean_absolute_error: 0.1371\n",
            "Epoch 23/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0296 - mean_absolute_error: 0.1327 - val_loss: 0.0284 - val_mean_absolute_error: 0.1289\n",
            "Epoch 24/27\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0295 - mean_absolute_error: 0.1319 - val_loss: 0.0290 - val_mean_absolute_error: 0.1314\n",
            "Epoch 25/27\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0289 - mean_absolute_error: 0.1311 - val_loss: 0.0297 - val_mean_absolute_error: 0.1315\n",
            "Epoch 26/27\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0285 - mean_absolute_error: 0.1297 - val_loss: 0.0288 - val_mean_absolute_error: 0.1300\n",
            "Epoch 27/27\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1295 - val_loss: 0.0301 - val_mean_absolute_error: 0.1339\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_57 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_58 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/28\n",
            "10381/10381 [==============================] - 14s 1ms/step - loss: 0.0733 - mean_absolute_error: 0.2080 - val_loss: 0.0430 - val_mean_absolute_error: 0.1575\n",
            "Epoch 2/28\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0467 - mean_absolute_error: 0.1674 - val_loss: 0.0389 - val_mean_absolute_error: 0.1516\n",
            "Epoch 3/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0430 - mean_absolute_error: 0.1600 - val_loss: 0.0376 - val_mean_absolute_error: 0.1522\n",
            "Epoch 4/28\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0406 - mean_absolute_error: 0.1554 - val_loss: 0.0371 - val_mean_absolute_error: 0.1459\n",
            "Epoch 5/28\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0393 - mean_absolute_error: 0.1531 - val_loss: 0.0360 - val_mean_absolute_error: 0.1442\n",
            "Epoch 6/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0375 - mean_absolute_error: 0.1487 - val_loss: 0.0346 - val_mean_absolute_error: 0.1423\n",
            "Epoch 7/28\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0366 - mean_absolute_error: 0.1472 - val_loss: 0.0345 - val_mean_absolute_error: 0.1413\n",
            "Epoch 8/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0357 - mean_absolute_error: 0.1456 - val_loss: 0.0344 - val_mean_absolute_error: 0.1414\n",
            "Epoch 9/28\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0353 - mean_absolute_error: 0.1441 - val_loss: 0.0371 - val_mean_absolute_error: 0.1470\n",
            "Epoch 10/28\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0345 - mean_absolute_error: 0.1424 - val_loss: 0.0326 - val_mean_absolute_error: 0.1385\n",
            "Epoch 11/28\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0341 - mean_absolute_error: 0.1422 - val_loss: 0.0311 - val_mean_absolute_error: 0.1348\n",
            "Epoch 12/28\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0333 - mean_absolute_error: 0.1404 - val_loss: 0.0317 - val_mean_absolute_error: 0.1362\n",
            "Epoch 13/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0330 - mean_absolute_error: 0.1394 - val_loss: 0.0326 - val_mean_absolute_error: 0.1379\n",
            "Epoch 14/28\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0326 - mean_absolute_error: 0.1391 - val_loss: 0.0311 - val_mean_absolute_error: 0.1355\n",
            "Epoch 15/28\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0327 - mean_absolute_error: 0.1391 - val_loss: 0.0306 - val_mean_absolute_error: 0.1337\n",
            "Epoch 16/28\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0319 - mean_absolute_error: 0.1376 - val_loss: 0.0311 - val_mean_absolute_error: 0.1348\n",
            "Epoch 17/28\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0314 - mean_absolute_error: 0.1361 - val_loss: 0.0303 - val_mean_absolute_error: 0.1341\n",
            "Epoch 18/28\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0311 - mean_absolute_error: 0.1356 - val_loss: 0.0303 - val_mean_absolute_error: 0.1333\n",
            "Epoch 19/28\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0305 - mean_absolute_error: 0.1339 - val_loss: 0.0299 - val_mean_absolute_error: 0.1323\n",
            "Epoch 20/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0305 - mean_absolute_error: 0.1344 - val_loss: 0.0310 - val_mean_absolute_error: 0.1339\n",
            "Epoch 21/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0301 - mean_absolute_error: 0.1329 - val_loss: 0.0301 - val_mean_absolute_error: 0.1328\n",
            "Epoch 22/28\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0298 - mean_absolute_error: 0.1326 - val_loss: 0.0291 - val_mean_absolute_error: 0.1310\n",
            "Epoch 23/28\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0300 - mean_absolute_error: 0.1332 - val_loss: 0.0328 - val_mean_absolute_error: 0.1411\n",
            "Epoch 24/28\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0294 - mean_absolute_error: 0.1314 - val_loss: 0.0296 - val_mean_absolute_error: 0.1319\n",
            "Epoch 25/28\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0289 - mean_absolute_error: 0.1304 - val_loss: 0.0288 - val_mean_absolute_error: 0.1299\n",
            "Epoch 26/28\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0292 - mean_absolute_error: 0.1311 - val_loss: 0.0282 - val_mean_absolute_error: 0.1279\n",
            "Epoch 27/28\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0285 - mean_absolute_error: 0.1298 - val_loss: 0.0289 - val_mean_absolute_error: 0.1296\n",
            "Epoch 28/28\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0286 - mean_absolute_error: 0.1304 - val_loss: 0.0292 - val_mean_absolute_error: 0.1304\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_59 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_60 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/29\n",
            "10381/10381 [==============================] - 14s 1ms/step - loss: 0.0737 - mean_absolute_error: 0.2085 - val_loss: 0.0478 - val_mean_absolute_error: 0.1658\n",
            "Epoch 2/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0478 - mean_absolute_error: 0.1705 - val_loss: 0.0398 - val_mean_absolute_error: 0.1552\n",
            "Epoch 3/29\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0437 - mean_absolute_error: 0.1621 - val_loss: 0.0374 - val_mean_absolute_error: 0.1480\n",
            "Epoch 4/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0411 - mean_absolute_error: 0.1561 - val_loss: 0.0411 - val_mean_absolute_error: 0.1617\n",
            "Epoch 5/29\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0399 - mean_absolute_error: 0.1538 - val_loss: 0.0357 - val_mean_absolute_error: 0.1431\n",
            "Epoch 6/29\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0380 - mean_absolute_error: 0.1498 - val_loss: 0.0355 - val_mean_absolute_error: 0.1439\n",
            "Epoch 7/29\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0373 - mean_absolute_error: 0.1482 - val_loss: 0.0339 - val_mean_absolute_error: 0.1404\n",
            "Epoch 8/29\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0363 - mean_absolute_error: 0.1462 - val_loss: 0.0337 - val_mean_absolute_error: 0.1393\n",
            "Epoch 9/29\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0360 - mean_absolute_error: 0.1456 - val_loss: 0.0329 - val_mean_absolute_error: 0.1390\n",
            "Epoch 10/29\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0347 - mean_absolute_error: 0.1432 - val_loss: 0.0345 - val_mean_absolute_error: 0.1441\n",
            "Epoch 11/29\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0346 - mean_absolute_error: 0.1424 - val_loss: 0.0347 - val_mean_absolute_error: 0.1421\n",
            "Epoch 12/29\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0337 - mean_absolute_error: 0.1412 - val_loss: 0.0318 - val_mean_absolute_error: 0.1357\n",
            "Epoch 13/29\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0333 - mean_absolute_error: 0.1396 - val_loss: 0.0339 - val_mean_absolute_error: 0.1405\n",
            "Epoch 14/29\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0327 - mean_absolute_error: 0.1392 - val_loss: 0.0315 - val_mean_absolute_error: 0.1370\n",
            "Epoch 15/29\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0326 - mean_absolute_error: 0.1387 - val_loss: 0.0308 - val_mean_absolute_error: 0.1341\n",
            "Epoch 16/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0319 - mean_absolute_error: 0.1374 - val_loss: 0.0311 - val_mean_absolute_error: 0.1352\n",
            "Epoch 17/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0313 - mean_absolute_error: 0.1360 - val_loss: 0.0310 - val_mean_absolute_error: 0.1355\n",
            "Epoch 18/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0310 - mean_absolute_error: 0.1351 - val_loss: 0.0303 - val_mean_absolute_error: 0.1339\n",
            "Epoch 19/29\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0306 - mean_absolute_error: 0.1350 - val_loss: 0.0330 - val_mean_absolute_error: 0.1427\n",
            "Epoch 20/29\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0306 - mean_absolute_error: 0.1344 - val_loss: 0.0299 - val_mean_absolute_error: 0.1333\n",
            "Epoch 21/29\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0303 - mean_absolute_error: 0.1337 - val_loss: 0.0339 - val_mean_absolute_error: 0.1428\n",
            "Epoch 22/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0299 - mean_absolute_error: 0.1324 - val_loss: 0.0317 - val_mean_absolute_error: 0.1343\n",
            "Epoch 23/29\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0297 - mean_absolute_error: 0.1321 - val_loss: 0.0292 - val_mean_absolute_error: 0.1307\n",
            "Epoch 24/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0295 - mean_absolute_error: 0.1318 - val_loss: 0.0286 - val_mean_absolute_error: 0.1294\n",
            "Epoch 25/29\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0289 - mean_absolute_error: 0.1306 - val_loss: 0.0287 - val_mean_absolute_error: 0.1306\n",
            "Epoch 26/29\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0290 - mean_absolute_error: 0.1312 - val_loss: 0.0311 - val_mean_absolute_error: 0.1358\n",
            "Epoch 27/29\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0288 - mean_absolute_error: 0.1301 - val_loss: 0.0285 - val_mean_absolute_error: 0.1292\n",
            "Epoch 28/29\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1303 - val_loss: 0.0277 - val_mean_absolute_error: 0.1268\n",
            "Epoch 29/29\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0285 - mean_absolute_error: 0.1292 - val_loss: 0.0313 - val_mean_absolute_error: 0.1350\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_61 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_62 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/30\n",
            "10381/10381 [==============================] - 15s 1ms/step - loss: 0.0728 - mean_absolute_error: 0.2082 - val_loss: 0.0456 - val_mean_absolute_error: 0.1690\n",
            "Epoch 2/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0481 - mean_absolute_error: 0.1708 - val_loss: 0.0403 - val_mean_absolute_error: 0.1536\n",
            "Epoch 3/30\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0436 - mean_absolute_error: 0.1622 - val_loss: 0.0385 - val_mean_absolute_error: 0.1517\n",
            "Epoch 4/30\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0406 - mean_absolute_error: 0.1558 - val_loss: 0.0358 - val_mean_absolute_error: 0.1444\n",
            "Epoch 5/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0391 - mean_absolute_error: 0.1516 - val_loss: 0.0347 - val_mean_absolute_error: 0.1430\n",
            "Epoch 6/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0378 - mean_absolute_error: 0.1493 - val_loss: 0.0345 - val_mean_absolute_error: 0.1439\n",
            "Epoch 7/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0368 - mean_absolute_error: 0.1474 - val_loss: 0.0332 - val_mean_absolute_error: 0.1398\n",
            "Epoch 8/30\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0362 - mean_absolute_error: 0.1460 - val_loss: 0.0341 - val_mean_absolute_error: 0.1409\n",
            "Epoch 9/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0351 - mean_absolute_error: 0.1441 - val_loss: 0.0341 - val_mean_absolute_error: 0.1414\n",
            "Epoch 10/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0347 - mean_absolute_error: 0.1429 - val_loss: 0.0328 - val_mean_absolute_error: 0.1387\n",
            "Epoch 11/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0339 - mean_absolute_error: 0.1416 - val_loss: 0.0327 - val_mean_absolute_error: 0.1383\n",
            "Epoch 12/30\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0334 - mean_absolute_error: 0.1406 - val_loss: 0.0317 - val_mean_absolute_error: 0.1369\n",
            "Epoch 13/30\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0328 - mean_absolute_error: 0.1397 - val_loss: 0.0313 - val_mean_absolute_error: 0.1350\n",
            "Epoch 14/30\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0322 - mean_absolute_error: 0.1382 - val_loss: 0.0319 - val_mean_absolute_error: 0.1371\n",
            "Epoch 15/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0321 - mean_absolute_error: 0.1374 - val_loss: 0.0333 - val_mean_absolute_error: 0.1396\n",
            "Epoch 16/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0318 - mean_absolute_error: 0.1376 - val_loss: 0.0319 - val_mean_absolute_error: 0.1377\n",
            "Epoch 17/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0316 - mean_absolute_error: 0.1367 - val_loss: 0.0305 - val_mean_absolute_error: 0.1338\n",
            "Epoch 18/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0311 - mean_absolute_error: 0.1348 - val_loss: 0.0300 - val_mean_absolute_error: 0.1323\n",
            "Epoch 19/30\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0311 - mean_absolute_error: 0.1352 - val_loss: 0.0318 - val_mean_absolute_error: 0.1396\n",
            "Epoch 20/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0306 - mean_absolute_error: 0.1344 - val_loss: 0.0299 - val_mean_absolute_error: 0.1319\n",
            "Epoch 21/30\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0300 - mean_absolute_error: 0.1332 - val_loss: 0.0315 - val_mean_absolute_error: 0.1357\n",
            "Epoch 22/30\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0299 - mean_absolute_error: 0.1327 - val_loss: 0.0305 - val_mean_absolute_error: 0.1327\n",
            "Epoch 23/30\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0297 - mean_absolute_error: 0.1320 - val_loss: 0.0300 - val_mean_absolute_error: 0.1320\n",
            "Epoch 24/30\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0293 - mean_absolute_error: 0.1315 - val_loss: 0.0286 - val_mean_absolute_error: 0.1286\n",
            "Epoch 25/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0291 - mean_absolute_error: 0.1308 - val_loss: 0.0286 - val_mean_absolute_error: 0.1282\n",
            "Epoch 26/30\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0288 - mean_absolute_error: 0.1298 - val_loss: 0.0284 - val_mean_absolute_error: 0.1288\n",
            "Epoch 27/30\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0285 - mean_absolute_error: 0.1300 - val_loss: 0.0285 - val_mean_absolute_error: 0.1284\n",
            "Epoch 28/30\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0284 - mean_absolute_error: 0.1291 - val_loss: 0.0283 - val_mean_absolute_error: 0.1281\n",
            "Epoch 29/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0287 - mean_absolute_error: 0.1299 - val_loss: 0.0280 - val_mean_absolute_error: 0.1273\n",
            "Epoch 30/30\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0283 - mean_absolute_error: 0.1292 - val_loss: 0.0281 - val_mean_absolute_error: 0.1281\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_63 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_64 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/31\n",
            "10381/10381 [==============================] - 15s 1ms/step - loss: 0.0718 - mean_absolute_error: 0.2049 - val_loss: 0.0436 - val_mean_absolute_error: 0.1607\n",
            "Epoch 2/31\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0485 - mean_absolute_error: 0.1718 - val_loss: 0.0418 - val_mean_absolute_error: 0.1550\n",
            "Epoch 3/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0441 - mean_absolute_error: 0.1624 - val_loss: 0.0378 - val_mean_absolute_error: 0.1499\n",
            "Epoch 4/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0408 - mean_absolute_error: 0.1555 - val_loss: 0.0372 - val_mean_absolute_error: 0.1507\n",
            "Epoch 5/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0391 - mean_absolute_error: 0.1530 - val_loss: 0.0356 - val_mean_absolute_error: 0.1435\n",
            "Epoch 6/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0374 - mean_absolute_error: 0.1486 - val_loss: 0.0336 - val_mean_absolute_error: 0.1397\n",
            "Epoch 7/31\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0366 - mean_absolute_error: 0.1469 - val_loss: 0.0353 - val_mean_absolute_error: 0.1478\n",
            "Epoch 8/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0361 - mean_absolute_error: 0.1460 - val_loss: 0.0342 - val_mean_absolute_error: 0.1411\n",
            "Epoch 9/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0348 - mean_absolute_error: 0.1432 - val_loss: 0.0357 - val_mean_absolute_error: 0.1475\n",
            "Epoch 10/31\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0344 - mean_absolute_error: 0.1426 - val_loss: 0.0325 - val_mean_absolute_error: 0.1385\n",
            "Epoch 11/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0341 - mean_absolute_error: 0.1423 - val_loss: 0.0321 - val_mean_absolute_error: 0.1380\n",
            "Epoch 12/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0333 - mean_absolute_error: 0.1403 - val_loss: 0.0317 - val_mean_absolute_error: 0.1366\n",
            "Epoch 13/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0331 - mean_absolute_error: 0.1397 - val_loss: 0.0323 - val_mean_absolute_error: 0.1397\n",
            "Epoch 14/31\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0324 - mean_absolute_error: 0.1378 - val_loss: 0.0310 - val_mean_absolute_error: 0.1341\n",
            "Epoch 15/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0321 - mean_absolute_error: 0.1374 - val_loss: 0.0302 - val_mean_absolute_error: 0.1320\n",
            "Epoch 16/31\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0317 - mean_absolute_error: 0.1369 - val_loss: 0.0311 - val_mean_absolute_error: 0.1348\n",
            "Epoch 17/31\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0317 - mean_absolute_error: 0.1365 - val_loss: 0.0298 - val_mean_absolute_error: 0.1316\n",
            "Epoch 18/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0308 - mean_absolute_error: 0.1346 - val_loss: 0.0299 - val_mean_absolute_error: 0.1322\n",
            "Epoch 19/31\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0310 - mean_absolute_error: 0.1354 - val_loss: 0.0323 - val_mean_absolute_error: 0.1367\n",
            "Epoch 20/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0307 - mean_absolute_error: 0.1346 - val_loss: 0.0313 - val_mean_absolute_error: 0.1358\n",
            "Epoch 21/31\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0305 - mean_absolute_error: 0.1339 - val_loss: 0.0298 - val_mean_absolute_error: 0.1327\n",
            "Epoch 22/31\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0294 - mean_absolute_error: 0.1316 - val_loss: 0.0287 - val_mean_absolute_error: 0.1294\n",
            "Epoch 23/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0296 - mean_absolute_error: 0.1317 - val_loss: 0.0282 - val_mean_absolute_error: 0.1282\n",
            "Epoch 24/31\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0292 - mean_absolute_error: 0.1310 - val_loss: 0.0285 - val_mean_absolute_error: 0.1290\n",
            "Epoch 25/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0290 - mean_absolute_error: 0.1308 - val_loss: 0.0286 - val_mean_absolute_error: 0.1299\n",
            "Epoch 26/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0289 - mean_absolute_error: 0.1307 - val_loss: 0.0294 - val_mean_absolute_error: 0.1299\n",
            "Epoch 27/31\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0288 - mean_absolute_error: 0.1300 - val_loss: 0.0291 - val_mean_absolute_error: 0.1297\n",
            "Epoch 28/31\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0286 - mean_absolute_error: 0.1300 - val_loss: 0.0293 - val_mean_absolute_error: 0.1311\n",
            "Epoch 29/31\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0286 - mean_absolute_error: 0.1299 - val_loss: 0.0283 - val_mean_absolute_error: 0.1272\n",
            "Epoch 30/31\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0285 - mean_absolute_error: 0.1299 - val_loss: 0.0291 - val_mean_absolute_error: 0.1311\n",
            "Epoch 31/31\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0283 - mean_absolute_error: 0.1295 - val_loss: 0.0289 - val_mean_absolute_error: 0.1293\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_65 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_66 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/32\n",
            "10381/10381 [==============================] - 16s 2ms/step - loss: 0.0724 - mean_absolute_error: 0.2058 - val_loss: 0.0427 - val_mean_absolute_error: 0.1587\n",
            "Epoch 2/32\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0476 - mean_absolute_error: 0.1695 - val_loss: 0.0464 - val_mean_absolute_error: 0.1704\n",
            "Epoch 3/32\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0439 - mean_absolute_error: 0.1625 - val_loss: 0.0431 - val_mean_absolute_error: 0.1632\n",
            "Epoch 4/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0413 - mean_absolute_error: 0.1567 - val_loss: 0.0374 - val_mean_absolute_error: 0.1512\n",
            "Epoch 5/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0394 - mean_absolute_error: 0.1524 - val_loss: 0.0361 - val_mean_absolute_error: 0.1486\n",
            "Epoch 6/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0384 - mean_absolute_error: 0.1502 - val_loss: 0.0403 - val_mean_absolute_error: 0.1532\n",
            "Epoch 7/32\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0369 - mean_absolute_error: 0.1476 - val_loss: 0.0342 - val_mean_absolute_error: 0.1433\n",
            "Epoch 8/32\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0355 - mean_absolute_error: 0.1447 - val_loss: 0.0330 - val_mean_absolute_error: 0.1396\n",
            "Epoch 9/32\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0353 - mean_absolute_error: 0.1445 - val_loss: 0.0322 - val_mean_absolute_error: 0.1375\n",
            "Epoch 10/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0344 - mean_absolute_error: 0.1423 - val_loss: 0.0341 - val_mean_absolute_error: 0.1424\n",
            "Epoch 11/32\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0340 - mean_absolute_error: 0.1416 - val_loss: 0.0322 - val_mean_absolute_error: 0.1377\n",
            "Epoch 12/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0336 - mean_absolute_error: 0.1410 - val_loss: 0.0329 - val_mean_absolute_error: 0.1386\n",
            "Epoch 13/32\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0329 - mean_absolute_error: 0.1397 - val_loss: 0.0325 - val_mean_absolute_error: 0.1378\n",
            "Epoch 14/32\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0328 - mean_absolute_error: 0.1390 - val_loss: 0.0308 - val_mean_absolute_error: 0.1335\n",
            "Epoch 15/32\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0322 - mean_absolute_error: 0.1380 - val_loss: 0.0310 - val_mean_absolute_error: 0.1351\n",
            "Epoch 16/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0319 - mean_absolute_error: 0.1374 - val_loss: 0.0304 - val_mean_absolute_error: 0.1331\n",
            "Epoch 17/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0313 - mean_absolute_error: 0.1362 - val_loss: 0.0298 - val_mean_absolute_error: 0.1312\n",
            "Epoch 18/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0312 - mean_absolute_error: 0.1361 - val_loss: 0.0300 - val_mean_absolute_error: 0.1320\n",
            "Epoch 19/32\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0302 - mean_absolute_error: 0.1333 - val_loss: 0.0297 - val_mean_absolute_error: 0.1316\n",
            "Epoch 20/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0303 - mean_absolute_error: 0.1339 - val_loss: 0.0332 - val_mean_absolute_error: 0.1379\n",
            "Epoch 21/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0301 - mean_absolute_error: 0.1333 - val_loss: 0.0307 - val_mean_absolute_error: 0.1334\n",
            "Epoch 22/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0304 - mean_absolute_error: 0.1337 - val_loss: 0.0316 - val_mean_absolute_error: 0.1365\n",
            "Epoch 23/32\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0299 - mean_absolute_error: 0.1327 - val_loss: 0.0295 - val_mean_absolute_error: 0.1310\n",
            "Epoch 24/32\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0293 - mean_absolute_error: 0.1320 - val_loss: 0.0290 - val_mean_absolute_error: 0.1293\n",
            "Epoch 25/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0291 - mean_absolute_error: 0.1310 - val_loss: 0.0313 - val_mean_absolute_error: 0.1343\n",
            "Epoch 26/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0293 - mean_absolute_error: 0.1314 - val_loss: 0.0288 - val_mean_absolute_error: 0.1300\n",
            "Epoch 27/32\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0290 - mean_absolute_error: 0.1306 - val_loss: 0.0283 - val_mean_absolute_error: 0.1285\n",
            "Epoch 28/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0289 - mean_absolute_error: 0.1305 - val_loss: 0.0286 - val_mean_absolute_error: 0.1301\n",
            "Epoch 29/32\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0283 - mean_absolute_error: 0.1289 - val_loss: 0.0294 - val_mean_absolute_error: 0.1337\n",
            "Epoch 30/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0283 - mean_absolute_error: 0.1291 - val_loss: 0.0278 - val_mean_absolute_error: 0.1271\n",
            "Epoch 31/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0282 - mean_absolute_error: 0.1290 - val_loss: 0.0290 - val_mean_absolute_error: 0.1305\n",
            "Epoch 32/32\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0276 - mean_absolute_error: 0.1272 - val_loss: 0.0286 - val_mean_absolute_error: 0.1296\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_67 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_68 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/33\n",
            "10381/10381 [==============================] - 16s 2ms/step - loss: 0.0746 - mean_absolute_error: 0.2088 - val_loss: 0.0421 - val_mean_absolute_error: 0.1578\n",
            "Epoch 2/33\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0474 - mean_absolute_error: 0.1695 - val_loss: 0.0470 - val_mean_absolute_error: 0.1733\n",
            "Epoch 3/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0435 - mean_absolute_error: 0.1618 - val_loss: 0.0380 - val_mean_absolute_error: 0.1501\n",
            "Epoch 4/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0407 - mean_absolute_error: 0.1555 - val_loss: 0.0394 - val_mean_absolute_error: 0.1534\n",
            "Epoch 5/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0389 - mean_absolute_error: 0.1510 - val_loss: 0.0346 - val_mean_absolute_error: 0.1424\n",
            "Epoch 6/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0377 - mean_absolute_error: 0.1486 - val_loss: 0.0342 - val_mean_absolute_error: 0.1411\n",
            "Epoch 7/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0367 - mean_absolute_error: 0.1469 - val_loss: 0.0334 - val_mean_absolute_error: 0.1396\n",
            "Epoch 8/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0360 - mean_absolute_error: 0.1458 - val_loss: 0.0340 - val_mean_absolute_error: 0.1404\n",
            "Epoch 9/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0350 - mean_absolute_error: 0.1439 - val_loss: 0.0325 - val_mean_absolute_error: 0.1379\n",
            "Epoch 10/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0346 - mean_absolute_error: 0.1429 - val_loss: 0.0330 - val_mean_absolute_error: 0.1381\n",
            "Epoch 11/33\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0339 - mean_absolute_error: 0.1409 - val_loss: 0.0356 - val_mean_absolute_error: 0.1457\n",
            "Epoch 12/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0331 - mean_absolute_error: 0.1399 - val_loss: 0.0316 - val_mean_absolute_error: 0.1359\n",
            "Epoch 13/33\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0328 - mean_absolute_error: 0.1389 - val_loss: 0.0325 - val_mean_absolute_error: 0.1384\n",
            "Epoch 14/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0328 - mean_absolute_error: 0.1397 - val_loss: 0.0314 - val_mean_absolute_error: 0.1356\n",
            "Epoch 15/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0319 - mean_absolute_error: 0.1373 - val_loss: 0.0306 - val_mean_absolute_error: 0.1332\n",
            "Epoch 16/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0314 - mean_absolute_error: 0.1369 - val_loss: 0.0331 - val_mean_absolute_error: 0.1407\n",
            "Epoch 17/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0313 - mean_absolute_error: 0.1365 - val_loss: 0.0338 - val_mean_absolute_error: 0.1414\n",
            "Epoch 18/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0308 - mean_absolute_error: 0.1349 - val_loss: 0.0311 - val_mean_absolute_error: 0.1362\n",
            "Epoch 19/33\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0304 - mean_absolute_error: 0.1342 - val_loss: 0.0295 - val_mean_absolute_error: 0.1315\n",
            "Epoch 20/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0301 - mean_absolute_error: 0.1330 - val_loss: 0.0307 - val_mean_absolute_error: 0.1340\n",
            "Epoch 21/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0304 - mean_absolute_error: 0.1338 - val_loss: 0.0301 - val_mean_absolute_error: 0.1318\n",
            "Epoch 22/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0296 - mean_absolute_error: 0.1323 - val_loss: 0.0290 - val_mean_absolute_error: 0.1298\n",
            "Epoch 23/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0299 - mean_absolute_error: 0.1326 - val_loss: 0.0301 - val_mean_absolute_error: 0.1330\n",
            "Epoch 24/33\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0292 - mean_absolute_error: 0.1316 - val_loss: 0.0286 - val_mean_absolute_error: 0.1294\n",
            "Epoch 25/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0290 - mean_absolute_error: 0.1307 - val_loss: 0.0282 - val_mean_absolute_error: 0.1276\n",
            "Epoch 26/33\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0290 - mean_absolute_error: 0.1307 - val_loss: 0.0279 - val_mean_absolute_error: 0.1276\n",
            "Epoch 27/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0290 - mean_absolute_error: 0.1301 - val_loss: 0.0296 - val_mean_absolute_error: 0.1317\n",
            "Epoch 28/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0284 - mean_absolute_error: 0.1295 - val_loss: 0.0291 - val_mean_absolute_error: 0.1305\n",
            "Epoch 29/33\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0283 - mean_absolute_error: 0.1290 - val_loss: 0.0285 - val_mean_absolute_error: 0.1288\n",
            "Epoch 30/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0281 - mean_absolute_error: 0.1284 - val_loss: 0.0299 - val_mean_absolute_error: 0.1319\n",
            "Epoch 31/33\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0283 - mean_absolute_error: 0.1290 - val_loss: 0.0294 - val_mean_absolute_error: 0.1302\n",
            "Epoch 32/33\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0278 - mean_absolute_error: 0.1281 - val_loss: 0.0309 - val_mean_absolute_error: 0.1348\n",
            "Epoch 33/33\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0275 - mean_absolute_error: 0.1277 - val_loss: 0.0274 - val_mean_absolute_error: 0.1266\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_69 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_70 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/34\n",
            "10381/10381 [==============================] - 17s 2ms/step - loss: 0.0707 - mean_absolute_error: 0.2041 - val_loss: 0.0438 - val_mean_absolute_error: 0.1594\n",
            "Epoch 2/34\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0472 - mean_absolute_error: 0.1691 - val_loss: 0.0407 - val_mean_absolute_error: 0.1529\n",
            "Epoch 3/34\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0436 - mean_absolute_error: 0.1613 - val_loss: 0.0408 - val_mean_absolute_error: 0.1592\n",
            "Epoch 4/34\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0414 - mean_absolute_error: 0.1567 - val_loss: 0.0365 - val_mean_absolute_error: 0.1477\n",
            "Epoch 5/34\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0391 - mean_absolute_error: 0.1519 - val_loss: 0.0355 - val_mean_absolute_error: 0.1443\n",
            "Epoch 6/34\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0381 - mean_absolute_error: 0.1501 - val_loss: 0.0341 - val_mean_absolute_error: 0.1409\n",
            "Epoch 7/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0362 - mean_absolute_error: 0.1464 - val_loss: 0.0335 - val_mean_absolute_error: 0.1403\n",
            "Epoch 8/34\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0359 - mean_absolute_error: 0.1453 - val_loss: 0.0339 - val_mean_absolute_error: 0.1399\n",
            "Epoch 9/34\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0351 - mean_absolute_error: 0.1441 - val_loss: 0.0337 - val_mean_absolute_error: 0.1399\n",
            "Epoch 10/34\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0348 - mean_absolute_error: 0.1428 - val_loss: 0.0352 - val_mean_absolute_error: 0.1428\n",
            "Epoch 11/34\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0341 - mean_absolute_error: 0.1425 - val_loss: 0.0335 - val_mean_absolute_error: 0.1420\n",
            "Epoch 12/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0334 - mean_absolute_error: 0.1407 - val_loss: 0.0362 - val_mean_absolute_error: 0.1494\n",
            "Epoch 13/34\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0333 - mean_absolute_error: 0.1404 - val_loss: 0.0315 - val_mean_absolute_error: 0.1343\n",
            "Epoch 14/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0326 - mean_absolute_error: 0.1385 - val_loss: 0.0333 - val_mean_absolute_error: 0.1421\n",
            "Epoch 15/34\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0323 - mean_absolute_error: 0.1383 - val_loss: 0.0314 - val_mean_absolute_error: 0.1357\n",
            "Epoch 16/34\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0316 - mean_absolute_error: 0.1363 - val_loss: 0.0305 - val_mean_absolute_error: 0.1336\n",
            "Epoch 17/34\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0318 - mean_absolute_error: 0.1370 - val_loss: 0.0312 - val_mean_absolute_error: 0.1351\n",
            "Epoch 18/34\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0308 - mean_absolute_error: 0.1349 - val_loss: 0.0342 - val_mean_absolute_error: 0.1443\n",
            "Epoch 19/34\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0310 - mean_absolute_error: 0.1350 - val_loss: 0.0310 - val_mean_absolute_error: 0.1346\n",
            "Epoch 20/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0304 - mean_absolute_error: 0.1342 - val_loss: 0.0311 - val_mean_absolute_error: 0.1348\n",
            "Epoch 21/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0299 - mean_absolute_error: 0.1336 - val_loss: 0.0307 - val_mean_absolute_error: 0.1338\n",
            "Epoch 22/34\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0303 - mean_absolute_error: 0.1332 - val_loss: 0.0298 - val_mean_absolute_error: 0.1307\n",
            "Epoch 23/34\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0299 - mean_absolute_error: 0.1329 - val_loss: 0.0365 - val_mean_absolute_error: 0.1465\n",
            "Epoch 24/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0297 - mean_absolute_error: 0.1322 - val_loss: 0.0297 - val_mean_absolute_error: 0.1298\n",
            "Epoch 25/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0291 - mean_absolute_error: 0.1312 - val_loss: 0.0316 - val_mean_absolute_error: 0.1384\n",
            "Epoch 26/34\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0293 - mean_absolute_error: 0.1320 - val_loss: 0.0295 - val_mean_absolute_error: 0.1317\n",
            "Epoch 27/34\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0289 - mean_absolute_error: 0.1303 - val_loss: 0.0280 - val_mean_absolute_error: 0.1273\n",
            "Epoch 28/34\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0288 - mean_absolute_error: 0.1304 - val_loss: 0.0286 - val_mean_absolute_error: 0.1279\n",
            "Epoch 29/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1298 - val_loss: 0.0283 - val_mean_absolute_error: 0.1284\n",
            "Epoch 30/34\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0282 - mean_absolute_error: 0.1288 - val_loss: 0.0317 - val_mean_absolute_error: 0.1381\n",
            "Epoch 31/34\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0280 - mean_absolute_error: 0.1283 - val_loss: 0.0287 - val_mean_absolute_error: 0.1299\n",
            "Epoch 32/34\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0278 - mean_absolute_error: 0.1278 - val_loss: 0.0307 - val_mean_absolute_error: 0.1326\n",
            "Epoch 33/34\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0279 - mean_absolute_error: 0.1274 - val_loss: 0.0280 - val_mean_absolute_error: 0.1271\n",
            "Epoch 34/34\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0277 - mean_absolute_error: 0.1275 - val_loss: 0.0278 - val_mean_absolute_error: 0.1272\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_71 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_72 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/35\n",
            "10381/10381 [==============================] - 17s 2ms/step - loss: 0.0739 - mean_absolute_error: 0.2080 - val_loss: 0.0451 - val_mean_absolute_error: 0.1683\n",
            "Epoch 2/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0477 - mean_absolute_error: 0.1694 - val_loss: 0.0421 - val_mean_absolute_error: 0.1612\n",
            "Epoch 3/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0428 - mean_absolute_error: 0.1597 - val_loss: 0.0387 - val_mean_absolute_error: 0.1552\n",
            "Epoch 4/35\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0405 - mean_absolute_error: 0.1551 - val_loss: 0.0369 - val_mean_absolute_error: 0.1454\n",
            "Epoch 5/35\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0390 - mean_absolute_error: 0.1515 - val_loss: 0.0364 - val_mean_absolute_error: 0.1454\n",
            "Epoch 6/35\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0379 - mean_absolute_error: 0.1501 - val_loss: 0.0377 - val_mean_absolute_error: 0.1482\n",
            "Epoch 7/35\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0365 - mean_absolute_error: 0.1470 - val_loss: 0.0361 - val_mean_absolute_error: 0.1495\n",
            "Epoch 8/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0362 - mean_absolute_error: 0.1465 - val_loss: 0.0333 - val_mean_absolute_error: 0.1407\n",
            "Epoch 9/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0354 - mean_absolute_error: 0.1444 - val_loss: 0.0327 - val_mean_absolute_error: 0.1374\n",
            "Epoch 10/35\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0347 - mean_absolute_error: 0.1433 - val_loss: 0.0322 - val_mean_absolute_error: 0.1383\n",
            "Epoch 11/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0341 - mean_absolute_error: 0.1421 - val_loss: 0.0366 - val_mean_absolute_error: 0.1503\n",
            "Epoch 12/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0335 - mean_absolute_error: 0.1412 - val_loss: 0.0315 - val_mean_absolute_error: 0.1353\n",
            "Epoch 13/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0325 - mean_absolute_error: 0.1391 - val_loss: 0.0351 - val_mean_absolute_error: 0.1452\n",
            "Epoch 14/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0324 - mean_absolute_error: 0.1380 - val_loss: 0.0328 - val_mean_absolute_error: 0.1380\n",
            "Epoch 15/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0324 - mean_absolute_error: 0.1386 - val_loss: 0.0309 - val_mean_absolute_error: 0.1343\n",
            "Epoch 16/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0316 - mean_absolute_error: 0.1367 - val_loss: 0.0326 - val_mean_absolute_error: 0.1384\n",
            "Epoch 17/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0315 - mean_absolute_error: 0.1366 - val_loss: 0.0323 - val_mean_absolute_error: 0.1386\n",
            "Epoch 18/35\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0310 - mean_absolute_error: 0.1353 - val_loss: 0.0299 - val_mean_absolute_error: 0.1318\n",
            "Epoch 19/35\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0306 - mean_absolute_error: 0.1342 - val_loss: 0.0306 - val_mean_absolute_error: 0.1338\n",
            "Epoch 20/35\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0302 - mean_absolute_error: 0.1336 - val_loss: 0.0330 - val_mean_absolute_error: 0.1393\n",
            "Epoch 21/35\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0299 - mean_absolute_error: 0.1327 - val_loss: 0.0331 - val_mean_absolute_error: 0.1405\n",
            "Epoch 22/35\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0302 - mean_absolute_error: 0.1332 - val_loss: 0.0308 - val_mean_absolute_error: 0.1365\n",
            "Epoch 23/35\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0294 - mean_absolute_error: 0.1313 - val_loss: 0.0309 - val_mean_absolute_error: 0.1343\n",
            "Epoch 24/35\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0295 - mean_absolute_error: 0.1318 - val_loss: 0.0292 - val_mean_absolute_error: 0.1308\n",
            "Epoch 25/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0294 - mean_absolute_error: 0.1320 - val_loss: 0.0324 - val_mean_absolute_error: 0.1360\n",
            "Epoch 26/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0289 - mean_absolute_error: 0.1302 - val_loss: 0.0290 - val_mean_absolute_error: 0.1313\n",
            "Epoch 27/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0284 - mean_absolute_error: 0.1293 - val_loss: 0.0300 - val_mean_absolute_error: 0.1331\n",
            "Epoch 28/35\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0288 - mean_absolute_error: 0.1307 - val_loss: 0.0303 - val_mean_absolute_error: 0.1344\n",
            "Epoch 29/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0281 - mean_absolute_error: 0.1287 - val_loss: 0.0309 - val_mean_absolute_error: 0.1364\n",
            "Epoch 30/35\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0284 - mean_absolute_error: 0.1295 - val_loss: 0.0306 - val_mean_absolute_error: 0.1348\n",
            "Epoch 31/35\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0282 - mean_absolute_error: 0.1292 - val_loss: 0.0283 - val_mean_absolute_error: 0.1292\n",
            "Epoch 32/35\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0278 - mean_absolute_error: 0.1280 - val_loss: 0.0289 - val_mean_absolute_error: 0.1314\n",
            "Epoch 33/35\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0278 - mean_absolute_error: 0.1280 - val_loss: 0.0280 - val_mean_absolute_error: 0.1270\n",
            "Epoch 34/35\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0276 - mean_absolute_error: 0.1277 - val_loss: 0.0276 - val_mean_absolute_error: 0.1273\n",
            "Epoch 35/35\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0272 - mean_absolute_error: 0.1263 - val_loss: 0.0283 - val_mean_absolute_error: 0.1295\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_73 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_74 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/36\n",
            "10381/10381 [==============================] - 17s 2ms/step - loss: 0.0736 - mean_absolute_error: 0.2085 - val_loss: 0.0442 - val_mean_absolute_error: 0.1655\n",
            "Epoch 2/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0478 - mean_absolute_error: 0.1707 - val_loss: 0.0401 - val_mean_absolute_error: 0.1532\n",
            "Epoch 3/36\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0433 - mean_absolute_error: 0.1604 - val_loss: 0.0428 - val_mean_absolute_error: 0.1603\n",
            "Epoch 4/36\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0408 - mean_absolute_error: 0.1559 - val_loss: 0.0397 - val_mean_absolute_error: 0.1588\n",
            "Epoch 5/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0397 - mean_absolute_error: 0.1536 - val_loss: 0.0361 - val_mean_absolute_error: 0.1474\n",
            "Epoch 6/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0383 - mean_absolute_error: 0.1506 - val_loss: 0.0399 - val_mean_absolute_error: 0.1534\n",
            "Epoch 7/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0367 - mean_absolute_error: 0.1473 - val_loss: 0.0339 - val_mean_absolute_error: 0.1414\n",
            "Epoch 8/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0359 - mean_absolute_error: 0.1455 - val_loss: 0.0332 - val_mean_absolute_error: 0.1396\n",
            "Epoch 9/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0350 - mean_absolute_error: 0.1444 - val_loss: 0.0336 - val_mean_absolute_error: 0.1399\n",
            "Epoch 10/36\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0347 - mean_absolute_error: 0.1427 - val_loss: 0.0328 - val_mean_absolute_error: 0.1384\n",
            "Epoch 11/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0341 - mean_absolute_error: 0.1419 - val_loss: 0.0368 - val_mean_absolute_error: 0.1478\n",
            "Epoch 12/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0339 - mean_absolute_error: 0.1414 - val_loss: 0.0347 - val_mean_absolute_error: 0.1457\n",
            "Epoch 13/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0330 - mean_absolute_error: 0.1395 - val_loss: 0.0331 - val_mean_absolute_error: 0.1418\n",
            "Epoch 14/36\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0330 - mean_absolute_error: 0.1396 - val_loss: 0.0317 - val_mean_absolute_error: 0.1364\n",
            "Epoch 15/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0326 - mean_absolute_error: 0.1385 - val_loss: 0.0326 - val_mean_absolute_error: 0.1386\n",
            "Epoch 16/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0324 - mean_absolute_error: 0.1381 - val_loss: 0.0304 - val_mean_absolute_error: 0.1330\n",
            "Epoch 17/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0317 - mean_absolute_error: 0.1369 - val_loss: 0.0315 - val_mean_absolute_error: 0.1365\n",
            "Epoch 18/36\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0313 - mean_absolute_error: 0.1358 - val_loss: 0.0310 - val_mean_absolute_error: 0.1352\n",
            "Epoch 19/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0313 - mean_absolute_error: 0.1361 - val_loss: 0.0318 - val_mean_absolute_error: 0.1364\n",
            "Epoch 20/36\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0308 - mean_absolute_error: 0.1346 - val_loss: 0.0298 - val_mean_absolute_error: 0.1310\n",
            "Epoch 21/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0303 - mean_absolute_error: 0.1334 - val_loss: 0.0341 - val_mean_absolute_error: 0.1401\n",
            "Epoch 22/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0302 - mean_absolute_error: 0.1338 - val_loss: 0.0294 - val_mean_absolute_error: 0.1314\n",
            "Epoch 23/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0298 - mean_absolute_error: 0.1326 - val_loss: 0.0312 - val_mean_absolute_error: 0.1351\n",
            "Epoch 24/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0293 - mean_absolute_error: 0.1315 - val_loss: 0.0297 - val_mean_absolute_error: 0.1322\n",
            "Epoch 25/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0292 - mean_absolute_error: 0.1315 - val_loss: 0.0295 - val_mean_absolute_error: 0.1311\n",
            "Epoch 26/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0291 - mean_absolute_error: 0.1310 - val_loss: 0.0307 - val_mean_absolute_error: 0.1343\n",
            "Epoch 27/36\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0289 - mean_absolute_error: 0.1303 - val_loss: 0.0289 - val_mean_absolute_error: 0.1294\n",
            "Epoch 28/36\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0283 - mean_absolute_error: 0.1291 - val_loss: 0.0311 - val_mean_absolute_error: 0.1341\n",
            "Epoch 29/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0282 - mean_absolute_error: 0.1289 - val_loss: 0.0283 - val_mean_absolute_error: 0.1286\n",
            "Epoch 30/36\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0284 - mean_absolute_error: 0.1293 - val_loss: 0.0291 - val_mean_absolute_error: 0.1299\n",
            "Epoch 31/36\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0279 - mean_absolute_error: 0.1282 - val_loss: 0.0300 - val_mean_absolute_error: 0.1357\n",
            "Epoch 32/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1276 - val_loss: 0.0302 - val_mean_absolute_error: 0.1349\n",
            "Epoch 33/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0279 - mean_absolute_error: 0.1284 - val_loss: 0.0288 - val_mean_absolute_error: 0.1294\n",
            "Epoch 34/36\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0276 - mean_absolute_error: 0.1270 - val_loss: 0.0284 - val_mean_absolute_error: 0.1282\n",
            "Epoch 35/36\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0278 - mean_absolute_error: 0.1276 - val_loss: 0.0280 - val_mean_absolute_error: 0.1274\n",
            "Epoch 36/36\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0271 - mean_absolute_error: 0.1264 - val_loss: 0.0284 - val_mean_absolute_error: 0.1297\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_75 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_76 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/37\n",
            "10381/10381 [==============================] - 18s 2ms/step - loss: 0.0737 - mean_absolute_error: 0.2095 - val_loss: 0.0420 - val_mean_absolute_error: 0.1583\n",
            "Epoch 2/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0470 - mean_absolute_error: 0.1688 - val_loss: 0.0403 - val_mean_absolute_error: 0.1535\n",
            "Epoch 3/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0430 - mean_absolute_error: 0.1602 - val_loss: 0.0413 - val_mean_absolute_error: 0.1539\n",
            "Epoch 4/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0408 - mean_absolute_error: 0.1559 - val_loss: 0.0363 - val_mean_absolute_error: 0.1458\n",
            "Epoch 5/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0388 - mean_absolute_error: 0.1512 - val_loss: 0.0418 - val_mean_absolute_error: 0.1576\n",
            "Epoch 6/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0377 - mean_absolute_error: 0.1495 - val_loss: 0.0351 - val_mean_absolute_error: 0.1448\n",
            "Epoch 7/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0367 - mean_absolute_error: 0.1477 - val_loss: 0.0334 - val_mean_absolute_error: 0.1387\n",
            "Epoch 8/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0361 - mean_absolute_error: 0.1461 - val_loss: 0.0326 - val_mean_absolute_error: 0.1374\n",
            "Epoch 9/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0351 - mean_absolute_error: 0.1441 - val_loss: 0.0328 - val_mean_absolute_error: 0.1387\n",
            "Epoch 10/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0339 - mean_absolute_error: 0.1418 - val_loss: 0.0331 - val_mean_absolute_error: 0.1385\n",
            "Epoch 11/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0346 - mean_absolute_error: 0.1432 - val_loss: 0.0332 - val_mean_absolute_error: 0.1418\n",
            "Epoch 12/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0334 - mean_absolute_error: 0.1404 - val_loss: 0.0330 - val_mean_absolute_error: 0.1391\n",
            "Epoch 13/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0329 - mean_absolute_error: 0.1396 - val_loss: 0.0322 - val_mean_absolute_error: 0.1373\n",
            "Epoch 14/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0323 - mean_absolute_error: 0.1381 - val_loss: 0.0327 - val_mean_absolute_error: 0.1401\n",
            "Epoch 15/37\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0322 - mean_absolute_error: 0.1380 - val_loss: 0.0316 - val_mean_absolute_error: 0.1370\n",
            "Epoch 16/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0316 - mean_absolute_error: 0.1362 - val_loss: 0.0305 - val_mean_absolute_error: 0.1334\n",
            "Epoch 17/37\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0313 - mean_absolute_error: 0.1359 - val_loss: 0.0316 - val_mean_absolute_error: 0.1362\n",
            "Epoch 18/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0307 - mean_absolute_error: 0.1349 - val_loss: 0.0316 - val_mean_absolute_error: 0.1348\n",
            "Epoch 19/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0304 - mean_absolute_error: 0.1339 - val_loss: 0.0291 - val_mean_absolute_error: 0.1301\n",
            "Epoch 20/37\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0307 - mean_absolute_error: 0.1345 - val_loss: 0.0300 - val_mean_absolute_error: 0.1317\n",
            "Epoch 21/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0296 - mean_absolute_error: 0.1323 - val_loss: 0.0285 - val_mean_absolute_error: 0.1290\n",
            "Epoch 22/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0299 - mean_absolute_error: 0.1325 - val_loss: 0.0299 - val_mean_absolute_error: 0.1315\n",
            "Epoch 23/37\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0292 - mean_absolute_error: 0.1311 - val_loss: 0.0289 - val_mean_absolute_error: 0.1297\n",
            "Epoch 24/37\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0290 - mean_absolute_error: 0.1307 - val_loss: 0.0304 - val_mean_absolute_error: 0.1346\n",
            "Epoch 25/37\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0294 - mean_absolute_error: 0.1313 - val_loss: 0.0297 - val_mean_absolute_error: 0.1319\n",
            "Epoch 26/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0288 - mean_absolute_error: 0.1308 - val_loss: 0.0281 - val_mean_absolute_error: 0.1276\n",
            "Epoch 27/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0288 - mean_absolute_error: 0.1302 - val_loss: 0.0288 - val_mean_absolute_error: 0.1284\n",
            "Epoch 28/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0287 - mean_absolute_error: 0.1297 - val_loss: 0.0290 - val_mean_absolute_error: 0.1294\n",
            "Epoch 29/37\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0283 - mean_absolute_error: 0.1292 - val_loss: 0.0288 - val_mean_absolute_error: 0.1315\n",
            "Epoch 30/37\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0279 - mean_absolute_error: 0.1279 - val_loss: 0.0304 - val_mean_absolute_error: 0.1340\n",
            "Epoch 31/37\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0280 - mean_absolute_error: 0.1283 - val_loss: 0.0282 - val_mean_absolute_error: 0.1275\n",
            "Epoch 32/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0276 - mean_absolute_error: 0.1276 - val_loss: 0.0285 - val_mean_absolute_error: 0.1282\n",
            "Epoch 33/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1273 - val_loss: 0.0310 - val_mean_absolute_error: 0.1362\n",
            "Epoch 34/37\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0275 - mean_absolute_error: 0.1270 - val_loss: 0.0292 - val_mean_absolute_error: 0.1313\n",
            "Epoch 35/37\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0273 - mean_absolute_error: 0.1268 - val_loss: 0.0277 - val_mean_absolute_error: 0.1268\n",
            "Epoch 36/37\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1275 - val_loss: 0.0276 - val_mean_absolute_error: 0.1265\n",
            "Epoch 37/37\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0269 - mean_absolute_error: 0.1259 - val_loss: 0.0295 - val_mean_absolute_error: 0.1311\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_77 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_78 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/38\n",
            "10381/10381 [==============================] - 18s 2ms/step - loss: 0.0731 - mean_absolute_error: 0.2081 - val_loss: 0.0418 - val_mean_absolute_error: 0.1583\n",
            "Epoch 2/38\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0478 - mean_absolute_error: 0.1706 - val_loss: 0.0422 - val_mean_absolute_error: 0.1553\n",
            "Epoch 3/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0438 - mean_absolute_error: 0.1613 - val_loss: 0.0387 - val_mean_absolute_error: 0.1491\n",
            "Epoch 4/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0406 - mean_absolute_error: 0.1553 - val_loss: 0.0391 - val_mean_absolute_error: 0.1522\n",
            "Epoch 5/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0392 - mean_absolute_error: 0.1529 - val_loss: 0.0412 - val_mean_absolute_error: 0.1647\n",
            "Epoch 6/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0378 - mean_absolute_error: 0.1491 - val_loss: 0.0363 - val_mean_absolute_error: 0.1444\n",
            "Epoch 7/38\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0368 - mean_absolute_error: 0.1465 - val_loss: 0.0344 - val_mean_absolute_error: 0.1407\n",
            "Epoch 8/38\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0358 - mean_absolute_error: 0.1452 - val_loss: 0.0329 - val_mean_absolute_error: 0.1384\n",
            "Epoch 9/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0351 - mean_absolute_error: 0.1434 - val_loss: 0.0326 - val_mean_absolute_error: 0.1380\n",
            "Epoch 10/38\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0348 - mean_absolute_error: 0.1428 - val_loss: 0.0329 - val_mean_absolute_error: 0.1392\n",
            "Epoch 11/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0341 - mean_absolute_error: 0.1424 - val_loss: 0.0349 - val_mean_absolute_error: 0.1470\n",
            "Epoch 12/38\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0335 - mean_absolute_error: 0.1406 - val_loss: 0.0316 - val_mean_absolute_error: 0.1362\n",
            "Epoch 13/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0328 - mean_absolute_error: 0.1387 - val_loss: 0.0332 - val_mean_absolute_error: 0.1406\n",
            "Epoch 14/38\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0327 - mean_absolute_error: 0.1386 - val_loss: 0.0306 - val_mean_absolute_error: 0.1333\n",
            "Epoch 15/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0321 - mean_absolute_error: 0.1378 - val_loss: 0.0316 - val_mean_absolute_error: 0.1364\n",
            "Epoch 16/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0313 - mean_absolute_error: 0.1357 - val_loss: 0.0323 - val_mean_absolute_error: 0.1377\n",
            "Epoch 17/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0319 - mean_absolute_error: 0.1375 - val_loss: 0.0308 - val_mean_absolute_error: 0.1337\n",
            "Epoch 18/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0309 - mean_absolute_error: 0.1347 - val_loss: 0.0323 - val_mean_absolute_error: 0.1387\n",
            "Epoch 19/38\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0308 - mean_absolute_error: 0.1345 - val_loss: 0.0313 - val_mean_absolute_error: 0.1349\n",
            "Epoch 20/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0299 - mean_absolute_error: 0.1335 - val_loss: 0.0289 - val_mean_absolute_error: 0.1297\n",
            "Epoch 21/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0303 - mean_absolute_error: 0.1339 - val_loss: 0.0290 - val_mean_absolute_error: 0.1295\n",
            "Epoch 22/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0301 - mean_absolute_error: 0.1329 - val_loss: 0.0286 - val_mean_absolute_error: 0.1295\n",
            "Epoch 23/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0297 - mean_absolute_error: 0.1318 - val_loss: 0.0298 - val_mean_absolute_error: 0.1319\n",
            "Epoch 24/38\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0297 - mean_absolute_error: 0.1325 - val_loss: 0.0298 - val_mean_absolute_error: 0.1315\n",
            "Epoch 25/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0292 - mean_absolute_error: 0.1315 - val_loss: 0.0305 - val_mean_absolute_error: 0.1337\n",
            "Epoch 26/38\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0289 - mean_absolute_error: 0.1305 - val_loss: 0.0286 - val_mean_absolute_error: 0.1292\n",
            "Epoch 27/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0291 - mean_absolute_error: 0.1308 - val_loss: 0.0287 - val_mean_absolute_error: 0.1291\n",
            "Epoch 28/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0290 - mean_absolute_error: 0.1304 - val_loss: 0.0289 - val_mean_absolute_error: 0.1296\n",
            "Epoch 29/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1298 - val_loss: 0.0283 - val_mean_absolute_error: 0.1283\n",
            "Epoch 30/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0280 - mean_absolute_error: 0.1285 - val_loss: 0.0292 - val_mean_absolute_error: 0.1308\n",
            "Epoch 31/38\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0283 - mean_absolute_error: 0.1288 - val_loss: 0.0296 - val_mean_absolute_error: 0.1308\n",
            "Epoch 32/38\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0277 - mean_absolute_error: 0.1277 - val_loss: 0.0274 - val_mean_absolute_error: 0.1264\n",
            "Epoch 33/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0279 - mean_absolute_error: 0.1287 - val_loss: 0.0292 - val_mean_absolute_error: 0.1315\n",
            "Epoch 34/38\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0276 - mean_absolute_error: 0.1275 - val_loss: 0.0284 - val_mean_absolute_error: 0.1291\n",
            "Epoch 35/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1275 - val_loss: 0.0286 - val_mean_absolute_error: 0.1286\n",
            "Epoch 36/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0274 - mean_absolute_error: 0.1269 - val_loss: 0.0291 - val_mean_absolute_error: 0.1316\n",
            "Epoch 37/38\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0271 - mean_absolute_error: 0.1261 - val_loss: 0.0278 - val_mean_absolute_error: 0.1273\n",
            "Epoch 38/38\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0270 - mean_absolute_error: 0.1264 - val_loss: 0.0276 - val_mean_absolute_error: 0.1263\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_79 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_80 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/39\n",
            "10381/10381 [==============================] - 19s 2ms/step - loss: 0.0735 - mean_absolute_error: 0.2085 - val_loss: 0.0466 - val_mean_absolute_error: 0.1641\n",
            "Epoch 2/39\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0473 - mean_absolute_error: 0.1697 - val_loss: 0.0414 - val_mean_absolute_error: 0.1575\n",
            "Epoch 3/39\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0433 - mean_absolute_error: 0.1615 - val_loss: 0.0420 - val_mean_absolute_error: 0.1573\n",
            "Epoch 4/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0403 - mean_absolute_error: 0.1546 - val_loss: 0.0370 - val_mean_absolute_error: 0.1480\n",
            "Epoch 5/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0388 - mean_absolute_error: 0.1515 - val_loss: 0.0347 - val_mean_absolute_error: 0.1420\n",
            "Epoch 6/39\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0382 - mean_absolute_error: 0.1502 - val_loss: 0.0360 - val_mean_absolute_error: 0.1480\n",
            "Epoch 7/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0364 - mean_absolute_error: 0.1465 - val_loss: 0.0349 - val_mean_absolute_error: 0.1450\n",
            "Epoch 8/39\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0357 - mean_absolute_error: 0.1455 - val_loss: 0.0334 - val_mean_absolute_error: 0.1396\n",
            "Epoch 9/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0352 - mean_absolute_error: 0.1441 - val_loss: 0.0339 - val_mean_absolute_error: 0.1433\n",
            "Epoch 10/39\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0350 - mean_absolute_error: 0.1430 - val_loss: 0.0320 - val_mean_absolute_error: 0.1366\n",
            "Epoch 11/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0335 - mean_absolute_error: 0.1405 - val_loss: 0.0382 - val_mean_absolute_error: 0.1502\n",
            "Epoch 12/39\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0335 - mean_absolute_error: 0.1404 - val_loss: 0.0329 - val_mean_absolute_error: 0.1390\n",
            "Epoch 13/39\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0331 - mean_absolute_error: 0.1394 - val_loss: 0.0342 - val_mean_absolute_error: 0.1413\n",
            "Epoch 14/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0323 - mean_absolute_error: 0.1382 - val_loss: 0.0309 - val_mean_absolute_error: 0.1348\n",
            "Epoch 15/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0323 - mean_absolute_error: 0.1381 - val_loss: 0.0309 - val_mean_absolute_error: 0.1345\n",
            "Epoch 16/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0317 - mean_absolute_error: 0.1372 - val_loss: 0.0315 - val_mean_absolute_error: 0.1352\n",
            "Epoch 17/39\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0314 - mean_absolute_error: 0.1360 - val_loss: 0.0323 - val_mean_absolute_error: 0.1370\n",
            "Epoch 18/39\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0310 - mean_absolute_error: 0.1354 - val_loss: 0.0322 - val_mean_absolute_error: 0.1384\n",
            "Epoch 19/39\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0308 - mean_absolute_error: 0.1342 - val_loss: 0.0319 - val_mean_absolute_error: 0.1374\n",
            "Epoch 20/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0304 - mean_absolute_error: 0.1343 - val_loss: 0.0288 - val_mean_absolute_error: 0.1296\n",
            "Epoch 21/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0302 - mean_absolute_error: 0.1334 - val_loss: 0.0301 - val_mean_absolute_error: 0.1324\n",
            "Epoch 22/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0298 - mean_absolute_error: 0.1322 - val_loss: 0.0294 - val_mean_absolute_error: 0.1314\n",
            "Epoch 23/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0296 - mean_absolute_error: 0.1324 - val_loss: 0.0289 - val_mean_absolute_error: 0.1296\n",
            "Epoch 24/39\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0295 - mean_absolute_error: 0.1318 - val_loss: 0.0293 - val_mean_absolute_error: 0.1310\n",
            "Epoch 25/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0289 - mean_absolute_error: 0.1304 - val_loss: 0.0289 - val_mean_absolute_error: 0.1298\n",
            "Epoch 26/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0293 - mean_absolute_error: 0.1311 - val_loss: 0.0284 - val_mean_absolute_error: 0.1282\n",
            "Epoch 27/39\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1302 - val_loss: 0.0288 - val_mean_absolute_error: 0.1293\n",
            "Epoch 28/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0289 - mean_absolute_error: 0.1302 - val_loss: 0.0320 - val_mean_absolute_error: 0.1359\n",
            "Epoch 29/39\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0285 - mean_absolute_error: 0.1293 - val_loss: 0.0283 - val_mean_absolute_error: 0.1287\n",
            "Epoch 30/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0283 - mean_absolute_error: 0.1293 - val_loss: 0.0293 - val_mean_absolute_error: 0.1310\n",
            "Epoch 31/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0281 - mean_absolute_error: 0.1286 - val_loss: 0.0283 - val_mean_absolute_error: 0.1280\n",
            "Epoch 32/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0281 - mean_absolute_error: 0.1282 - val_loss: 0.0285 - val_mean_absolute_error: 0.1285\n",
            "Epoch 33/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0281 - mean_absolute_error: 0.1287 - val_loss: 0.0303 - val_mean_absolute_error: 0.1330\n",
            "Epoch 34/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0278 - mean_absolute_error: 0.1282 - val_loss: 0.0278 - val_mean_absolute_error: 0.1265\n",
            "Epoch 35/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0271 - mean_absolute_error: 0.1267 - val_loss: 0.0282 - val_mean_absolute_error: 0.1273\n",
            "Epoch 36/39\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0270 - mean_absolute_error: 0.1261 - val_loss: 0.0284 - val_mean_absolute_error: 0.1278\n",
            "Epoch 37/39\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0272 - mean_absolute_error: 0.1267 - val_loss: 0.0279 - val_mean_absolute_error: 0.1271\n",
            "Epoch 38/39\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0270 - mean_absolute_error: 0.1262 - val_loss: 0.0271 - val_mean_absolute_error: 0.1255\n",
            "Epoch 39/39\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0269 - mean_absolute_error: 0.1260 - val_loss: 0.0273 - val_mean_absolute_error: 0.1262\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_81 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_82 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/40\n",
            "10381/10381 [==============================] - 19s 2ms/step - loss: 0.0707 - mean_absolute_error: 0.2046 - val_loss: 0.0455 - val_mean_absolute_error: 0.1677\n",
            "Epoch 2/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0481 - mean_absolute_error: 0.1705 - val_loss: 0.0404 - val_mean_absolute_error: 0.1585\n",
            "Epoch 3/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0438 - mean_absolute_error: 0.1620 - val_loss: 0.0404 - val_mean_absolute_error: 0.1582\n",
            "Epoch 4/40\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0406 - mean_absolute_error: 0.1552 - val_loss: 0.0365 - val_mean_absolute_error: 0.1480\n",
            "Epoch 5/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0396 - mean_absolute_error: 0.1529 - val_loss: 0.0387 - val_mean_absolute_error: 0.1503\n",
            "Epoch 6/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0380 - mean_absolute_error: 0.1499 - val_loss: 0.0345 - val_mean_absolute_error: 0.1417\n",
            "Epoch 7/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0365 - mean_absolute_error: 0.1464 - val_loss: 0.0334 - val_mean_absolute_error: 0.1398\n",
            "Epoch 8/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0359 - mean_absolute_error: 0.1456 - val_loss: 0.0341 - val_mean_absolute_error: 0.1423\n",
            "Epoch 9/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0353 - mean_absolute_error: 0.1445 - val_loss: 0.0374 - val_mean_absolute_error: 0.1504\n",
            "Epoch 10/40\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0344 - mean_absolute_error: 0.1425 - val_loss: 0.0324 - val_mean_absolute_error: 0.1376\n",
            "Epoch 11/40\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0340 - mean_absolute_error: 0.1421 - val_loss: 0.0321 - val_mean_absolute_error: 0.1373\n",
            "Epoch 12/40\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0334 - mean_absolute_error: 0.1400 - val_loss: 0.0319 - val_mean_absolute_error: 0.1362\n",
            "Epoch 13/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0328 - mean_absolute_error: 0.1392 - val_loss: 0.0310 - val_mean_absolute_error: 0.1347\n",
            "Epoch 14/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0327 - mean_absolute_error: 0.1389 - val_loss: 0.0320 - val_mean_absolute_error: 0.1365\n",
            "Epoch 15/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0321 - mean_absolute_error: 0.1374 - val_loss: 0.0315 - val_mean_absolute_error: 0.1365\n",
            "Epoch 16/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0317 - mean_absolute_error: 0.1373 - val_loss: 0.0309 - val_mean_absolute_error: 0.1341\n",
            "Epoch 17/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0315 - mean_absolute_error: 0.1363 - val_loss: 0.0315 - val_mean_absolute_error: 0.1357\n",
            "Epoch 18/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0308 - mean_absolute_error: 0.1353 - val_loss: 0.0336 - val_mean_absolute_error: 0.1410\n",
            "Epoch 19/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0309 - mean_absolute_error: 0.1346 - val_loss: 0.0301 - val_mean_absolute_error: 0.1318\n",
            "Epoch 20/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0302 - mean_absolute_error: 0.1331 - val_loss: 0.0339 - val_mean_absolute_error: 0.1431\n",
            "Epoch 21/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0300 - mean_absolute_error: 0.1332 - val_loss: 0.0322 - val_mean_absolute_error: 0.1373\n",
            "Epoch 22/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0298 - mean_absolute_error: 0.1328 - val_loss: 0.0303 - val_mean_absolute_error: 0.1340\n",
            "Epoch 23/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0299 - mean_absolute_error: 0.1326 - val_loss: 0.0295 - val_mean_absolute_error: 0.1311\n",
            "Epoch 24/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0295 - mean_absolute_error: 0.1323 - val_loss: 0.0296 - val_mean_absolute_error: 0.1319\n",
            "Epoch 25/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0291 - mean_absolute_error: 0.1305 - val_loss: 0.0299 - val_mean_absolute_error: 0.1351\n",
            "Epoch 26/40\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0294 - mean_absolute_error: 0.1320 - val_loss: 0.0293 - val_mean_absolute_error: 0.1293\n",
            "Epoch 27/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0287 - mean_absolute_error: 0.1302 - val_loss: 0.0283 - val_mean_absolute_error: 0.1278\n",
            "Epoch 28/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1299 - val_loss: 0.0286 - val_mean_absolute_error: 0.1286\n",
            "Epoch 29/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0283 - mean_absolute_error: 0.1292 - val_loss: 0.0301 - val_mean_absolute_error: 0.1316\n",
            "Epoch 30/40\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0286 - mean_absolute_error: 0.1297 - val_loss: 0.0274 - val_mean_absolute_error: 0.1262\n",
            "Epoch 31/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0281 - mean_absolute_error: 0.1285 - val_loss: 0.0290 - val_mean_absolute_error: 0.1298\n",
            "Epoch 32/40\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0275 - mean_absolute_error: 0.1274 - val_loss: 0.0308 - val_mean_absolute_error: 0.1377\n",
            "Epoch 33/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0277 - mean_absolute_error: 0.1279 - val_loss: 0.0280 - val_mean_absolute_error: 0.1275\n",
            "Epoch 34/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0276 - mean_absolute_error: 0.1278 - val_loss: 0.0283 - val_mean_absolute_error: 0.1278\n",
            "Epoch 35/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0273 - mean_absolute_error: 0.1266 - val_loss: 0.0305 - val_mean_absolute_error: 0.1343\n",
            "Epoch 36/40\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0272 - mean_absolute_error: 0.1264 - val_loss: 0.0313 - val_mean_absolute_error: 0.1353\n",
            "Epoch 37/40\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0269 - mean_absolute_error: 0.1260 - val_loss: 0.0281 - val_mean_absolute_error: 0.1288\n",
            "Epoch 38/40\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0273 - mean_absolute_error: 0.1268 - val_loss: 0.0283 - val_mean_absolute_error: 0.1280\n",
            "Epoch 39/40\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0267 - mean_absolute_error: 0.1257 - val_loss: 0.0291 - val_mean_absolute_error: 0.1292\n",
            "Epoch 40/40\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0267 - mean_absolute_error: 0.1251 - val_loss: 0.0291 - val_mean_absolute_error: 0.1299\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_83 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_84 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/41\n",
            "10381/10381 [==============================] - 20s 2ms/step - loss: 0.0718 - mean_absolute_error: 0.2064 - val_loss: 0.0420 - val_mean_absolute_error: 0.1590\n",
            "Epoch 2/41\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0467 - mean_absolute_error: 0.1679 - val_loss: 0.0404 - val_mean_absolute_error: 0.1526\n",
            "Epoch 3/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0436 - mean_absolute_error: 0.1617 - val_loss: 0.0391 - val_mean_absolute_error: 0.1508\n",
            "Epoch 4/41\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0396 - mean_absolute_error: 0.1533 - val_loss: 0.0367 - val_mean_absolute_error: 0.1473\n",
            "Epoch 5/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0385 - mean_absolute_error: 0.1509 - val_loss: 0.0350 - val_mean_absolute_error: 0.1446\n",
            "Epoch 6/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0382 - mean_absolute_error: 0.1499 - val_loss: 0.0359 - val_mean_absolute_error: 0.1474\n",
            "Epoch 7/41\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0370 - mean_absolute_error: 0.1475 - val_loss: 0.0348 - val_mean_absolute_error: 0.1426\n",
            "Epoch 8/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0361 - mean_absolute_error: 0.1461 - val_loss: 0.0338 - val_mean_absolute_error: 0.1392\n",
            "Epoch 9/41\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0356 - mean_absolute_error: 0.1448 - val_loss: 0.0331 - val_mean_absolute_error: 0.1392\n",
            "Epoch 10/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0346 - mean_absolute_error: 0.1423 - val_loss: 0.0323 - val_mean_absolute_error: 0.1384\n",
            "Epoch 11/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0337 - mean_absolute_error: 0.1410 - val_loss: 0.0348 - val_mean_absolute_error: 0.1428\n",
            "Epoch 12/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0336 - mean_absolute_error: 0.1410 - val_loss: 0.0333 - val_mean_absolute_error: 0.1404\n",
            "Epoch 13/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0331 - mean_absolute_error: 0.1395 - val_loss: 0.0327 - val_mean_absolute_error: 0.1383\n",
            "Epoch 14/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0327 - mean_absolute_error: 0.1390 - val_loss: 0.0300 - val_mean_absolute_error: 0.1325\n",
            "Epoch 15/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0319 - mean_absolute_error: 0.1375 - val_loss: 0.0339 - val_mean_absolute_error: 0.1404\n",
            "Epoch 16/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0320 - mean_absolute_error: 0.1370 - val_loss: 0.0312 - val_mean_absolute_error: 0.1346\n",
            "Epoch 17/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0309 - mean_absolute_error: 0.1351 - val_loss: 0.0309 - val_mean_absolute_error: 0.1347\n",
            "Epoch 18/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0309 - mean_absolute_error: 0.1349 - val_loss: 0.0304 - val_mean_absolute_error: 0.1336\n",
            "Epoch 19/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0307 - mean_absolute_error: 0.1347 - val_loss: 0.0303 - val_mean_absolute_error: 0.1340\n",
            "Epoch 20/41\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0304 - mean_absolute_error: 0.1341 - val_loss: 0.0299 - val_mean_absolute_error: 0.1334\n",
            "Epoch 21/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0299 - mean_absolute_error: 0.1329 - val_loss: 0.0306 - val_mean_absolute_error: 0.1331\n",
            "Epoch 22/41\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0297 - mean_absolute_error: 0.1319 - val_loss: 0.0322 - val_mean_absolute_error: 0.1386\n",
            "Epoch 23/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0295 - mean_absolute_error: 0.1322 - val_loss: 0.0305 - val_mean_absolute_error: 0.1360\n",
            "Epoch 24/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0294 - mean_absolute_error: 0.1316 - val_loss: 0.0287 - val_mean_absolute_error: 0.1288\n",
            "Epoch 25/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0288 - mean_absolute_error: 0.1298 - val_loss: 0.0286 - val_mean_absolute_error: 0.1286\n",
            "Epoch 26/41\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0288 - mean_absolute_error: 0.1304 - val_loss: 0.0297 - val_mean_absolute_error: 0.1324\n",
            "Epoch 27/41\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0287 - mean_absolute_error: 0.1304 - val_loss: 0.0304 - val_mean_absolute_error: 0.1325\n",
            "Epoch 28/41\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0286 - mean_absolute_error: 0.1295 - val_loss: 0.0293 - val_mean_absolute_error: 0.1312\n",
            "Epoch 29/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0282 - mean_absolute_error: 0.1288 - val_loss: 0.0287 - val_mean_absolute_error: 0.1290\n",
            "Epoch 30/41\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0281 - mean_absolute_error: 0.1289 - val_loss: 0.0283 - val_mean_absolute_error: 0.1279\n",
            "Epoch 31/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1275 - val_loss: 0.0299 - val_mean_absolute_error: 0.1310\n",
            "Epoch 32/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0274 - mean_absolute_error: 0.1271 - val_loss: 0.0297 - val_mean_absolute_error: 0.1335\n",
            "Epoch 33/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0283 - mean_absolute_error: 0.1294 - val_loss: 0.0286 - val_mean_absolute_error: 0.1302\n",
            "Epoch 34/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0274 - mean_absolute_error: 0.1264 - val_loss: 0.0288 - val_mean_absolute_error: 0.1321\n",
            "Epoch 35/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0275 - mean_absolute_error: 0.1278 - val_loss: 0.0299 - val_mean_absolute_error: 0.1316\n",
            "Epoch 36/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0272 - mean_absolute_error: 0.1267 - val_loss: 0.0292 - val_mean_absolute_error: 0.1297\n",
            "Epoch 37/41\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0269 - mean_absolute_error: 0.1258 - val_loss: 0.0281 - val_mean_absolute_error: 0.1275\n",
            "Epoch 38/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0269 - mean_absolute_error: 0.1262 - val_loss: 0.0282 - val_mean_absolute_error: 0.1286\n",
            "Epoch 39/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0269 - mean_absolute_error: 0.1257 - val_loss: 0.0273 - val_mean_absolute_error: 0.1259\n",
            "Epoch 40/41\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0265 - mean_absolute_error: 0.1255 - val_loss: 0.0296 - val_mean_absolute_error: 0.1323\n",
            "Epoch 41/41\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0264 - mean_absolute_error: 0.1251 - val_loss: 0.0273 - val_mean_absolute_error: 0.1256\n",
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_85 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_86 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/42\n",
            "10381/10381 [==============================] - 20s 2ms/step - loss: 0.0727 - mean_absolute_error: 0.2063 - val_loss: 0.0487 - val_mean_absolute_error: 0.1754\n",
            "Epoch 2/42\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0468 - mean_absolute_error: 0.1681 - val_loss: 0.0394 - val_mean_absolute_error: 0.1531\n",
            "Epoch 3/42\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0434 - mean_absolute_error: 0.1619 - val_loss: 0.0383 - val_mean_absolute_error: 0.1494\n",
            "Epoch 4/42\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0411 - mean_absolute_error: 0.1564 - val_loss: 0.0356 - val_mean_absolute_error: 0.1444\n",
            "Epoch 5/42\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0389 - mean_absolute_error: 0.1513 - val_loss: 0.0359 - val_mean_absolute_error: 0.1468\n",
            "Epoch 6/42\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0382 - mean_absolute_error: 0.1504 - val_loss: 0.0343 - val_mean_absolute_error: 0.1423\n",
            "Epoch 7/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0368 - mean_absolute_error: 0.1478 - val_loss: 0.0368 - val_mean_absolute_error: 0.1493\n",
            "Epoch 8/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0360 - mean_absolute_error: 0.1453 - val_loss: 0.0351 - val_mean_absolute_error: 0.1428\n",
            "Epoch 9/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0359 - mean_absolute_error: 0.1456 - val_loss: 0.0345 - val_mean_absolute_error: 0.1428\n",
            "Epoch 10/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0350 - mean_absolute_error: 0.1441 - val_loss: 0.0333 - val_mean_absolute_error: 0.1396\n",
            "Epoch 11/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0339 - mean_absolute_error: 0.1412 - val_loss: 0.0357 - val_mean_absolute_error: 0.1464\n",
            "Epoch 12/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0334 - mean_absolute_error: 0.1401 - val_loss: 0.0314 - val_mean_absolute_error: 0.1349\n",
            "Epoch 13/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0335 - mean_absolute_error: 0.1406 - val_loss: 0.0357 - val_mean_absolute_error: 0.1487\n",
            "Epoch 14/42\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0324 - mean_absolute_error: 0.1384 - val_loss: 0.0324 - val_mean_absolute_error: 0.1396\n",
            "Epoch 15/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0322 - mean_absolute_error: 0.1378 - val_loss: 0.0350 - val_mean_absolute_error: 0.1424\n",
            "Epoch 16/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0316 - mean_absolute_error: 0.1369 - val_loss: 0.0313 - val_mean_absolute_error: 0.1363\n",
            "Epoch 17/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0314 - mean_absolute_error: 0.1364 - val_loss: 0.0304 - val_mean_absolute_error: 0.1325\n",
            "Epoch 18/42\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0313 - mean_absolute_error: 0.1363 - val_loss: 0.0305 - val_mean_absolute_error: 0.1327\n",
            "Epoch 19/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0308 - mean_absolute_error: 0.1351 - val_loss: 0.0308 - val_mean_absolute_error: 0.1343\n",
            "Epoch 20/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0310 - mean_absolute_error: 0.1345 - val_loss: 0.0315 - val_mean_absolute_error: 0.1370\n",
            "Epoch 21/42\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0302 - mean_absolute_error: 0.1336 - val_loss: 0.0294 - val_mean_absolute_error: 0.1307\n",
            "Epoch 22/42\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0303 - mean_absolute_error: 0.1336 - val_loss: 0.0299 - val_mean_absolute_error: 0.1310\n",
            "Epoch 23/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0297 - mean_absolute_error: 0.1326 - val_loss: 0.0308 - val_mean_absolute_error: 0.1353\n",
            "Epoch 24/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0298 - mean_absolute_error: 0.1326 - val_loss: 0.0305 - val_mean_absolute_error: 0.1347\n",
            "Epoch 25/42\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0292 - mean_absolute_error: 0.1310 - val_loss: 0.0294 - val_mean_absolute_error: 0.1318\n",
            "Epoch 26/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0291 - mean_absolute_error: 0.1306 - val_loss: 0.0297 - val_mean_absolute_error: 0.1318\n",
            "Epoch 27/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0289 - mean_absolute_error: 0.1302 - val_loss: 0.0289 - val_mean_absolute_error: 0.1298\n",
            "Epoch 28/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0287 - mean_absolute_error: 0.1301 - val_loss: 0.0301 - val_mean_absolute_error: 0.1335\n",
            "Epoch 29/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0286 - mean_absolute_error: 0.1299 - val_loss: 0.0284 - val_mean_absolute_error: 0.1278\n",
            "Epoch 30/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0282 - mean_absolute_error: 0.1287 - val_loss: 0.0279 - val_mean_absolute_error: 0.1275\n",
            "Epoch 31/42\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0280 - mean_absolute_error: 0.1283 - val_loss: 0.0287 - val_mean_absolute_error: 0.1289\n",
            "Epoch 32/42\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0280 - mean_absolute_error: 0.1286 - val_loss: 0.0280 - val_mean_absolute_error: 0.1262\n",
            "Epoch 33/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0277 - mean_absolute_error: 0.1271 - val_loss: 0.0324 - val_mean_absolute_error: 0.1380\n",
            "Epoch 34/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1274 - val_loss: 0.0279 - val_mean_absolute_error: 0.1277\n",
            "Epoch 35/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0273 - mean_absolute_error: 0.1268 - val_loss: 0.0272 - val_mean_absolute_error: 0.1255\n",
            "Epoch 36/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0273 - mean_absolute_error: 0.1272 - val_loss: 0.0281 - val_mean_absolute_error: 0.1275\n",
            "Epoch 37/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0268 - mean_absolute_error: 0.1247 - val_loss: 0.0282 - val_mean_absolute_error: 0.1291\n",
            "Epoch 38/42\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0269 - mean_absolute_error: 0.1259 - val_loss: 0.0273 - val_mean_absolute_error: 0.1258\n",
            "Epoch 39/42\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0267 - mean_absolute_error: 0.1252 - val_loss: 0.0295 - val_mean_absolute_error: 0.1309\n",
            "Epoch 40/42\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0271 - mean_absolute_error: 0.1263 - val_loss: 0.0275 - val_mean_absolute_error: 0.1260\n",
            "Epoch 41/42\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0266 - mean_absolute_error: 0.1249 - val_loss: 0.0280 - val_mean_absolute_error: 0.1272\n",
            "Epoch 42/42\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0268 - mean_absolute_error: 0.1253 - val_loss: 0.0273 - val_mean_absolute_error: 0.1261\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_87 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_88 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/43\n",
            "10381/10381 [==============================] - 21s 2ms/step - loss: 0.0728 - mean_absolute_error: 0.2080 - val_loss: 0.0493 - val_mean_absolute_error: 0.1799\n",
            "Epoch 2/43\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0470 - mean_absolute_error: 0.1684 - val_loss: 0.0407 - val_mean_absolute_error: 0.1539\n",
            "Epoch 3/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0431 - mean_absolute_error: 0.1605 - val_loss: 0.0411 - val_mean_absolute_error: 0.1595\n",
            "Epoch 4/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0407 - mean_absolute_error: 0.1549 - val_loss: 0.0377 - val_mean_absolute_error: 0.1524\n",
            "Epoch 5/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0392 - mean_absolute_error: 0.1524 - val_loss: 0.0348 - val_mean_absolute_error: 0.1423\n",
            "Epoch 6/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0379 - mean_absolute_error: 0.1496 - val_loss: 0.0346 - val_mean_absolute_error: 0.1418\n",
            "Epoch 7/43\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0366 - mean_absolute_error: 0.1468 - val_loss: 0.0347 - val_mean_absolute_error: 0.1416\n",
            "Epoch 8/43\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0356 - mean_absolute_error: 0.1448 - val_loss: 0.0341 - val_mean_absolute_error: 0.1408\n",
            "Epoch 9/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0350 - mean_absolute_error: 0.1438 - val_loss: 0.0328 - val_mean_absolute_error: 0.1393\n",
            "Epoch 10/43\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0339 - mean_absolute_error: 0.1413 - val_loss: 0.0333 - val_mean_absolute_error: 0.1410\n",
            "Epoch 11/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0334 - mean_absolute_error: 0.1407 - val_loss: 0.0317 - val_mean_absolute_error: 0.1358\n",
            "Epoch 12/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0336 - mean_absolute_error: 0.1411 - val_loss: 0.0310 - val_mean_absolute_error: 0.1345\n",
            "Epoch 13/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0328 - mean_absolute_error: 0.1392 - val_loss: 0.0319 - val_mean_absolute_error: 0.1368\n",
            "Epoch 14/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0324 - mean_absolute_error: 0.1388 - val_loss: 0.0321 - val_mean_absolute_error: 0.1379\n",
            "Epoch 15/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0319 - mean_absolute_error: 0.1369 - val_loss: 0.0309 - val_mean_absolute_error: 0.1343\n",
            "Epoch 16/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0317 - mean_absolute_error: 0.1368 - val_loss: 0.0311 - val_mean_absolute_error: 0.1362\n",
            "Epoch 17/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0317 - mean_absolute_error: 0.1371 - val_loss: 0.0305 - val_mean_absolute_error: 0.1340\n",
            "Epoch 18/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0309 - mean_absolute_error: 0.1355 - val_loss: 0.0333 - val_mean_absolute_error: 0.1404\n",
            "Epoch 19/43\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0310 - mean_absolute_error: 0.1349 - val_loss: 0.0307 - val_mean_absolute_error: 0.1347\n",
            "Epoch 20/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0302 - mean_absolute_error: 0.1332 - val_loss: 0.0305 - val_mean_absolute_error: 0.1333\n",
            "Epoch 21/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0301 - mean_absolute_error: 0.1335 - val_loss: 0.0288 - val_mean_absolute_error: 0.1291\n",
            "Epoch 22/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0300 - mean_absolute_error: 0.1328 - val_loss: 0.0295 - val_mean_absolute_error: 0.1313\n",
            "Epoch 23/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0294 - mean_absolute_error: 0.1315 - val_loss: 0.0286 - val_mean_absolute_error: 0.1295\n",
            "Epoch 24/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0293 - mean_absolute_error: 0.1315 - val_loss: 0.0292 - val_mean_absolute_error: 0.1313\n",
            "Epoch 25/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0289 - mean_absolute_error: 0.1305 - val_loss: 0.0286 - val_mean_absolute_error: 0.1288\n",
            "Epoch 26/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0291 - mean_absolute_error: 0.1315 - val_loss: 0.0289 - val_mean_absolute_error: 0.1303\n",
            "Epoch 27/43\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0289 - mean_absolute_error: 0.1305 - val_loss: 0.0284 - val_mean_absolute_error: 0.1275\n",
            "Epoch 28/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0286 - mean_absolute_error: 0.1304 - val_loss: 0.0290 - val_mean_absolute_error: 0.1313\n",
            "Epoch 29/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0281 - mean_absolute_error: 0.1290 - val_loss: 0.0329 - val_mean_absolute_error: 0.1396\n",
            "Epoch 30/43\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0281 - mean_absolute_error: 0.1289 - val_loss: 0.0278 - val_mean_absolute_error: 0.1278\n",
            "Epoch 31/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0277 - mean_absolute_error: 0.1278 - val_loss: 0.0274 - val_mean_absolute_error: 0.1258\n",
            "Epoch 32/43\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0280 - mean_absolute_error: 0.1290 - val_loss: 0.0286 - val_mean_absolute_error: 0.1279\n",
            "Epoch 33/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0277 - mean_absolute_error: 0.1270 - val_loss: 0.0290 - val_mean_absolute_error: 0.1299\n",
            "Epoch 34/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0275 - mean_absolute_error: 0.1272 - val_loss: 0.0311 - val_mean_absolute_error: 0.1340\n",
            "Epoch 35/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0273 - mean_absolute_error: 0.1267 - val_loss: 0.0287 - val_mean_absolute_error: 0.1317\n",
            "Epoch 36/43\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0273 - mean_absolute_error: 0.1265 - val_loss: 0.0281 - val_mean_absolute_error: 0.1279\n",
            "Epoch 37/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0271 - mean_absolute_error: 0.1262 - val_loss: 0.0274 - val_mean_absolute_error: 0.1260\n",
            "Epoch 38/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0268 - mean_absolute_error: 0.1254 - val_loss: 0.0279 - val_mean_absolute_error: 0.1281\n",
            "Epoch 39/43\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0265 - mean_absolute_error: 0.1249 - val_loss: 0.0279 - val_mean_absolute_error: 0.1273\n",
            "Epoch 40/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0263 - mean_absolute_error: 0.1243 - val_loss: 0.0273 - val_mean_absolute_error: 0.1265\n",
            "Epoch 41/43\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0264 - mean_absolute_error: 0.1248 - val_loss: 0.0288 - val_mean_absolute_error: 0.1311\n",
            "Epoch 42/43\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0265 - mean_absolute_error: 0.1254 - val_loss: 0.0287 - val_mean_absolute_error: 0.1320\n",
            "Epoch 43/43\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0259 - mean_absolute_error: 0.1238 - val_loss: 0.0276 - val_mean_absolute_error: 0.1279\n",
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_89 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_90 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/44\n",
            "10381/10381 [==============================] - 21s 2ms/step - loss: 0.0739 - mean_absolute_error: 0.2081 - val_loss: 0.0465 - val_mean_absolute_error: 0.1726\n",
            "Epoch 2/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0468 - mean_absolute_error: 0.1686 - val_loss: 0.0411 - val_mean_absolute_error: 0.1551\n",
            "Epoch 3/44\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0436 - mean_absolute_error: 0.1616 - val_loss: 0.0441 - val_mean_absolute_error: 0.1607\n",
            "Epoch 4/44\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0408 - mean_absolute_error: 0.1559 - val_loss: 0.0362 - val_mean_absolute_error: 0.1446\n",
            "Epoch 5/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0392 - mean_absolute_error: 0.1521 - val_loss: 0.0354 - val_mean_absolute_error: 0.1425\n",
            "Epoch 6/44\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0375 - mean_absolute_error: 0.1491 - val_loss: 0.0345 - val_mean_absolute_error: 0.1429\n",
            "Epoch 7/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0370 - mean_absolute_error: 0.1480 - val_loss: 0.0360 - val_mean_absolute_error: 0.1448\n",
            "Epoch 8/44\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0358 - mean_absolute_error: 0.1447 - val_loss: 0.0364 - val_mean_absolute_error: 0.1462\n",
            "Epoch 9/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0354 - mean_absolute_error: 0.1454 - val_loss: 0.0344 - val_mean_absolute_error: 0.1416\n",
            "Epoch 10/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0346 - mean_absolute_error: 0.1429 - val_loss: 0.0345 - val_mean_absolute_error: 0.1425\n",
            "Epoch 11/44\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0339 - mean_absolute_error: 0.1414 - val_loss: 0.0331 - val_mean_absolute_error: 0.1411\n",
            "Epoch 12/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0333 - mean_absolute_error: 0.1406 - val_loss: 0.0329 - val_mean_absolute_error: 0.1389\n",
            "Epoch 13/44\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0333 - mean_absolute_error: 0.1400 - val_loss: 0.0311 - val_mean_absolute_error: 0.1349\n",
            "Epoch 14/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0321 - mean_absolute_error: 0.1377 - val_loss: 0.0322 - val_mean_absolute_error: 0.1363\n",
            "Epoch 15/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0323 - mean_absolute_error: 0.1374 - val_loss: 0.0310 - val_mean_absolute_error: 0.1353\n",
            "Epoch 16/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0319 - mean_absolute_error: 0.1371 - val_loss: 0.0305 - val_mean_absolute_error: 0.1338\n",
            "Epoch 17/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0313 - mean_absolute_error: 0.1359 - val_loss: 0.0313 - val_mean_absolute_error: 0.1349\n",
            "Epoch 18/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0312 - mean_absolute_error: 0.1357 - val_loss: 0.0306 - val_mean_absolute_error: 0.1343\n",
            "Epoch 19/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0307 - mean_absolute_error: 0.1347 - val_loss: 0.0324 - val_mean_absolute_error: 0.1373\n",
            "Epoch 20/44\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0304 - mean_absolute_error: 0.1334 - val_loss: 0.0298 - val_mean_absolute_error: 0.1337\n",
            "Epoch 21/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0302 - mean_absolute_error: 0.1338 - val_loss: 0.0295 - val_mean_absolute_error: 0.1314\n",
            "Epoch 22/44\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0300 - mean_absolute_error: 0.1333 - val_loss: 0.0295 - val_mean_absolute_error: 0.1312\n",
            "Epoch 23/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0296 - mean_absolute_error: 0.1321 - val_loss: 0.0281 - val_mean_absolute_error: 0.1281\n",
            "Epoch 24/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0291 - mean_absolute_error: 0.1319 - val_loss: 0.0286 - val_mean_absolute_error: 0.1284\n",
            "Epoch 25/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0294 - mean_absolute_error: 0.1310 - val_loss: 0.0305 - val_mean_absolute_error: 0.1343\n",
            "Epoch 26/44\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0290 - mean_absolute_error: 0.1308 - val_loss: 0.0303 - val_mean_absolute_error: 0.1321\n",
            "Epoch 27/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0287 - mean_absolute_error: 0.1304 - val_loss: 0.0286 - val_mean_absolute_error: 0.1301\n",
            "Epoch 28/44\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0284 - mean_absolute_error: 0.1297 - val_loss: 0.0282 - val_mean_absolute_error: 0.1278\n",
            "Epoch 29/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0277 - mean_absolute_error: 0.1279 - val_loss: 0.0312 - val_mean_absolute_error: 0.1344\n",
            "Epoch 30/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0281 - mean_absolute_error: 0.1286 - val_loss: 0.0275 - val_mean_absolute_error: 0.1261\n",
            "Epoch 31/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0279 - mean_absolute_error: 0.1276 - val_loss: 0.0279 - val_mean_absolute_error: 0.1271\n",
            "Epoch 32/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0277 - mean_absolute_error: 0.1283 - val_loss: 0.0297 - val_mean_absolute_error: 0.1294\n",
            "Epoch 33/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0278 - mean_absolute_error: 0.1275 - val_loss: 0.0292 - val_mean_absolute_error: 0.1338\n",
            "Epoch 34/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0274 - mean_absolute_error: 0.1272 - val_loss: 0.0274 - val_mean_absolute_error: 0.1260\n",
            "Epoch 35/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0273 - mean_absolute_error: 0.1265 - val_loss: 0.0281 - val_mean_absolute_error: 0.1291\n",
            "Epoch 36/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0271 - mean_absolute_error: 0.1259 - val_loss: 0.0286 - val_mean_absolute_error: 0.1293\n",
            "Epoch 37/44\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0269 - mean_absolute_error: 0.1258 - val_loss: 0.0279 - val_mean_absolute_error: 0.1280\n",
            "Epoch 38/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0271 - mean_absolute_error: 0.1266 - val_loss: 0.0269 - val_mean_absolute_error: 0.1246\n",
            "Epoch 39/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0269 - mean_absolute_error: 0.1262 - val_loss: 0.0273 - val_mean_absolute_error: 0.1258\n",
            "Epoch 40/44\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0263 - mean_absolute_error: 0.1248 - val_loss: 0.0273 - val_mean_absolute_error: 0.1265\n",
            "Epoch 41/44\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0263 - mean_absolute_error: 0.1247 - val_loss: 0.0286 - val_mean_absolute_error: 0.1279\n",
            "Epoch 42/44\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0263 - mean_absolute_error: 0.1244 - val_loss: 0.0272 - val_mean_absolute_error: 0.1254\n",
            "Epoch 43/44\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0261 - mean_absolute_error: 0.1238 - val_loss: 0.0287 - val_mean_absolute_error: 0.1295\n",
            "Epoch 44/44\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0263 - mean_absolute_error: 0.1242 - val_loss: 0.0264 - val_mean_absolute_error: 0.1237\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_91 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_92 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/45\n",
            "10381/10381 [==============================] - 22s 2ms/step - loss: 0.0716 - mean_absolute_error: 0.2057 - val_loss: 0.0428 - val_mean_absolute_error: 0.1580\n",
            "Epoch 2/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0473 - mean_absolute_error: 0.1686 - val_loss: 0.0490 - val_mean_absolute_error: 0.1791\n",
            "Epoch 3/45\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0436 - mean_absolute_error: 0.1615 - val_loss: 0.0387 - val_mean_absolute_error: 0.1509\n",
            "Epoch 4/45\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0407 - mean_absolute_error: 0.1558 - val_loss: 0.0353 - val_mean_absolute_error: 0.1442\n",
            "Epoch 5/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0390 - mean_absolute_error: 0.1517 - val_loss: 0.0373 - val_mean_absolute_error: 0.1471\n",
            "Epoch 6/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0379 - mean_absolute_error: 0.1492 - val_loss: 0.0338 - val_mean_absolute_error: 0.1402\n",
            "Epoch 7/45\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0373 - mean_absolute_error: 0.1484 - val_loss: 0.0335 - val_mean_absolute_error: 0.1400\n",
            "Epoch 8/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0355 - mean_absolute_error: 0.1443 - val_loss: 0.0342 - val_mean_absolute_error: 0.1415\n",
            "Epoch 9/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0348 - mean_absolute_error: 0.1433 - val_loss: 0.0373 - val_mean_absolute_error: 0.1482\n",
            "Epoch 10/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0342 - mean_absolute_error: 0.1414 - val_loss: 0.0356 - val_mean_absolute_error: 0.1475\n",
            "Epoch 11/45\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0342 - mean_absolute_error: 0.1421 - val_loss: 0.0355 - val_mean_absolute_error: 0.1456\n",
            "Epoch 12/45\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0334 - mean_absolute_error: 0.1407 - val_loss: 0.0336 - val_mean_absolute_error: 0.1403\n",
            "Epoch 13/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0337 - mean_absolute_error: 0.1407 - val_loss: 0.0331 - val_mean_absolute_error: 0.1388\n",
            "Epoch 14/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0325 - mean_absolute_error: 0.1384 - val_loss: 0.0324 - val_mean_absolute_error: 0.1363\n",
            "Epoch 15/45\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0322 - mean_absolute_error: 0.1381 - val_loss: 0.0314 - val_mean_absolute_error: 0.1373\n",
            "Epoch 16/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0319 - mean_absolute_error: 0.1376 - val_loss: 0.0306 - val_mean_absolute_error: 0.1340\n",
            "Epoch 17/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0315 - mean_absolute_error: 0.1366 - val_loss: 0.0311 - val_mean_absolute_error: 0.1361\n",
            "Epoch 18/45\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0312 - mean_absolute_error: 0.1354 - val_loss: 0.0299 - val_mean_absolute_error: 0.1320\n",
            "Epoch 19/45\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0309 - mean_absolute_error: 0.1351 - val_loss: 0.0307 - val_mean_absolute_error: 0.1350\n",
            "Epoch 20/45\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0304 - mean_absolute_error: 0.1342 - val_loss: 0.0303 - val_mean_absolute_error: 0.1337\n",
            "Epoch 21/45\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0306 - mean_absolute_error: 0.1344 - val_loss: 0.0310 - val_mean_absolute_error: 0.1351\n",
            "Epoch 22/45\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0298 - mean_absolute_error: 0.1328 - val_loss: 0.0304 - val_mean_absolute_error: 0.1367\n",
            "Epoch 23/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0302 - mean_absolute_error: 0.1333 - val_loss: 0.0301 - val_mean_absolute_error: 0.1328\n",
            "Epoch 24/45\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0297 - mean_absolute_error: 0.1325 - val_loss: 0.0291 - val_mean_absolute_error: 0.1295\n",
            "Epoch 25/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0292 - mean_absolute_error: 0.1313 - val_loss: 0.0278 - val_mean_absolute_error: 0.1275\n",
            "Epoch 26/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0290 - mean_absolute_error: 0.1311 - val_loss: 0.0291 - val_mean_absolute_error: 0.1301\n",
            "Epoch 27/45\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0287 - mean_absolute_error: 0.1299 - val_loss: 0.0302 - val_mean_absolute_error: 0.1324\n",
            "Epoch 28/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0284 - mean_absolute_error: 0.1293 - val_loss: 0.0283 - val_mean_absolute_error: 0.1281\n",
            "Epoch 29/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0284 - mean_absolute_error: 0.1291 - val_loss: 0.0283 - val_mean_absolute_error: 0.1285\n",
            "Epoch 30/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0285 - mean_absolute_error: 0.1297 - val_loss: 0.0320 - val_mean_absolute_error: 0.1367\n",
            "Epoch 31/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0281 - mean_absolute_error: 0.1287 - val_loss: 0.0276 - val_mean_absolute_error: 0.1269\n",
            "Epoch 32/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0278 - mean_absolute_error: 0.1283 - val_loss: 0.0289 - val_mean_absolute_error: 0.1293\n",
            "Epoch 33/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0279 - mean_absolute_error: 0.1281 - val_loss: 0.0279 - val_mean_absolute_error: 0.1277\n",
            "Epoch 34/45\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0276 - mean_absolute_error: 0.1275 - val_loss: 0.0275 - val_mean_absolute_error: 0.1262\n",
            "Epoch 35/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0274 - mean_absolute_error: 0.1273 - val_loss: 0.0278 - val_mean_absolute_error: 0.1273\n",
            "Epoch 36/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0273 - mean_absolute_error: 0.1265 - val_loss: 0.0273 - val_mean_absolute_error: 0.1258\n",
            "Epoch 37/45\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0268 - mean_absolute_error: 0.1252 - val_loss: 0.0289 - val_mean_absolute_error: 0.1293\n",
            "Epoch 38/45\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0270 - mean_absolute_error: 0.1266 - val_loss: 0.0276 - val_mean_absolute_error: 0.1262\n",
            "Epoch 39/45\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0269 - mean_absolute_error: 0.1265 - val_loss: 0.0269 - val_mean_absolute_error: 0.1250\n",
            "Epoch 40/45\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0267 - mean_absolute_error: 0.1253 - val_loss: 0.0284 - val_mean_absolute_error: 0.1296\n",
            "Epoch 41/45\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0270 - mean_absolute_error: 0.1259 - val_loss: 0.0278 - val_mean_absolute_error: 0.1273\n",
            "Epoch 42/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0264 - mean_absolute_error: 0.1245 - val_loss: 0.0276 - val_mean_absolute_error: 0.1268\n",
            "Epoch 43/45\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0261 - mean_absolute_error: 0.1236 - val_loss: 0.0296 - val_mean_absolute_error: 0.1345\n",
            "Epoch 44/45\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0262 - mean_absolute_error: 0.1242 - val_loss: 0.0268 - val_mean_absolute_error: 0.1251\n",
            "Epoch 45/45\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0260 - mean_absolute_error: 0.1240 - val_loss: 0.0274 - val_mean_absolute_error: 0.1276\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_93 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_94 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/46\n",
            "10381/10381 [==============================] - 22s 2ms/step - loss: 0.0745 - mean_absolute_error: 0.2084 - val_loss: 0.0431 - val_mean_absolute_error: 0.1604\n",
            "Epoch 2/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0475 - mean_absolute_error: 0.1703 - val_loss: 0.0414 - val_mean_absolute_error: 0.1584\n",
            "Epoch 3/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0440 - mean_absolute_error: 0.1627 - val_loss: 0.0372 - val_mean_absolute_error: 0.1466\n",
            "Epoch 4/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0407 - mean_absolute_error: 0.1556 - val_loss: 0.0367 - val_mean_absolute_error: 0.1452\n",
            "Epoch 5/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0389 - mean_absolute_error: 0.1517 - val_loss: 0.0350 - val_mean_absolute_error: 0.1423\n",
            "Epoch 6/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0373 - mean_absolute_error: 0.1484 - val_loss: 0.0363 - val_mean_absolute_error: 0.1497\n",
            "Epoch 7/46\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0372 - mean_absolute_error: 0.1474 - val_loss: 0.0345 - val_mean_absolute_error: 0.1448\n",
            "Epoch 8/46\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0359 - mean_absolute_error: 0.1453 - val_loss: 0.0329 - val_mean_absolute_error: 0.1385\n",
            "Epoch 9/46\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0352 - mean_absolute_error: 0.1438 - val_loss: 0.0337 - val_mean_absolute_error: 0.1403\n",
            "Epoch 10/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0346 - mean_absolute_error: 0.1432 - val_loss: 0.0322 - val_mean_absolute_error: 0.1376\n",
            "Epoch 11/46\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0338 - mean_absolute_error: 0.1407 - val_loss: 0.0337 - val_mean_absolute_error: 0.1438\n",
            "Epoch 12/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0333 - mean_absolute_error: 0.1399 - val_loss: 0.0332 - val_mean_absolute_error: 0.1413\n",
            "Epoch 13/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0324 - mean_absolute_error: 0.1387 - val_loss: 0.0315 - val_mean_absolute_error: 0.1355\n",
            "Epoch 14/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0329 - mean_absolute_error: 0.1397 - val_loss: 0.0314 - val_mean_absolute_error: 0.1351\n",
            "Epoch 15/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0316 - mean_absolute_error: 0.1368 - val_loss: 0.0328 - val_mean_absolute_error: 0.1412\n",
            "Epoch 16/46\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0319 - mean_absolute_error: 0.1373 - val_loss: 0.0313 - val_mean_absolute_error: 0.1370\n",
            "Epoch 17/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0311 - mean_absolute_error: 0.1354 - val_loss: 0.0309 - val_mean_absolute_error: 0.1353\n",
            "Epoch 18/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0306 - mean_absolute_error: 0.1343 - val_loss: 0.0338 - val_mean_absolute_error: 0.1406\n",
            "Epoch 19/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0307 - mean_absolute_error: 0.1348 - val_loss: 0.0292 - val_mean_absolute_error: 0.1301\n",
            "Epoch 20/46\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0305 - mean_absolute_error: 0.1346 - val_loss: 0.0293 - val_mean_absolute_error: 0.1313\n",
            "Epoch 21/46\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0299 - mean_absolute_error: 0.1326 - val_loss: 0.0328 - val_mean_absolute_error: 0.1373\n",
            "Epoch 22/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0297 - mean_absolute_error: 0.1322 - val_loss: 0.0303 - val_mean_absolute_error: 0.1326\n",
            "Epoch 23/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0297 - mean_absolute_error: 0.1318 - val_loss: 0.0288 - val_mean_absolute_error: 0.1293\n",
            "Epoch 24/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0293 - mean_absolute_error: 0.1315 - val_loss: 0.0292 - val_mean_absolute_error: 0.1303\n",
            "Epoch 25/46\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0290 - mean_absolute_error: 0.1311 - val_loss: 0.0295 - val_mean_absolute_error: 0.1329\n",
            "Epoch 26/46\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0291 - mean_absolute_error: 0.1307 - val_loss: 0.0300 - val_mean_absolute_error: 0.1323\n",
            "Epoch 27/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0287 - mean_absolute_error: 0.1303 - val_loss: 0.0287 - val_mean_absolute_error: 0.1303\n",
            "Epoch 28/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0283 - mean_absolute_error: 0.1288 - val_loss: 0.0291 - val_mean_absolute_error: 0.1309\n",
            "Epoch 29/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0285 - mean_absolute_error: 0.1296 - val_loss: 0.0288 - val_mean_absolute_error: 0.1306\n",
            "Epoch 30/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0284 - mean_absolute_error: 0.1290 - val_loss: 0.0287 - val_mean_absolute_error: 0.1307\n",
            "Epoch 31/46\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0281 - mean_absolute_error: 0.1288 - val_loss: 0.0287 - val_mean_absolute_error: 0.1294\n",
            "Epoch 32/46\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0280 - mean_absolute_error: 0.1283 - val_loss: 0.0287 - val_mean_absolute_error: 0.1296\n",
            "Epoch 33/46\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0274 - mean_absolute_error: 0.1274 - val_loss: 0.0274 - val_mean_absolute_error: 0.1259\n",
            "Epoch 34/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0276 - mean_absolute_error: 0.1273 - val_loss: 0.0296 - val_mean_absolute_error: 0.1320\n",
            "Epoch 35/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0275 - mean_absolute_error: 0.1274 - val_loss: 0.0287 - val_mean_absolute_error: 0.1292\n",
            "Epoch 36/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0273 - mean_absolute_error: 0.1264 - val_loss: 0.0274 - val_mean_absolute_error: 0.1261\n",
            "Epoch 37/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0275 - mean_absolute_error: 0.1273 - val_loss: 0.0275 - val_mean_absolute_error: 0.1265\n",
            "Epoch 38/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0266 - mean_absolute_error: 0.1251 - val_loss: 0.0284 - val_mean_absolute_error: 0.1287\n",
            "Epoch 39/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0269 - mean_absolute_error: 0.1263 - val_loss: 0.0287 - val_mean_absolute_error: 0.1303\n",
            "Epoch 40/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0266 - mean_absolute_error: 0.1257 - val_loss: 0.0278 - val_mean_absolute_error: 0.1278\n",
            "Epoch 41/46\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0264 - mean_absolute_error: 0.1245 - val_loss: 0.0267 - val_mean_absolute_error: 0.1247\n",
            "Epoch 42/46\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0265 - mean_absolute_error: 0.1253 - val_loss: 0.0274 - val_mean_absolute_error: 0.1266\n",
            "Epoch 43/46\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0265 - mean_absolute_error: 0.1251 - val_loss: 0.0291 - val_mean_absolute_error: 0.1306\n",
            "Epoch 44/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0263 - mean_absolute_error: 0.1244 - val_loss: 0.0278 - val_mean_absolute_error: 0.1274\n",
            "Epoch 45/46\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0261 - mean_absolute_error: 0.1244 - val_loss: 0.0267 - val_mean_absolute_error: 0.1245\n",
            "Epoch 46/46\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0259 - mean_absolute_error: 0.1236 - val_loss: 0.0274 - val_mean_absolute_error: 0.1260\n",
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_95 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_96 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/47\n",
            "10381/10381 [==============================] - 22s 2ms/step - loss: 0.0744 - mean_absolute_error: 0.2089 - val_loss: 0.0552 - val_mean_absolute_error: 0.1797\n",
            "Epoch 2/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0483 - mean_absolute_error: 0.1712 - val_loss: 0.0397 - val_mean_absolute_error: 0.1515\n",
            "Epoch 3/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0435 - mean_absolute_error: 0.1619 - val_loss: 0.0374 - val_mean_absolute_error: 0.1483\n",
            "Epoch 4/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0411 - mean_absolute_error: 0.1570 - val_loss: 0.0372 - val_mean_absolute_error: 0.1470\n",
            "Epoch 5/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0392 - mean_absolute_error: 0.1520 - val_loss: 0.0353 - val_mean_absolute_error: 0.1454\n",
            "Epoch 6/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0383 - mean_absolute_error: 0.1503 - val_loss: 0.0354 - val_mean_absolute_error: 0.1444\n",
            "Epoch 7/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0373 - mean_absolute_error: 0.1490 - val_loss: 0.0339 - val_mean_absolute_error: 0.1404\n",
            "Epoch 8/47\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0358 - mean_absolute_error: 0.1453 - val_loss: 0.0376 - val_mean_absolute_error: 0.1488\n",
            "Epoch 9/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0354 - mean_absolute_error: 0.1446 - val_loss: 0.0334 - val_mean_absolute_error: 0.1397\n",
            "Epoch 10/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0350 - mean_absolute_error: 0.1437 - val_loss: 0.0350 - val_mean_absolute_error: 0.1449\n",
            "Epoch 11/47\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0348 - mean_absolute_error: 0.1434 - val_loss: 0.0331 - val_mean_absolute_error: 0.1413\n",
            "Epoch 12/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0334 - mean_absolute_error: 0.1408 - val_loss: 0.0321 - val_mean_absolute_error: 0.1365\n",
            "Epoch 13/47\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0332 - mean_absolute_error: 0.1401 - val_loss: 0.0321 - val_mean_absolute_error: 0.1372\n",
            "Epoch 14/47\n",
            "10381/10381 [==============================] - 0s 37us/step - loss: 0.0327 - mean_absolute_error: 0.1390 - val_loss: 0.0329 - val_mean_absolute_error: 0.1416\n",
            "Epoch 15/47\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0318 - mean_absolute_error: 0.1366 - val_loss: 0.0315 - val_mean_absolute_error: 0.1356\n",
            "Epoch 16/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0319 - mean_absolute_error: 0.1372 - val_loss: 0.0306 - val_mean_absolute_error: 0.1336\n",
            "Epoch 17/47\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0314 - mean_absolute_error: 0.1362 - val_loss: 0.0316 - val_mean_absolute_error: 0.1360\n",
            "Epoch 18/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0310 - mean_absolute_error: 0.1355 - val_loss: 0.0311 - val_mean_absolute_error: 0.1358\n",
            "Epoch 19/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0307 - mean_absolute_error: 0.1350 - val_loss: 0.0295 - val_mean_absolute_error: 0.1312\n",
            "Epoch 20/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0309 - mean_absolute_error: 0.1347 - val_loss: 0.0293 - val_mean_absolute_error: 0.1309\n",
            "Epoch 21/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0301 - mean_absolute_error: 0.1327 - val_loss: 0.0292 - val_mean_absolute_error: 0.1305\n",
            "Epoch 22/47\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0298 - mean_absolute_error: 0.1326 - val_loss: 0.0308 - val_mean_absolute_error: 0.1330\n",
            "Epoch 23/47\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0296 - mean_absolute_error: 0.1327 - val_loss: 0.0294 - val_mean_absolute_error: 0.1318\n",
            "Epoch 24/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0295 - mean_absolute_error: 0.1320 - val_loss: 0.0306 - val_mean_absolute_error: 0.1337\n",
            "Epoch 25/47\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0293 - mean_absolute_error: 0.1313 - val_loss: 0.0286 - val_mean_absolute_error: 0.1282\n",
            "Epoch 26/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0290 - mean_absolute_error: 0.1305 - val_loss: 0.0301 - val_mean_absolute_error: 0.1320\n",
            "Epoch 27/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0289 - mean_absolute_error: 0.1302 - val_loss: 0.0312 - val_mean_absolute_error: 0.1362\n",
            "Epoch 28/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0284 - mean_absolute_error: 0.1297 - val_loss: 0.0290 - val_mean_absolute_error: 0.1298\n",
            "Epoch 29/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0284 - mean_absolute_error: 0.1299 - val_loss: 0.0283 - val_mean_absolute_error: 0.1277\n",
            "Epoch 30/47\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0286 - mean_absolute_error: 0.1298 - val_loss: 0.0289 - val_mean_absolute_error: 0.1304\n",
            "Epoch 31/47\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0281 - mean_absolute_error: 0.1286 - val_loss: 0.0291 - val_mean_absolute_error: 0.1304\n",
            "Epoch 32/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0277 - mean_absolute_error: 0.1277 - val_loss: 0.0278 - val_mean_absolute_error: 0.1277\n",
            "Epoch 33/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0279 - mean_absolute_error: 0.1279 - val_loss: 0.0294 - val_mean_absolute_error: 0.1303\n",
            "Epoch 34/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0276 - mean_absolute_error: 0.1277 - val_loss: 0.0287 - val_mean_absolute_error: 0.1293\n",
            "Epoch 35/47\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0277 - mean_absolute_error: 0.1280 - val_loss: 0.0280 - val_mean_absolute_error: 0.1280\n",
            "Epoch 36/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0271 - mean_absolute_error: 0.1266 - val_loss: 0.0290 - val_mean_absolute_error: 0.1316\n",
            "Epoch 37/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0270 - mean_absolute_error: 0.1261 - val_loss: 0.0286 - val_mean_absolute_error: 0.1316\n",
            "Epoch 38/47\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0271 - mean_absolute_error: 0.1264 - val_loss: 0.0279 - val_mean_absolute_error: 0.1274\n",
            "Epoch 39/47\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0266 - mean_absolute_error: 0.1255 - val_loss: 0.0277 - val_mean_absolute_error: 0.1272\n",
            "Epoch 40/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0265 - mean_absolute_error: 0.1252 - val_loss: 0.0277 - val_mean_absolute_error: 0.1279\n",
            "Epoch 41/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0267 - mean_absolute_error: 0.1255 - val_loss: 0.0277 - val_mean_absolute_error: 0.1270\n",
            "Epoch 42/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0267 - mean_absolute_error: 0.1253 - val_loss: 0.0282 - val_mean_absolute_error: 0.1284\n",
            "Epoch 43/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0265 - mean_absolute_error: 0.1250 - val_loss: 0.0276 - val_mean_absolute_error: 0.1281\n",
            "Epoch 44/47\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0261 - mean_absolute_error: 0.1239 - val_loss: 0.0270 - val_mean_absolute_error: 0.1260\n",
            "Epoch 45/47\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0258 - mean_absolute_error: 0.1227 - val_loss: 0.0292 - val_mean_absolute_error: 0.1297\n",
            "Epoch 46/47\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0259 - mean_absolute_error: 0.1229 - val_loss: 0.0270 - val_mean_absolute_error: 0.1258\n",
            "Epoch 47/47\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0259 - mean_absolute_error: 0.1236 - val_loss: 0.0270 - val_mean_absolute_error: 0.1253\n",
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_97 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_98 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/48\n",
            "10381/10381 [==============================] - 23s 2ms/step - loss: 0.0741 - mean_absolute_error: 0.2089 - val_loss: 0.0435 - val_mean_absolute_error: 0.1614\n",
            "Epoch 2/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0464 - mean_absolute_error: 0.1675 - val_loss: 0.0397 - val_mean_absolute_error: 0.1545\n",
            "Epoch 3/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0439 - mean_absolute_error: 0.1618 - val_loss: 0.0413 - val_mean_absolute_error: 0.1584\n",
            "Epoch 4/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0406 - mean_absolute_error: 0.1550 - val_loss: 0.0363 - val_mean_absolute_error: 0.1469\n",
            "Epoch 5/48\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0389 - mean_absolute_error: 0.1517 - val_loss: 0.0347 - val_mean_absolute_error: 0.1419\n",
            "Epoch 6/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0372 - mean_absolute_error: 0.1477 - val_loss: 0.0348 - val_mean_absolute_error: 0.1425\n",
            "Epoch 7/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0367 - mean_absolute_error: 0.1469 - val_loss: 0.0347 - val_mean_absolute_error: 0.1424\n",
            "Epoch 8/48\n",
            "10381/10381 [==============================] - 0s 48us/step - loss: 0.0359 - mean_absolute_error: 0.1454 - val_loss: 0.0326 - val_mean_absolute_error: 0.1379\n",
            "Epoch 9/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0353 - mean_absolute_error: 0.1441 - val_loss: 0.0321 - val_mean_absolute_error: 0.1370\n",
            "Epoch 10/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0349 - mean_absolute_error: 0.1436 - val_loss: 0.0334 - val_mean_absolute_error: 0.1415\n",
            "Epoch 11/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0339 - mean_absolute_error: 0.1413 - val_loss: 0.0322 - val_mean_absolute_error: 0.1379\n",
            "Epoch 12/48\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0335 - mean_absolute_error: 0.1402 - val_loss: 0.0321 - val_mean_absolute_error: 0.1381\n",
            "Epoch 13/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0329 - mean_absolute_error: 0.1390 - val_loss: 0.0319 - val_mean_absolute_error: 0.1358\n",
            "Epoch 14/48\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0323 - mean_absolute_error: 0.1383 - val_loss: 0.0313 - val_mean_absolute_error: 0.1357\n",
            "Epoch 15/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0322 - mean_absolute_error: 0.1380 - val_loss: 0.0319 - val_mean_absolute_error: 0.1374\n",
            "Epoch 16/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0320 - mean_absolute_error: 0.1377 - val_loss: 0.0300 - val_mean_absolute_error: 0.1324\n",
            "Epoch 17/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0312 - mean_absolute_error: 0.1355 - val_loss: 0.0316 - val_mean_absolute_error: 0.1362\n",
            "Epoch 18/48\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0309 - mean_absolute_error: 0.1356 - val_loss: 0.0310 - val_mean_absolute_error: 0.1336\n",
            "Epoch 19/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0305 - mean_absolute_error: 0.1341 - val_loss: 0.0295 - val_mean_absolute_error: 0.1313\n",
            "Epoch 20/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0302 - mean_absolute_error: 0.1331 - val_loss: 0.0310 - val_mean_absolute_error: 0.1372\n",
            "Epoch 21/48\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0300 - mean_absolute_error: 0.1326 - val_loss: 0.0289 - val_mean_absolute_error: 0.1306\n",
            "Epoch 22/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0294 - mean_absolute_error: 0.1317 - val_loss: 0.0301 - val_mean_absolute_error: 0.1328\n",
            "Epoch 23/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0298 - mean_absolute_error: 0.1321 - val_loss: 0.0300 - val_mean_absolute_error: 0.1327\n",
            "Epoch 24/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0291 - mean_absolute_error: 0.1313 - val_loss: 0.0294 - val_mean_absolute_error: 0.1309\n",
            "Epoch 25/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0292 - mean_absolute_error: 0.1314 - val_loss: 0.0306 - val_mean_absolute_error: 0.1332\n",
            "Epoch 26/48\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0289 - mean_absolute_error: 0.1306 - val_loss: 0.0282 - val_mean_absolute_error: 0.1289\n",
            "Epoch 27/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0286 - mean_absolute_error: 0.1296 - val_loss: 0.0287 - val_mean_absolute_error: 0.1299\n",
            "Epoch 28/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0283 - mean_absolute_error: 0.1289 - val_loss: 0.0285 - val_mean_absolute_error: 0.1281\n",
            "Epoch 29/48\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0282 - mean_absolute_error: 0.1295 - val_loss: 0.0280 - val_mean_absolute_error: 0.1288\n",
            "Epoch 30/48\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0282 - mean_absolute_error: 0.1288 - val_loss: 0.0275 - val_mean_absolute_error: 0.1264\n",
            "Epoch 31/48\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0281 - mean_absolute_error: 0.1283 - val_loss: 0.0277 - val_mean_absolute_error: 0.1273\n",
            "Epoch 32/48\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0280 - mean_absolute_error: 0.1281 - val_loss: 0.0306 - val_mean_absolute_error: 0.1325\n",
            "Epoch 33/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0277 - mean_absolute_error: 0.1273 - val_loss: 0.0288 - val_mean_absolute_error: 0.1298\n",
            "Epoch 34/48\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0275 - mean_absolute_error: 0.1273 - val_loss: 0.0286 - val_mean_absolute_error: 0.1287\n",
            "Epoch 35/48\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0275 - mean_absolute_error: 0.1276 - val_loss: 0.0289 - val_mean_absolute_error: 0.1300\n",
            "Epoch 36/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0267 - mean_absolute_error: 0.1255 - val_loss: 0.0322 - val_mean_absolute_error: 0.1408\n",
            "Epoch 37/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0272 - mean_absolute_error: 0.1263 - val_loss: 0.0300 - val_mean_absolute_error: 0.1327\n",
            "Epoch 38/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0267 - mean_absolute_error: 0.1252 - val_loss: 0.0278 - val_mean_absolute_error: 0.1276\n",
            "Epoch 39/48\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0269 - mean_absolute_error: 0.1257 - val_loss: 0.0270 - val_mean_absolute_error: 0.1253\n",
            "Epoch 40/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0264 - mean_absolute_error: 0.1247 - val_loss: 0.0285 - val_mean_absolute_error: 0.1296\n",
            "Epoch 41/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0266 - mean_absolute_error: 0.1252 - val_loss: 0.0279 - val_mean_absolute_error: 0.1268\n",
            "Epoch 42/48\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0263 - mean_absolute_error: 0.1243 - val_loss: 0.0271 - val_mean_absolute_error: 0.1255\n",
            "Epoch 43/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0263 - mean_absolute_error: 0.1245 - val_loss: 0.0277 - val_mean_absolute_error: 0.1262\n",
            "Epoch 44/48\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0262 - mean_absolute_error: 0.1245 - val_loss: 0.0280 - val_mean_absolute_error: 0.1274\n",
            "Epoch 45/48\n",
            "10381/10381 [==============================] - 0s 46us/step - loss: 0.0261 - mean_absolute_error: 0.1241 - val_loss: 0.0278 - val_mean_absolute_error: 0.1271\n",
            "Epoch 46/48\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0259 - mean_absolute_error: 0.1233 - val_loss: 0.0269 - val_mean_absolute_error: 0.1249\n",
            "Epoch 47/48\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0262 - mean_absolute_error: 0.1239 - val_loss: 0.0268 - val_mean_absolute_error: 0.1250\n",
            "Epoch 48/48\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0252 - mean_absolute_error: 0.1218 - val_loss: 0.0288 - val_mean_absolute_error: 0.1279\n",
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_99 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_100 (LSTM)              (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10381 samples, validate on 2596 samples\n",
            "Epoch 1/49\n",
            "10381/10381 [==============================] - 23s 2ms/step - loss: 0.0735 - mean_absolute_error: 0.2077 - val_loss: 0.0600 - val_mean_absolute_error: 0.1879\n",
            "Epoch 2/49\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0477 - mean_absolute_error: 0.1701 - val_loss: 0.0389 - val_mean_absolute_error: 0.1525\n",
            "Epoch 3/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0437 - mean_absolute_error: 0.1621 - val_loss: 0.0401 - val_mean_absolute_error: 0.1517\n",
            "Epoch 4/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0405 - mean_absolute_error: 0.1549 - val_loss: 0.0390 - val_mean_absolute_error: 0.1530\n",
            "Epoch 5/49\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0388 - mean_absolute_error: 0.1514 - val_loss: 0.0355 - val_mean_absolute_error: 0.1463\n",
            "Epoch 6/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0378 - mean_absolute_error: 0.1494 - val_loss: 0.0346 - val_mean_absolute_error: 0.1445\n",
            "Epoch 7/49\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0365 - mean_absolute_error: 0.1470 - val_loss: 0.0345 - val_mean_absolute_error: 0.1444\n",
            "Epoch 8/49\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0358 - mean_absolute_error: 0.1446 - val_loss: 0.0344 - val_mean_absolute_error: 0.1442\n",
            "Epoch 9/49\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0352 - mean_absolute_error: 0.1441 - val_loss: 0.0338 - val_mean_absolute_error: 0.1419\n",
            "Epoch 10/49\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0337 - mean_absolute_error: 0.1408 - val_loss: 0.0407 - val_mean_absolute_error: 0.1555\n",
            "Epoch 11/49\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0339 - mean_absolute_error: 0.1405 - val_loss: 0.0320 - val_mean_absolute_error: 0.1370\n",
            "Epoch 12/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0339 - mean_absolute_error: 0.1413 - val_loss: 0.0332 - val_mean_absolute_error: 0.1395\n",
            "Epoch 13/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0327 - mean_absolute_error: 0.1386 - val_loss: 0.0337 - val_mean_absolute_error: 0.1410\n",
            "Epoch 14/49\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0323 - mean_absolute_error: 0.1381 - val_loss: 0.0311 - val_mean_absolute_error: 0.1355\n",
            "Epoch 15/49\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0319 - mean_absolute_error: 0.1375 - val_loss: 0.0310 - val_mean_absolute_error: 0.1352\n",
            "Epoch 16/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0314 - mean_absolute_error: 0.1361 - val_loss: 0.0307 - val_mean_absolute_error: 0.1340\n",
            "Epoch 17/49\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0311 - mean_absolute_error: 0.1355 - val_loss: 0.0306 - val_mean_absolute_error: 0.1352\n",
            "Epoch 18/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0312 - mean_absolute_error: 0.1353 - val_loss: 0.0335 - val_mean_absolute_error: 0.1402\n",
            "Epoch 19/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0305 - mean_absolute_error: 0.1340 - val_loss: 0.0291 - val_mean_absolute_error: 0.1303\n",
            "Epoch 20/49\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0304 - mean_absolute_error: 0.1337 - val_loss: 0.0287 - val_mean_absolute_error: 0.1292\n",
            "Epoch 21/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0303 - mean_absolute_error: 0.1332 - val_loss: 0.0318 - val_mean_absolute_error: 0.1366\n",
            "Epoch 22/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0297 - mean_absolute_error: 0.1325 - val_loss: 0.0310 - val_mean_absolute_error: 0.1347\n",
            "Epoch 23/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0294 - mean_absolute_error: 0.1317 - val_loss: 0.0301 - val_mean_absolute_error: 0.1326\n",
            "Epoch 24/49\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0291 - mean_absolute_error: 0.1309 - val_loss: 0.0282 - val_mean_absolute_error: 0.1282\n",
            "Epoch 25/49\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0287 - mean_absolute_error: 0.1301 - val_loss: 0.0299 - val_mean_absolute_error: 0.1324\n",
            "Epoch 26/49\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0289 - mean_absolute_error: 0.1305 - val_loss: 0.0317 - val_mean_absolute_error: 0.1358\n",
            "Epoch 27/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0291 - mean_absolute_error: 0.1308 - val_loss: 0.0286 - val_mean_absolute_error: 0.1291\n",
            "Epoch 28/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0288 - mean_absolute_error: 0.1308 - val_loss: 0.0280 - val_mean_absolute_error: 0.1279\n",
            "Epoch 29/49\n",
            "10381/10381 [==============================] - 0s 43us/step - loss: 0.0286 - mean_absolute_error: 0.1295 - val_loss: 0.0282 - val_mean_absolute_error: 0.1283\n",
            "Epoch 30/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0282 - mean_absolute_error: 0.1286 - val_loss: 0.0308 - val_mean_absolute_error: 0.1332\n",
            "Epoch 31/49\n",
            "10381/10381 [==============================] - 0s 38us/step - loss: 0.0283 - mean_absolute_error: 0.1294 - val_loss: 0.0288 - val_mean_absolute_error: 0.1283\n",
            "Epoch 32/49\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0280 - mean_absolute_error: 0.1280 - val_loss: 0.0274 - val_mean_absolute_error: 0.1262\n",
            "Epoch 33/49\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0276 - mean_absolute_error: 0.1277 - val_loss: 0.0307 - val_mean_absolute_error: 0.1342\n",
            "Epoch 34/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0274 - mean_absolute_error: 0.1269 - val_loss: 0.0284 - val_mean_absolute_error: 0.1288\n",
            "Epoch 35/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0274 - mean_absolute_error: 0.1271 - val_loss: 0.0278 - val_mean_absolute_error: 0.1272\n",
            "Epoch 36/49\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0275 - mean_absolute_error: 0.1274 - val_loss: 0.0276 - val_mean_absolute_error: 0.1263\n",
            "Epoch 37/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0267 - mean_absolute_error: 0.1252 - val_loss: 0.0280 - val_mean_absolute_error: 0.1272\n",
            "Epoch 38/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0265 - mean_absolute_error: 0.1248 - val_loss: 0.0272 - val_mean_absolute_error: 0.1261\n",
            "Epoch 39/49\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0269 - mean_absolute_error: 0.1253 - val_loss: 0.0272 - val_mean_absolute_error: 0.1258\n",
            "Epoch 40/49\n",
            "10381/10381 [==============================] - 0s 42us/step - loss: 0.0266 - mean_absolute_error: 0.1248 - val_loss: 0.0280 - val_mean_absolute_error: 0.1286\n",
            "Epoch 41/49\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0263 - mean_absolute_error: 0.1251 - val_loss: 0.0272 - val_mean_absolute_error: 0.1256\n",
            "Epoch 42/49\n",
            "10381/10381 [==============================] - 0s 45us/step - loss: 0.0262 - mean_absolute_error: 0.1242 - val_loss: 0.0291 - val_mean_absolute_error: 0.1296\n",
            "Epoch 43/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0260 - mean_absolute_error: 0.1232 - val_loss: 0.0288 - val_mean_absolute_error: 0.1290\n",
            "Epoch 44/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0260 - mean_absolute_error: 0.1239 - val_loss: 0.0271 - val_mean_absolute_error: 0.1257\n",
            "Epoch 45/49\n",
            "10381/10381 [==============================] - 0s 41us/step - loss: 0.0259 - mean_absolute_error: 0.1232 - val_loss: 0.0272 - val_mean_absolute_error: 0.1254\n",
            "Epoch 46/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0256 - mean_absolute_error: 0.1226 - val_loss: 0.0273 - val_mean_absolute_error: 0.1266\n",
            "Epoch 47/49\n",
            "10381/10381 [==============================] - 0s 40us/step - loss: 0.0255 - mean_absolute_error: 0.1227 - val_loss: 0.0280 - val_mean_absolute_error: 0.1280\n",
            "Epoch 48/49\n",
            "10381/10381 [==============================] - 0s 39us/step - loss: 0.0257 - mean_absolute_error: 0.1225 - val_loss: 0.0280 - val_mean_absolute_error: 0.1289\n",
            "Epoch 49/49\n",
            "10381/10381 [==============================] - 0s 44us/step - loss: 0.0254 - mean_absolute_error: 0.1220 - val_loss: 0.0274 - val_mean_absolute_error: 0.1252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vGafBxVP98l",
        "colab_type": "code",
        "outputId": "c62ed40b-63c6-4ac2-8a4e-bb3fb086bbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "KAPPA_COHEN_SCORES\n",
        "# Plotting\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# plt.plot(history.history['mean_absolute_error'])\n",
        "# plt.plot(history.history['val_mean_absolute_error'])\n",
        "plt.plot(KAPPA_COHEN_SCORES)\n",
        "plt.title('Model - Kappa Cohen Scores')\n",
        "plt.ylabel('Mean Kappa Cohen Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Score'], loc='upper left')\n",
        "plt.savefig(\"myfig.png\", dpi=300)\n",
        "plt.show()\n",
        "# history.history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSwTGGBPkLBEYY0yQs0RgjDFB7v8BBCRqFvqUMocAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzccw9ifw4Jh",
        "colab_type": "code",
        "outputId": "fb8041af-af33-408e-e44f-6e467a408337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAINING\n",
        "lstm_model = get_model()\n",
        "history = lstm_model.fit(X_train, y_train, batch_size=64, epochs=100, validation_data=[X_test,y_test], verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1426 samples, validate on 357 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1426/1426 [==============================] - 6s 4ms/step - loss: 0.0692 - mean_absolute_error: 0.1831 - val_loss: 0.0218 - val_mean_absolute_error: 0.1160\n",
            "Epoch 2/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0207 - mean_absolute_error: 0.1125 - val_loss: 0.0224 - val_mean_absolute_error: 0.1132\n",
            "Epoch 3/100\n",
            "1426/1426 [==============================] - 0s 149us/step - loss: 0.0218 - mean_absolute_error: 0.1158 - val_loss: 0.0154 - val_mean_absolute_error: 0.0916\n",
            "Epoch 4/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0198 - mean_absolute_error: 0.1117 - val_loss: 0.0190 - val_mean_absolute_error: 0.1089\n",
            "Epoch 5/100\n",
            "1426/1426 [==============================] - 0s 154us/step - loss: 0.0188 - mean_absolute_error: 0.1073 - val_loss: 0.0152 - val_mean_absolute_error: 0.0922\n",
            "Epoch 6/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0177 - mean_absolute_error: 0.1062 - val_loss: 0.0149 - val_mean_absolute_error: 0.0903\n",
            "Epoch 7/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0176 - mean_absolute_error: 0.1051 - val_loss: 0.0180 - val_mean_absolute_error: 0.1001\n",
            "Epoch 8/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0170 - mean_absolute_error: 0.1021 - val_loss: 0.0190 - val_mean_absolute_error: 0.1037\n",
            "Epoch 9/100\n",
            "1426/1426 [==============================] - 0s 155us/step - loss: 0.0170 - mean_absolute_error: 0.1012 - val_loss: 0.0211 - val_mean_absolute_error: 0.1110\n",
            "Epoch 10/100\n",
            "1426/1426 [==============================] - 0s 149us/step - loss: 0.0174 - mean_absolute_error: 0.1027 - val_loss: 0.0143 - val_mean_absolute_error: 0.0891\n",
            "Epoch 11/100\n",
            "1426/1426 [==============================] - 0s 159us/step - loss: 0.0158 - mean_absolute_error: 0.0979 - val_loss: 0.0146 - val_mean_absolute_error: 0.0898\n",
            "Epoch 12/100\n",
            "1426/1426 [==============================] - 0s 147us/step - loss: 0.0161 - mean_absolute_error: 0.0974 - val_loss: 0.0164 - val_mean_absolute_error: 0.0962\n",
            "Epoch 13/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0157 - mean_absolute_error: 0.0977 - val_loss: 0.0144 - val_mean_absolute_error: 0.0903\n",
            "Epoch 14/100\n",
            "1426/1426 [==============================] - 0s 161us/step - loss: 0.0152 - mean_absolute_error: 0.0964 - val_loss: 0.0144 - val_mean_absolute_error: 0.0902\n",
            "Epoch 15/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0151 - mean_absolute_error: 0.0957 - val_loss: 0.0146 - val_mean_absolute_error: 0.0922\n",
            "Epoch 16/100\n",
            "1426/1426 [==============================] - 0s 154us/step - loss: 0.0149 - mean_absolute_error: 0.0953 - val_loss: 0.0147 - val_mean_absolute_error: 0.0916\n",
            "Epoch 17/100\n",
            "1426/1426 [==============================] - 0s 154us/step - loss: 0.0143 - mean_absolute_error: 0.0931 - val_loss: 0.0202 - val_mean_absolute_error: 0.1142\n",
            "Epoch 18/100\n",
            "1426/1426 [==============================] - 0s 166us/step - loss: 0.0153 - mean_absolute_error: 0.0969 - val_loss: 0.0139 - val_mean_absolute_error: 0.0894\n",
            "Epoch 19/100\n",
            "1426/1426 [==============================] - 0s 153us/step - loss: 0.0145 - mean_absolute_error: 0.0933 - val_loss: 0.0135 - val_mean_absolute_error: 0.0882\n",
            "Epoch 20/100\n",
            "1426/1426 [==============================] - 0s 149us/step - loss: 0.0149 - mean_absolute_error: 0.0941 - val_loss: 0.0152 - val_mean_absolute_error: 0.0946\n",
            "Epoch 21/100\n",
            "1426/1426 [==============================] - 0s 166us/step - loss: 0.0140 - mean_absolute_error: 0.0918 - val_loss: 0.0150 - val_mean_absolute_error: 0.0940\n",
            "Epoch 22/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0142 - mean_absolute_error: 0.0927 - val_loss: 0.0185 - val_mean_absolute_error: 0.1084\n",
            "Epoch 23/100\n",
            "1426/1426 [==============================] - 0s 159us/step - loss: 0.0145 - mean_absolute_error: 0.0942 - val_loss: 0.0162 - val_mean_absolute_error: 0.0990\n",
            "Epoch 24/100\n",
            "1426/1426 [==============================] - 0s 174us/step - loss: 0.0145 - mean_absolute_error: 0.0936 - val_loss: 0.0165 - val_mean_absolute_error: 0.0981\n",
            "Epoch 25/100\n",
            "1426/1426 [==============================] - 0s 168us/step - loss: 0.0148 - mean_absolute_error: 0.0940 - val_loss: 0.0142 - val_mean_absolute_error: 0.0905\n",
            "Epoch 26/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0146 - mean_absolute_error: 0.0942 - val_loss: 0.0132 - val_mean_absolute_error: 0.0862\n",
            "Epoch 27/100\n",
            "1426/1426 [==============================] - 0s 159us/step - loss: 0.0138 - mean_absolute_error: 0.0922 - val_loss: 0.0143 - val_mean_absolute_error: 0.0908\n",
            "Epoch 28/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0134 - mean_absolute_error: 0.0911 - val_loss: 0.0131 - val_mean_absolute_error: 0.0858\n",
            "Epoch 29/100\n",
            "1426/1426 [==============================] - 0s 145us/step - loss: 0.0139 - mean_absolute_error: 0.0913 - val_loss: 0.0133 - val_mean_absolute_error: 0.0862\n",
            "Epoch 30/100\n",
            "1426/1426 [==============================] - 0s 154us/step - loss: 0.0145 - mean_absolute_error: 0.0934 - val_loss: 0.0156 - val_mean_absolute_error: 0.0946\n",
            "Epoch 31/100\n",
            "1426/1426 [==============================] - 0s 148us/step - loss: 0.0138 - mean_absolute_error: 0.0917 - val_loss: 0.0158 - val_mean_absolute_error: 0.0969\n",
            "Epoch 32/100\n",
            "1426/1426 [==============================] - 0s 155us/step - loss: 0.0130 - mean_absolute_error: 0.0887 - val_loss: 0.0150 - val_mean_absolute_error: 0.0945\n",
            "Epoch 33/100\n",
            "1426/1426 [==============================] - 0s 159us/step - loss: 0.0132 - mean_absolute_error: 0.0901 - val_loss: 0.0136 - val_mean_absolute_error: 0.0878\n",
            "Epoch 34/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0139 - mean_absolute_error: 0.0918 - val_loss: 0.0131 - val_mean_absolute_error: 0.0857\n",
            "Epoch 35/100\n",
            "1426/1426 [==============================] - 0s 163us/step - loss: 0.0131 - mean_absolute_error: 0.0889 - val_loss: 0.0132 - val_mean_absolute_error: 0.0861\n",
            "Epoch 36/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0140 - mean_absolute_error: 0.0925 - val_loss: 0.0139 - val_mean_absolute_error: 0.0892\n",
            "Epoch 37/100\n",
            "1426/1426 [==============================] - 0s 163us/step - loss: 0.0138 - mean_absolute_error: 0.0924 - val_loss: 0.0134 - val_mean_absolute_error: 0.0863\n",
            "Epoch 38/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0131 - mean_absolute_error: 0.0901 - val_loss: 0.0158 - val_mean_absolute_error: 0.0972\n",
            "Epoch 39/100\n",
            "1426/1426 [==============================] - 0s 173us/step - loss: 0.0131 - mean_absolute_error: 0.0892 - val_loss: 0.0143 - val_mean_absolute_error: 0.0911\n",
            "Epoch 40/100\n",
            "1426/1426 [==============================] - 0s 164us/step - loss: 0.0131 - mean_absolute_error: 0.0897 - val_loss: 0.0248 - val_mean_absolute_error: 0.1249\n",
            "Epoch 41/100\n",
            "1426/1426 [==============================] - 0s 170us/step - loss: 0.0127 - mean_absolute_error: 0.0888 - val_loss: 0.0131 - val_mean_absolute_error: 0.0862\n",
            "Epoch 42/100\n",
            "1426/1426 [==============================] - 0s 164us/step - loss: 0.0131 - mean_absolute_error: 0.0902 - val_loss: 0.0149 - val_mean_absolute_error: 0.0915\n",
            "Epoch 43/100\n",
            "1426/1426 [==============================] - 0s 161us/step - loss: 0.0135 - mean_absolute_error: 0.0916 - val_loss: 0.0157 - val_mean_absolute_error: 0.0955\n",
            "Epoch 44/100\n",
            "1426/1426 [==============================] - 0s 154us/step - loss: 0.0133 - mean_absolute_error: 0.0907 - val_loss: 0.0132 - val_mean_absolute_error: 0.0868\n",
            "Epoch 45/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0127 - mean_absolute_error: 0.0879 - val_loss: 0.0212 - val_mean_absolute_error: 0.1139\n",
            "Epoch 46/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0127 - mean_absolute_error: 0.0895 - val_loss: 0.0177 - val_mean_absolute_error: 0.1047\n",
            "Epoch 47/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0129 - mean_absolute_error: 0.0882 - val_loss: 0.0132 - val_mean_absolute_error: 0.0862\n",
            "Epoch 48/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0128 - mean_absolute_error: 0.0881 - val_loss: 0.0131 - val_mean_absolute_error: 0.0861\n",
            "Epoch 49/100\n",
            "1426/1426 [==============================] - 0s 149us/step - loss: 0.0126 - mean_absolute_error: 0.0880 - val_loss: 0.0131 - val_mean_absolute_error: 0.0860\n",
            "Epoch 50/100\n",
            "1426/1426 [==============================] - 0s 160us/step - loss: 0.0126 - mean_absolute_error: 0.0872 - val_loss: 0.0131 - val_mean_absolute_error: 0.0855\n",
            "Epoch 51/100\n",
            "1426/1426 [==============================] - 0s 148us/step - loss: 0.0127 - mean_absolute_error: 0.0878 - val_loss: 0.0137 - val_mean_absolute_error: 0.0879\n",
            "Epoch 52/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0121 - mean_absolute_error: 0.0858 - val_loss: 0.0202 - val_mean_absolute_error: 0.1098\n",
            "Epoch 53/100\n",
            "1426/1426 [==============================] - 0s 157us/step - loss: 0.0125 - mean_absolute_error: 0.0879 - val_loss: 0.0130 - val_mean_absolute_error: 0.0857\n",
            "Epoch 54/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0123 - mean_absolute_error: 0.0877 - val_loss: 0.0155 - val_mean_absolute_error: 0.0957\n",
            "Epoch 55/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0123 - mean_absolute_error: 0.0873 - val_loss: 0.0147 - val_mean_absolute_error: 0.0918\n",
            "Epoch 56/100\n",
            "1426/1426 [==============================] - 0s 148us/step - loss: 0.0124 - mean_absolute_error: 0.0877 - val_loss: 0.0152 - val_mean_absolute_error: 0.0944\n",
            "Epoch 57/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0122 - mean_absolute_error: 0.0863 - val_loss: 0.0131 - val_mean_absolute_error: 0.0860\n",
            "Epoch 58/100\n",
            "1426/1426 [==============================] - 0s 153us/step - loss: 0.0126 - mean_absolute_error: 0.0879 - val_loss: 0.0133 - val_mean_absolute_error: 0.0868\n",
            "Epoch 59/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0122 - mean_absolute_error: 0.0863 - val_loss: 0.0128 - val_mean_absolute_error: 0.0854\n",
            "Epoch 60/100\n",
            "1426/1426 [==============================] - 0s 157us/step - loss: 0.0125 - mean_absolute_error: 0.0876 - val_loss: 0.0141 - val_mean_absolute_error: 0.0899\n",
            "Epoch 61/100\n",
            "1426/1426 [==============================] - 0s 157us/step - loss: 0.0121 - mean_absolute_error: 0.0861 - val_loss: 0.0132 - val_mean_absolute_error: 0.0865\n",
            "Epoch 62/100\n",
            "1426/1426 [==============================] - 0s 157us/step - loss: 0.0122 - mean_absolute_error: 0.0863 - val_loss: 0.0130 - val_mean_absolute_error: 0.0858\n",
            "Epoch 63/100\n",
            "1426/1426 [==============================] - 0s 170us/step - loss: 0.0121 - mean_absolute_error: 0.0864 - val_loss: 0.0129 - val_mean_absolute_error: 0.0856\n",
            "Epoch 64/100\n",
            "1426/1426 [==============================] - 0s 169us/step - loss: 0.0119 - mean_absolute_error: 0.0859 - val_loss: 0.0130 - val_mean_absolute_error: 0.0854\n",
            "Epoch 65/100\n",
            "1426/1426 [==============================] - 0s 148us/step - loss: 0.0118 - mean_absolute_error: 0.0837 - val_loss: 0.0171 - val_mean_absolute_error: 0.1024\n",
            "Epoch 66/100\n",
            "1426/1426 [==============================] - 0s 161us/step - loss: 0.0123 - mean_absolute_error: 0.0879 - val_loss: 0.0130 - val_mean_absolute_error: 0.0857\n",
            "Epoch 67/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0117 - mean_absolute_error: 0.0845 - val_loss: 0.0153 - val_mean_absolute_error: 0.0964\n",
            "Epoch 68/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0120 - mean_absolute_error: 0.0863 - val_loss: 0.0129 - val_mean_absolute_error: 0.0862\n",
            "Epoch 69/100\n",
            "1426/1426 [==============================] - 0s 157us/step - loss: 0.0120 - mean_absolute_error: 0.0860 - val_loss: 0.0132 - val_mean_absolute_error: 0.0863\n",
            "Epoch 70/100\n",
            "1426/1426 [==============================] - 0s 147us/step - loss: 0.0115 - mean_absolute_error: 0.0842 - val_loss: 0.0130 - val_mean_absolute_error: 0.0863\n",
            "Epoch 71/100\n",
            "1426/1426 [==============================] - 0s 162us/step - loss: 0.0124 - mean_absolute_error: 0.0875 - val_loss: 0.0159 - val_mean_absolute_error: 0.0958\n",
            "Epoch 72/100\n",
            "1426/1426 [==============================] - 0s 146us/step - loss: 0.0116 - mean_absolute_error: 0.0844 - val_loss: 0.0145 - val_mean_absolute_error: 0.0914\n",
            "Epoch 73/100\n",
            "1426/1426 [==============================] - 0s 164us/step - loss: 0.0117 - mean_absolute_error: 0.0860 - val_loss: 0.0131 - val_mean_absolute_error: 0.0862\n",
            "Epoch 74/100\n",
            "1426/1426 [==============================] - 0s 170us/step - loss: 0.0114 - mean_absolute_error: 0.0832 - val_loss: 0.0133 - val_mean_absolute_error: 0.0872\n",
            "Epoch 75/100\n",
            "1426/1426 [==============================] - 0s 161us/step - loss: 0.0121 - mean_absolute_error: 0.0871 - val_loss: 0.0128 - val_mean_absolute_error: 0.0859\n",
            "Epoch 76/100\n",
            "1426/1426 [==============================] - 0s 149us/step - loss: 0.0118 - mean_absolute_error: 0.0855 - val_loss: 0.0133 - val_mean_absolute_error: 0.0867\n",
            "Epoch 77/100\n",
            "1426/1426 [==============================] - 0s 148us/step - loss: 0.0112 - mean_absolute_error: 0.0833 - val_loss: 0.0139 - val_mean_absolute_error: 0.0899\n",
            "Epoch 78/100\n",
            "1426/1426 [==============================] - 0s 154us/step - loss: 0.0118 - mean_absolute_error: 0.0854 - val_loss: 0.0137 - val_mean_absolute_error: 0.0888\n",
            "Epoch 79/100\n",
            "1426/1426 [==============================] - 0s 167us/step - loss: 0.0118 - mean_absolute_error: 0.0859 - val_loss: 0.0169 - val_mean_absolute_error: 0.0993\n",
            "Epoch 80/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0119 - mean_absolute_error: 0.0865 - val_loss: 0.0162 - val_mean_absolute_error: 0.0981\n",
            "Epoch 81/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0114 - mean_absolute_error: 0.0839 - val_loss: 0.0132 - val_mean_absolute_error: 0.0861\n",
            "Epoch 82/100\n",
            "1426/1426 [==============================] - 0s 160us/step - loss: 0.0119 - mean_absolute_error: 0.0859 - val_loss: 0.0144 - val_mean_absolute_error: 0.0908\n",
            "Epoch 83/100\n",
            "1426/1426 [==============================] - 0s 148us/step - loss: 0.0116 - mean_absolute_error: 0.0845 - val_loss: 0.0132 - val_mean_absolute_error: 0.0866\n",
            "Epoch 84/100\n",
            "1426/1426 [==============================] - 0s 157us/step - loss: 0.0107 - mean_absolute_error: 0.0810 - val_loss: 0.0132 - val_mean_absolute_error: 0.0865\n",
            "Epoch 85/100\n",
            "1426/1426 [==============================] - 0s 173us/step - loss: 0.0115 - mean_absolute_error: 0.0837 - val_loss: 0.0173 - val_mean_absolute_error: 0.0997\n",
            "Epoch 86/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0116 - mean_absolute_error: 0.0849 - val_loss: 0.0128 - val_mean_absolute_error: 0.0855\n",
            "Epoch 87/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0113 - mean_absolute_error: 0.0830 - val_loss: 0.0127 - val_mean_absolute_error: 0.0852\n",
            "Epoch 88/100\n",
            "1426/1426 [==============================] - 0s 152us/step - loss: 0.0117 - mean_absolute_error: 0.0847 - val_loss: 0.0136 - val_mean_absolute_error: 0.0876\n",
            "Epoch 89/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0115 - mean_absolute_error: 0.0844 - val_loss: 0.0138 - val_mean_absolute_error: 0.0888\n",
            "Epoch 90/100\n",
            "1426/1426 [==============================] - 0s 150us/step - loss: 0.0118 - mean_absolute_error: 0.0844 - val_loss: 0.0128 - val_mean_absolute_error: 0.0854\n",
            "Epoch 91/100\n",
            "1426/1426 [==============================] - 0s 169us/step - loss: 0.0114 - mean_absolute_error: 0.0845 - val_loss: 0.0132 - val_mean_absolute_error: 0.0863\n",
            "Epoch 92/100\n",
            "1426/1426 [==============================] - 0s 164us/step - loss: 0.0115 - mean_absolute_error: 0.0853 - val_loss: 0.0137 - val_mean_absolute_error: 0.0892\n",
            "Epoch 93/100\n",
            "1426/1426 [==============================] - 0s 155us/step - loss: 0.0113 - mean_absolute_error: 0.0832 - val_loss: 0.0131 - val_mean_absolute_error: 0.0863\n",
            "Epoch 94/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0111 - mean_absolute_error: 0.0831 - val_loss: 0.0157 - val_mean_absolute_error: 0.0945\n",
            "Epoch 95/100\n",
            "1426/1426 [==============================] - 0s 153us/step - loss: 0.0114 - mean_absolute_error: 0.0835 - val_loss: 0.0144 - val_mean_absolute_error: 0.0906\n",
            "Epoch 96/100\n",
            "1426/1426 [==============================] - 0s 162us/step - loss: 0.0114 - mean_absolute_error: 0.0847 - val_loss: 0.0157 - val_mean_absolute_error: 0.0949\n",
            "Epoch 97/100\n",
            "1426/1426 [==============================] - 0s 158us/step - loss: 0.0106 - mean_absolute_error: 0.0814 - val_loss: 0.0137 - val_mean_absolute_error: 0.0888\n",
            "Epoch 98/100\n",
            "1426/1426 [==============================] - 0s 151us/step - loss: 0.0108 - mean_absolute_error: 0.0812 - val_loss: 0.0129 - val_mean_absolute_error: 0.0856\n",
            "Epoch 99/100\n",
            "1426/1426 [==============================] - 0s 156us/step - loss: 0.0115 - mean_absolute_error: 0.0839 - val_loss: 0.0129 - val_mean_absolute_error: 0.0860\n",
            "Epoch 100/100\n",
            "1426/1426 [==============================] - 0s 164us/step - loss: 0.0111 - mean_absolute_error: 0.0831 - val_loss: 0.0127 - val_mean_absolute_error: 0.0856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5V1M9VDr3Jp",
        "colab_type": "code",
        "outputId": "fbb71693-5a46-4ecd-b6ae-d07fe0376cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = lstm_model.predict(trainDataVecs)\n",
        "# score\n",
        "y_pred = y_pred * 100\n",
        "y_pred = np.around(y_pred)\n",
        "kappa_score = score\n",
        "kappa_score = kappa_score * 100\n",
        "kappa_score = np.around(kappa_score)\n",
        "result = cohen_kappa_score(kappa_score,y_pred,weights='quadratic')\n",
        "print(\"Kappa Score:\", result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kappa Score: 0.5644201258658694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isa-7lfpsd8Y",
        "colab_type": "code",
        "outputId": "9884bdac-b080-47b7-f9a2-b655f4f282c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Prediction\n",
        "num_essay = 6\n",
        "proto_essay = data['essay'][num_essay]\n",
        "proto_score = data['domain1_score'][num_essay]\n",
        "predict_essay = []\n",
        "predict_essay.append(essay_to_wordlist(proto_essay, remove_stopwords=True))\n",
        "hello = getAvgFeatureVecs(predict_essay, model_words, num_features)\n",
        "hello = np.reshape(hello, (hello.shape[0], 1, hello.shape[1]))\n",
        "hello.shape\n",
        "print(\"Actual score:\",proto_score)\n",
        "print(\"Predicted score:\",lstm_model.predict(hello)[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual score: 0.8333333333333334\n",
            "Predicted score: 0.78438145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsQ0kiT94xHh",
        "colab_type": "code",
        "outputId": "e0bacc23-41fc-4fac-f124-806f170a23ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plotting\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# plt.plot(history.history['mean_absolute_error'])\n",
        "plt.plot(history.history['val_mean_absolute_error'])\n",
        "plt.title('Val Mean Absolute error')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.savefig(\"just_val.png\", dpi=300)\n",
        "# plt.show()\n",
        "# history.history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTEAscBhjjAmIBQ5jjDEB+X9srkl4hxLuuAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}